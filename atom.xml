<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小龙播客</title>
  
  <subtitle>小龙播客</subtitle>
  <link href="https://xiaoyvlongoing.github.io/atom.xml" rel="self"/>
  
  <link href="https://xiaoyvlongoing.github.io/"/>
  <updated>2021-11-05T07:13:17.384Z</updated>
  <id>https://xiaoyvlongoing.github.io/</id>
  
  <author>
    <name>小龙</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer架构解析</title>
    <link href="https://xiaoyvlongoing.github.io/2020/12/05/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/"/>
    <id>https://xiaoyvlongoing.github.io/2020/12/05/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/</id>
    <published>2020-12-05T06:56:57.000Z</published>
    <updated>2021-11-05T07:13:17.384Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer架构解析"><a href="#Transformer架构解析" class="headerlink" title="Transformer架构解析"></a>Transformer架构解析</h1><p>Transformer总体架构可分为四个部分:编码层是连接到解码层的第二子层里面</p><p>Transformer：实现英语法（训练和预测有着很大的差别的）</p><p>预测的时候Decoder端本质上是单循环，单步输出</p><p>预测的时候：如果有10个单词就要循环10次，一次循环输出一次，inear和softmax一次，需要循环10次</p><p>训练的时候：并行的过程编码（英文）和解码（法文）并行向上，经过6层，12层一次输出，所以在训练的时候比较快，但是在测试的时候进行一次，一次的输出对比较忙些</p><p><img src="http://121.199.45.168:8001/img/4.png" alt="avatar"></p><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/1.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/2.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/3.png' height=px ><ul><li><p>输入部分</p><ul><li>源文本嵌入层及其位置编码器</li></ul><h3 id="文本嵌入层的作用"><a href="#文本嵌入层的作用" class="headerlink" title="文本嵌入层的作用"></a>文本嵌入层的作用</h3><ul><li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 导入必备的工具包</span><span class="token keyword">import</span> torch<span class="token comment" spellcheck="true"># 预定义的网络层torch.nn, 工具开发者已经帮助我们开发好的一些常用层, </span><span class="token comment" spellcheck="true"># 比如，卷积层, lstm层, embedding层等, 不需要我们再重新造轮子.</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token comment" spellcheck="true"># 数学计算工具包</span><span class="token keyword">import</span> math<span class="token comment" spellcheck="true"># torch中变量封装函数Variable.</span><span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable<span class="token comment" spellcheck="true"># 定义Embeddings类来实现文本嵌入层，这里s说明代表两个一模一样的嵌入层, 他们共享参数.</span><span class="token comment" spellcheck="true"># 该类继承nn.Module, 这样就有标准层的一些功能, 这里我们也可以理解为一种模式, 我们自己实现的所有层都会这样去写.</span><span class="token keyword">class</span> <span class="token class-name">Embeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> vocab<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""类的初始化函数, 有两个参数, d_model: 指词嵌入的维度, vocab: 指词表的大小."""</span>        <span class="token comment" spellcheck="true"># 接着就是使用super的方式指明继承nn.Module的初始化函数, 我们自己实现的所有层都会这样去写.</span>        super<span class="token punctuation">(</span>Embeddings<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 之后就是调用nn中的预定义层Embedding, 获得一个词嵌入对象self.lut</span>        self<span class="token punctuation">.</span>lut <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 最后就是将d_model传入类中</span>        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""可以将其理解为该层的前向传播逻辑，所有层中都会有此函数           当传给该类的实例化对象参数时, 自动调用该类函数           参数x: 因为Embedding层是首层, 所以代表输入给模型的文本通过词汇映射后的张量"""</span>        <span class="token comment" spellcheck="true"># 将x传给self.lut并与根号下self.d_model相乘作为结果返回</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>lut<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span></code></pre><h3 id="位置编码器的作用"><a href="#位置编码器的作用" class="headerlink" title="位置编码器的作用"></a>位置编码器的作用</h3><ul><li><p>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.</p></li><li><p>目标文本嵌入层及其位置编码器</p><ul><li>​    <img src="http://121.199.45.168:8001/img/5.png" alt="avatar"></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 定义位置编码器类, 我们同样把它看做一个层, 因此会继承nn.Module    </span><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""位置编码器类的初始化函数, 共有三个参数, 分别是d_model: 词嵌入维度,            dropout: 置0比率, max_len: 每个句子的最大长度"""</span>        super<span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 实例化nn中预定义的Dropout层, 并将dropout传入其中, 获得对象self.dropout</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始化一个位置编码矩阵, 它是一个0阵，矩阵的大小是max_len x d_model.</span>        pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始化一个绝对位置矩阵, 在我们这里，词汇的绝对位置就是用它的索引去表示. </span>        <span class="token comment" spellcheck="true"># 所以我们首先使用arange方法获得一个连续自然数向量，然后再使用unsqueeze方法拓展向量维度使其成为矩阵， </span>        <span class="token comment" spellcheck="true"># 又因为参数传的是1，代表矩阵拓展的位置，会使向量变成一个max_len x 1 的矩阵， </span>        position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_len<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中，</span>        <span class="token comment" spellcheck="true"># 最简单思路就是先将max_len x 1的绝对位置矩阵， 变换成max_len x d_model形状，然后覆盖原来的初始位置编码矩阵即可， </span>        <span class="token comment" spellcheck="true"># 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要求除了形状外，</span>        <span class="token comment" spellcheck="true"># 还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛.  这样我们就可以开始初始化这个变换矩阵了.</span>        <span class="token comment" spellcheck="true"># 首先使用arange获得一个自然数矩阵， 但是细心的同学们会发现， 我们这里并没有按照预计的一样初始化一个1xd_model的矩阵， </span>        <span class="token comment" spellcheck="true"># 而是有了一个跳跃，只初始化了一半即1xd_model/2 的矩阵。 为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵，</span>        <span class="token comment" spellcheck="true"># 我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正弦波上， 第二次初始化的变换矩阵分布在余弦波上， </span>        <span class="token comment" spellcheck="true"># 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵.</span>        div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span>                             <span class="token operator">-</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">10000.0</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>        pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span>        pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，</span>        <span class="token comment" spellcheck="true"># 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.</span>        pe <span class="token operator">=</span> pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，</span>        <span class="token comment" spellcheck="true"># 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤进行更新的增益对象. </span>        <span class="token comment" spellcheck="true"># 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.</span>        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'pe'</span><span class="token punctuation">,</span> pe<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""forward函数的参数是x, 表示文本序列的词嵌入表示"""</span>        <span class="token comment" spellcheck="true"># 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切片到与输入的x的第二维相同即x.size(1)，</span>        <span class="token comment" spellcheck="true"># 因为我们默认max_len为5000一般来讲实在太大了，很难有一条句子包含5000个词汇，所以要进行与输入张量的适配. </span>        <span class="token comment" spellcheck="true"># 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requires_grad设置成false.</span>        x <span class="token operator">=</span> x <span class="token operator">+</span> Variable<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                          requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 最后使用self.dropout对象进行'丢弃'操作, 并返回结果.</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><table><thead><tr><th>函数</th><th>函数性质</th><th>解释</th></tr></thead><tbody><tr><td><code>register_buffer</code>（ <em>name</em> ， <em>张量</em> ， <em>persistent = True</em> ）相当于定死的操作，并不能被进行梯度进行更新，加载</td><td>参数                                                                                                                            <strong>name</strong> （ <em>字符串</em> ）–缓冲区的名称。 可以使用给定名称从此模块访问缓冲区                                                                                                          <strong>tensor</strong> （ <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em> </a>）–要注册的缓冲区。                                                       <strong>persistent</strong> （ <a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em> </a>）–缓冲区是否是此模块的一部分 <a href="https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html?highlight=register_buffer#torch.jit.ScriptModule.state_dict"><code>state_dict</code></a>.</td><td>向模块添加缓冲区。这通常用于注册不应被视为模型参数的缓冲区。  例如，BatchNorm的  <code>running_mean</code>不是参数，而是模块状态的一部分。  默认情况下，缓冲区是持久性的，并将与参数一起保存。  可以通过设置更改此行为  <code>persistent</code> 至  <code>False</code>。  持久缓冲区和非持久缓冲区之间的唯一区别是后者将不属于该模块的一部分  <a href="https://pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html?highlight=register_buffer#torch.jit.ScriptModule.state_dict"><code>state_dict</code></a>. 缓冲区可以使用给定名称作为属性进行访问。</td></tr><tr><td></td><td></td><td></td></tr><tr><td><code>masked_fill</code>（ <em>掩码</em> ， <em>值</em> ） →张量</td><td>进行掩码操作</td><td></td></tr><tr><td>.clones</td><td>对函数进行克隆</td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table></li></ul></li><li><p>输出部分</p><ul><li>线性层</li><li>softmax层<ul><li><img src="http://121.199.45.168:8001/img/6.png" alt="avatar"></li></ul></li></ul></li><li><h3 id="编码器部分：编码和解码都可以用到，如果用到未来的信息会得到问题，在解码的时候尽量不能看到后面的信息"><a href="#编码器部分：编码和解码都可以用到，如果用到未来的信息会得到问题，在解码的时候尽量不能看到后面的信息" class="headerlink" title="编码器部分：编码和解码都可以用到，如果用到未来的信息会得到问题，在解码的时候尽量不能看到后面的信息"></a>编码器部分：编码和解码都可以用到，如果用到未来的信息会得到问题，在解码的时候尽量不能看到后面的信息</h3><ul><li>由N个编码器层堆叠而成 </li><li>每个编码器层由两个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><p><img src="http://121.199.45.168:8001/img/7.png" alt="avatar"></p><ul><li><h4 id="1-：掩码张量"><a href="#1-：掩码张量" class="headerlink" title="1 ：掩码张量"></a>1 ：掩码张量</h4></li></ul><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/4.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/5.png' height=px ><ul><li><table><thead><tr><th>函数</th><th>函数性质</th><th>函数解释</th></tr></thead><tbody><tr><td>掩码张量</td><td></td><td></td></tr><tr><td><code>torch.``triu</code>（ <em>输入</em> ， <em>对角线= 0</em> ， *** ， <em>输出=无</em> ） →张量</td><td><strong>input</strong> （ <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em> </a>）–输入张量。  <strong>对角线</strong> （ <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em> </a><em>，</em> <em>可选</em> ）–要考虑的对角线 ，out （ <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em> </a><em>，</em> <em>可选</em> ）–输出张量。</td><td>返回矩阵（二维张量）或矩阵批次的上三角部分  <code>input</code>，结果张量的其他元素  <code>out</code> 设置为0。 争论  <a href="https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal"><code>diagonal</code></a>控制要考虑的对角线。  如果  <a href="https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal"><code>diagonal</code></a>= 0时，矩阵的上三角部分定义为对角线上方和上方的元素，将保留主对角线上及上方的所有元素。  正值排除主要对角线上方的对角线，同样，负值排除主要对角线下方的对角线。  主要对角线是索引集  { ( 一世 , 一世 ) } \ lbrace（i，i）\ rbrace { （ i ， i ） }   为 一世 ∈ [ 0 , 分 ⁡ { d 1 , d 2 } − 1 ] 我\ in [0，\ min \ {d_ {1}，d_ {2} }-1] 我 ∈ [ 0 ， 分钟 { d 1 ， d 2 } - 1 ]   ，其中 d 1 , d 2 d_ {1}，d_ {2} d 1 ， d 2   是所述矩阵的维数。</td></tr><tr><td></td><td></td><td></td></tr></tbody></table></li><li><p>什么是掩码张量</p><ul><li>它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩，也可以说被替换, 它的表现形式是一个张量.</li></ul></li><li><p>掩码张量的作用:</p><ul><li>在transformer中,  掩码张量的主要作用在应用attention(将在下一小节讲解)时，有一些生成的attention张量中的值计算有可能已知了未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用.  所以，我们会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.</li></ul></li></ul></li></ul><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">subsequent_mask</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""生成向后遮掩的掩码张量, 参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵"""</span>    <span class="token comment" spellcheck="true"># 在函数中, 首先定义掩码张量的形状</span>    attn_shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> size<span class="token punctuation">,</span> size<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间, </span>    <span class="token comment" spellcheck="true"># 再使其中的数据类型变为无符号8位整形unit8 </span>    subsequent_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>attn_shape<span class="token punctuation">)</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'uint8'</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作, </span>    <span class="token comment" spellcheck="true"># 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减, </span>    <span class="token comment" spellcheck="true"># 如果是0, subsequent_mask中的该位置由0变成1</span>    <span class="token comment" spellcheck="true"># 如果是1, subsequent_mask中的该位置由1变成0 </span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> subsequent_mask<span class="token punctuation">)</span></code></pre><ul><li></li><li>解码器部分<ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接<ul><li>​    <img src="http://121.199.45.168:8001/img/8.png" alt="avatar"></li></ul></li></ul></li></ul><h4 id="学习了文本嵌入层的作用"><a href="#学习了文本嵌入层的作用" class="headerlink" title="学习了文本嵌入层的作用:"></a>学习了文本嵌入层的作用:</h4><ul><li>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中<strong>词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</strong></li></ul><h4 id="学习并实现了文本嵌入层的类-Embeddings"><a href="#学习并实现了文本嵌入层的类-Embeddings" class="headerlink" title="学习并实现了文本嵌入层的类: Embeddings"></a>学习并实现了文本嵌入层的类: Embeddings</h4><ul><li>初始化函数以d_model, 词嵌入维度, 和vocab, 词汇总数为参数, 内部主要使用了nn中的预定层Embedding进行词嵌入.</li><li>在forward函数中, 将输入x传入到Embedding的实例化对象中, 然后乘以一个根号下d_model进行缩放, 控制数值大小.</li><li>它的输出是文本嵌入后的结果.</li></ul><h4 id="学习了位置编码器的作用"><a href="#学习了位置编码器的作用" class="headerlink" title="学习了位置编码器的作用:"></a>学习了位置编码器的作用:</h4><ul><li>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失.</li></ul><h4 id="学习并实现了位置编码器的类-PositionalEncoding"><a href="#学习并实现了位置编码器的类-PositionalEncoding" class="headerlink" title="学习并实现了位置编码器的类: PositionalEncoding"></a>学习并实现了位置编码器的类: PositionalEncoding</h4><ul><li>初始化函数以d_model, dropout, max_len为参数, 分别代表d_model: 词嵌入维度, dropout: 置0比率, max_len: 每个句子的最大长度.</li><li>forward函数中的输入参数为x, 是Embedding层的输出.</li><li>最终输出一个加入了位置编码信息的词嵌入张量.</li></ul><h4 id="实现了绘制词汇向量中特征的分布曲线"><a href="#实现了绘制词汇向量中特征的分布曲线" class="headerlink" title="实现了绘制词汇向量中特征的分布曲线:"></a>实现了绘制词汇向量中特征的分布曲线:</h4><ul><li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化.</li><li>正弦波和余弦波的值域范围都是1到-1, 这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算.</li></ul><h3 id="2-注意力机制"><a href="#2-注意力机制" class="headerlink" title="2 注意力机制"></a>2 注意力机制</h3><p>我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制.</p><ul><li>什么是注意力计算规则:<ul><li>它需要三个指定的输入Q(query), K(key), V(value), 然后通过公式得到注意力的计算结果, 这个结果代表query在key和value作用下的表示. 而这个具体的计算规则有很多种, 我这里只介绍我们用到的这一种.</li></ul></li></ul><hr><ul><li>我们这里使用的注意力的计算规则:</li></ul><p><img src="http://121.199.45.168:8001/img/9.png" alt="avatar"></p><pre class=" language-python"><code class="language-python">假如我们有一个问题<span class="token punctuation">:</span> 给出一段文本，使用一些关键词对它进行描述!为了方便统一正确答案，这道题可能预先已经给大家写出了一些关键词作为提示<span class="token punctuation">.</span>其中这些给出的提示就可以看作是key， 而整个的文本信息就相当于是query，value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的答案信息，这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息，因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多，并且能够开始对我们query也就是这段文本，提取关键信息进行表示<span class="token punctuation">.</span>  这就是注意力作用的过程， 通过这个过程，我们最终脑子里的value发生了变化，根据提示key生成了query的关键词表示方法，也就是另外一种特征表示方法<span class="token punctuation">.</span>刚刚我们说到key和value一般情况下默认是相同，与query是不同的，这种是我们一般的注意力输入形式，但有一种特殊情况，就是我们query与key和value相同，这种情况我们称为自注意力机制，就如同我们的刚刚的例子， 使用一般注意力机制，是使用不同于给定文本的关键词表示它<span class="token punctuation">.</span> 而自注意力机制<span class="token punctuation">,</span>需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它<span class="token punctuation">,</span> 相当于对文本自身的一次特征提取<span class="token punctuation">.</span></code></pre><p>提示就是相当与：K</p><p>整个文本的信息：Q</p><p>自己得到的就是：V</p><p>一般注意力机制形式就是：K和V是很相近的，</p><p>自注意力机制：是Q，K，V这三个都是相当于相同，</p><p>dk的最后一维的大小, 一般情况下就等同于我们的词嵌入维度</p><p><img src="http://121.199.45.168:8001/img/9.png" alt="avatar"></p><p><img src="http://121.199.45.168:8001/img/10.png" alt="avatar"></p><p>在函数中, 首先取query的最后一维的大小, 一般情况下就等同于我们的词嵌入维度, 命名为d_k</p><p>query，key：因为他们最后一层都是词嵌入维度<br>transpose：先进行矩阵转置<br>matmul：矩阵相乘</p><h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><table><thead><tr><th>函数</th><th>函数性质</th><th>函数解释</th></tr></thead><tbody><tr><td><code>contiguous</code>(<em>memory_format=torch.contiguous_format</em>) → Tensor</td><td><strong>memory_format</strong> （ <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.memory_format"><code>torch.memory_format</code></a>（可选）–返回的Tensor所需的内存格式。  默认：  <code>torch.contiguous_format</code>.</td><td>返回一个 连续 的内存张量，其中包含与以下内容相同的数据 <code>self</code>张量。  如          果<br/><code>self</code> 张                    量已经是指定的内存格式，此函数返                                                                                                     回&lt;b</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><p>​                      </p><p>从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是，我只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，这些变换不会改变原有张量的尺寸，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量.<br>这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</p><p>多头注意力机制的作用:</p><ul><li>这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</li></ul><p><img src="http://121.199.45.168:8001/img/13.png" alt="avatar"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 用于深度拷贝的copy工具包</span><span class="token keyword">import</span> copy<span class="token comment" spellcheck="true"># 首先需要定义克隆函数, 因为在多头注意力机制的实现中, 用到多个结构相同的线性层.</span><span class="token comment" spellcheck="true"># 我们将使用clone函数将他们一同初始化在一个网络层列表对象中. 之后的结构中也会用到该函数.</span><span class="token keyword">def</span> <span class="token function">clones</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""用于生成相同网络层的克隆函数, 它的参数module表示要克隆的目标网络层, N代表需要克隆的数量"""</span>    <span class="token comment" spellcheck="true"># 在函数中, 我们通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层,</span>    <span class="token comment" spellcheck="true"># 然后将其放在nn.ModuleList类型的列表中存放.</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>module<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>实现了深度拷贝<span class="token comment" spellcheck="true"># 我们使用一个类来实现多头注意力机制的处理</span><span class="token keyword">class</span> <span class="token class-name">MultiHeadedAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> head<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""在类的初始化时, 会传入三个参数，head代表头数，embedding_dim代表词嵌入的维度，            dropout代表进行dropout操作时置0比率，默认是0.1."""</span>        super<span class="token punctuation">(</span>MultiHeadedAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，</span>        <span class="token comment" spellcheck="true"># 这是因为我们之后要给每个头分配等量的词特征.也就是embedding_dim/head个.</span>        <span class="token keyword">assert</span> embedding_dim <span class="token operator">%</span> head <span class="token operator">==</span> <span class="token number">0</span><span class="token comment" spellcheck="true">#embedding_dim：词嵌入维度，head：头</span>        <span class="token comment" spellcheck="true"># 得到每个头获得的分割词向量维度d_k</span>        self<span class="token punctuation">.</span>d_k <span class="token operator">=</span> embedding_dim <span class="token operator">//</span> head        <span class="token comment" spellcheck="true">#词嵌入拉平</span>        embedding_dim：词嵌入的维度        <span class="token comment" spellcheck="true"># 传入头数h</span>        self<span class="token punctuation">.</span>head <span class="token operator">=</span> head        <span class="token comment" spellcheck="true"># 然后获得线性层对象，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用clones函数克隆四个，</span>        <span class="token comment" spellcheck="true"># 为什么是四个呢，这是因为在多头注意力中，Q，K，V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个.线性层</span>        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> clones<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None.</span>        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> None        <span class="token comment" spellcheck="true"># 最后就是一个self.dropout对象，它通过nn中的Dropout实例化而来，置0比率为传进来的参数dropout.</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#多头注意力机制，计算的函数</span>        <span class="token triple-quoted-string string">"""前向逻辑函数, 它的输入参数有四个，前三个就是注意力机制需要的Q, K, V，           最后一个是注意力机制中可能需要的mask掩码张量，默认是None. """</span>        <span class="token comment" spellcheck="true"># 如果存在掩码张量mask</span>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 使用unsqueeze拓展维度，代表多头中的第n个头</span>            mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本.</span>        batch_size <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 之后就进入多头处理环节</span>        <span class="token comment" spellcheck="true"># 首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，将输入QKV分别传到线性层中，</span>        <span class="token comment" spellcheck="true"># 做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加了一个维度h，代表头数，</span>        <span class="token comment" spellcheck="true"># 这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，</span>        <span class="token comment" spellcheck="true"># 计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，</span>        <span class="token comment" spellcheck="true"># 为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，</span>        <span class="token comment" spellcheck="true"># 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.这样我们就得到了每个头的输入.</span>        query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value <span class="token operator">=</span> \           <span class="token punctuation">[</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>head<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true">#view（批次，自适应，头，每个头的维度），这里是有说明的，整一个批次其实是数据的连续存储的，先用批次进行划分，形成几个单独的模块，在进行划分头，和每个头的维度。</span>                       <span class="token keyword">for</span> model<span class="token punctuation">,</span> x <span class="token keyword">in</span> zip<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linears<span class="token punctuation">,</span> <span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#self.linears：拷贝的线性层，(query, key, value)：以元组的形象qkv</span><span class="token comment" spellcheck="true">#-1为自适应</span>        <span class="token comment" spellcheck="true"># 得到每个头的输入后，接下来就是将他们传入到attention中，</span>        <span class="token comment" spellcheck="true"># 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中.</span>        x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attn <span class="token operator">=</span> attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">,</span> dropout<span class="token operator">=</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#self.attn：注意力的权重矩阵</span>        <span class="token comment" spellcheck="true"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，</span>        <span class="token comment" spellcheck="true"># 因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法，</span>        <span class="token comment" spellcheck="true"># 这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，</span>        <span class="token comment" spellcheck="true"># 所以，下一步就是使用view重塑形状，变成和输入形状相同.</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>head <span class="token operator">*</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p>学习并实现了多头注意力机制的类: MultiHeadedAttention</p><ul><li>因为多头注意力机制中需要使用多个相同的线性层, 首先实现了克隆函数clones.</li><li>clones函数的输入是module，N，分别代表克隆的目标层，和克隆个数.</li><li>clones函数的输出是装有N个克隆层的Module列表.</li><li>接着实现MultiHeadedAttention类, 它的初始化函数输入是h, d_model, dropout分别代表头数，词嵌入维度和置零比率.</li><li>它的实例化对象输入是Q, K, V以及掩码张量mask.</li><li>它的实例化对象输出是通过多头注意力机制处理的Q的注意力表示.</li></ul><h3 id="前馈全连接层：对维度不进行改变，就是把对复杂过程的拟合程度不够-通过增加两层网络来增强模型的能力"><a href="#前馈全连接层：对维度不进行改变，就是把对复杂过程的拟合程度不够-通过增加两层网络来增强模型的能力" class="headerlink" title="前馈全连接层：对维度不进行改变，就是把对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力"></a>前馈全连接层：对维度不进行改变，就是把对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力</h3><ul><li>什么是前馈全连接层:<ul><li>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</li></ul></li></ul><hr><ul><li>前馈全连接层的作用:<ul><li>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力</li></ul></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 通过类PositionwiseFeedForward来实现前馈全连接层</span><span class="token keyword">class</span> <span class="token class-name">PositionwiseFeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#d_model：词嵌入的维度</span>        <span class="token triple-quoted-string string">"""初始化函数有三个输入参数分别是d_model：词嵌入维度, d_ff：前馈全连接层输入的维度,和dropout=0.1，第一个是线性层的输入维度也是第二个线性层的输出维度，           因为我们希望输入通过前馈全连接层后输入和输出的维度不变. 第二个参数d_ff就是第二个线性层的输入维度和第一个线性层的输出维度.            最后一个是dropout置0比率."""</span>        super<span class="token punctuation">(</span>PositionwiseFeedForward<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 首先按照我们预期使用nn实例化了两个线性层对象，self.w1和self.w2</span>        <span class="token comment" spellcheck="true"># 它们的参数分别是d_model, d_ff和d_ff, d_model</span>        self<span class="token punctuation">.</span>w1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 然后使用nn的Dropout实例化了对象self.dropout</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""输入参数为x，代表来自上一层的输出"""</span>        <span class="token comment" spellcheck="true"># 首先经过第一个线性层，然后使用Funtional中relu函数进行激活,</span>        <span class="token comment" spellcheck="true"># 之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果.</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>w2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>学习并实现了前馈全连接层的类: PositionwiseFeedForward</p><ul><li>它的实例化参数为d_model, d_ff, dropout, 分别代表词嵌入维度, 线性变换维度, 和置零比率.</li><li>它的输入参数x, 表示上层的输出.</li><li>它的输出是经过2层线性网络变换的特征表示.</li></ul><h3 id="规范化层：增加规范化是因为这个模型使用的大量的注意力机制存在很多的过大过小的情况，导致学习过程出现异常，使用规范化"><a href="#规范化层：增加规范化是因为这个模型使用的大量的注意力机制存在很多的过大过小的情况，导致学习过程出现异常，使用规范化" class="headerlink" title="规范化层：增加规范化是因为这个模型使用的大量的注意力机制存在很多的过大过小的情况，导致学习过程出现异常，使用规范化"></a>规范化层：增加规范化是因为这个模型使用的大量的注意力机制存在很多的过大过小的情况，导致学习过程出现异常，使用规范化</h3><p>规范化层的作用:</p><ul><li>它是所有深层网络模型都需要的标准网络层，因为随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况，这样可能会导致学习过程出现异常，模型可能收敛非常的慢. 因此都会在一定层数后接规范化层进行数值的规范化，使其特征数值在合理范围内.</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 通过LayerNorm实现规范化层的类</span><span class="token keyword">class</span> <span class="token class-name">LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""初始化函数有两个参数, 一个是features, 表示词嵌入的维度,           另一个是eps它是一个足够小的数, 在规范化公式的分母中出现,           防止分母为0.默认是1e-6."""</span>        super<span class="token punctuation">(</span>LayerNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 根据features的形状初始化两个参数张量a2，和b2，第一个初始化为1张量，</span>        <span class="token comment" spellcheck="true"># 也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，这两个张量就是规范化层的参数，</span>        <span class="token comment" spellcheck="true"># 因为直接对上一层得到的结果做规范化公式计算，将改变结果的正常表征，因此就需要有参数作为调节因子，</span>        <span class="token comment" spellcheck="true"># 使其即能满足规范化要求，又能不改变针对目标的表征.最后使用nn.parameter封装，代表他们是模型的参数。</span>        <span class="token comment" spellcheck="true">#nn.parameter封装会被训练的时候可以进行更新</span>        self<span class="token punctuation">.</span>a2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#一个全1</span>        self<span class="token punctuation">.</span>b2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#一个全0</span>        <span class="token comment" spellcheck="true"># 把eps传到类中</span>        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""输入参数x代表来自上一层的输出"""</span>        <span class="token comment" spellcheck="true"># 在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致.</span>        <span class="token comment" spellcheck="true"># 接着再求最后一个维度的标准差，然后就是根据规范化公式，用x减去均值除以标准差获得规范化的结果，</span>        <span class="token comment" spellcheck="true"># 最后对结果乘以我们的缩放参数，即a2，*号代表同型点乘，即对应位置进行乘法操作，加上位移参数b2.返回即可.</span>        mean <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#求均值</span>        std <span class="token operator">=</span> x<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#标准差</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>a2 <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>std <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b2</code></pre><p>学习并实现了规范化层的类: LayerNorm</p><ul><li>它的实例化参数有两个, features和eps，分别表示词嵌入特征大小，和一个足够小的数.</li><li>它的输入参数x代表来自上一层的输出.</li><li>它的输出就是经过规范化的特征表示.</li></ul><h3 id="子层连接结构"><a href="#子层连接结构" class="headerlink" title="子层连接结构"></a>子层连接结构</h3><p>什么是子层连接结构:</p><ul><li>如图所示，输入到每个子层以及规范化层的过程中，还使用了残差链接（跳跃连接），因此我们把这一部分结构整体叫做子层连接（代表子层及其链接结构），在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</li></ul><p><img src="http://121.199.45.168:8001/img/15.png" alt="avatar"></p><p><img src="http://121.199.45.168:8001/img/16.png" alt="avatar"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用SublayerConnection来实现子层连接结构的类</span><span class="token keyword">class</span> <span class="token class-name">SublayerConnection</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""它输入参数有两个, size以及dropout， size一般是都是词嵌入维度的大小，            dropout本身是对模型结构中的节点数进行随机抑制的比率，            又因为节点被抑制等效就是该节点的输出都是0，因此也可以把dropout看作是对输出矩阵的随机置0的比率.        """</span>        super<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 实例化了规范化对象self.norm</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>size<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 又使用nn中预定义的droupout实例化一个self.dropout对象.</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> sublayer<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""前向逻辑函数中, 接收上一个层或者子层的输入作为第一个参数，           将该子层连接中的子层函数作为第二个参数"""</span>        <span class="token comment" spellcheck="true"># 我们首先对输出进行规范化，然后将结果传给子层处理，之后再对子层进行dropout操作，</span>        <span class="token comment" spellcheck="true"># 随机停止一些网络中神经元的作用，来防止过拟合. 最后还有一个add操作， </span>        <span class="token comment" spellcheck="true"># 因为存在跳跃连接，所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出.</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>sublayer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>学习并实现了子层连接结构的类: SublayerConnection</p><ul><li>类的初始化函数输入参数是size, dropout, 分别代表词嵌入大小和置零比率.</li><li>它的实例化对象输入参数是x, sublayer, 分别代表上一层输出以及子层的函数表示.</li><li>它的输出就是通过子层连接结构处理的输出.</li></ul><h3 id="编码器层"><a href="#编码器层" class="headerlink" title="编码器层"></a>编码器层</h3><p>编码器层的作用:</p><ul><li>作为编码器的组成单元, 每个编码器层完成一次对输入的特征提取过程, 即编码过程.</li></ul><p><img src="http://121.199.45.168:8001/img/17.png" alt="avatar"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用EncoderLayer类实现编码器层</span><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> self_attn<span class="token punctuation">,</span> feed_forward<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""它的初始化函数参数有四个，分别是size，其实就是我们词嵌入维度的大小，它也将作为我们编码器层的大小,            第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制,            第三个是feed_froward, 之后我们将传入前馈全连接层实例化对象, 最后一个是置0比率dropout."""</span>        super<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 首先将self_attn和feed_forward传入其中.</span>        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> self_attn        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> feed_forward        <span class="token comment" spellcheck="true"># 如图所示, 编码器层中有两个子层连接结构, 所以使用clones函数进行克隆</span>        <span class="token comment" spellcheck="true">#SublayerConnection：实现子层连接</span>        self<span class="token punctuation">.</span>sublayer <span class="token operator">=</span> clones<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">(</span>size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#克隆了两个</span>        <span class="token comment" spellcheck="true"># 把size传入其中</span>        self<span class="token punctuation">.</span>size <span class="token operator">=</span> size    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""forward函数中有两个输入参数，x和mask，分别代表上一层的输出，和掩码张量mask."""</span>        <span class="token comment" spellcheck="true"># 里面就是按照结构图左侧的流程. 首先通过第一个子层连接结构，其中包含多头自注意力子层，</span>        <span class="token comment" spellcheck="true"># 然后通过第二个子层连接结构，其中包含前馈全连接子层. 最后返回结果.</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#三个x分别是Q,K,V</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python">size <span class="token operator">=</span> <span class="token number">512</span><span class="token comment" spellcheck="true">#</span>head <span class="token operator">=</span> <span class="token number">8</span><span class="token comment" spellcheck="true">#头</span>d_model <span class="token operator">=</span> <span class="token number">512</span><span class="token comment" spellcheck="true">#词嵌入维度</span>d_ff <span class="token operator">=</span> <span class="token number">64</span><span class="token comment" spellcheck="true">#矩阵的方阵</span>x <span class="token operator">=</span> pe_result<span class="token comment" spellcheck="true">#位置编码器</span>dropout <span class="token operator">=</span> <span class="token number">0.2</span><span class="token comment" spellcheck="true">#</span>self_attn <span class="token operator">=</span> MultiHeadedAttention<span class="token punctuation">(</span>head<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#多头注意力</span>ff <span class="token operator">=</span> PositionwiseFeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#前馈全连接网络</span>mask <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#伪的数</span></code></pre><pre class=" language-python"><code class="language-python">el <span class="token operator">=</span> EncoderLayer<span class="token punctuation">(</span>size<span class="token punctuation">,</span> self_attn<span class="token punctuation">,</span> ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#size：词嵌入，多头注意力，前馈全连接层，</span>el_result <span class="token operator">=</span> el<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>el_result<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>el_result<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></code></pre><p>学习并实现了编码器层的类: EncoderLayer</p><ul><li>类的初始化函数共有4个, 别是size，其实就是我们词嵌入维度的大小.  第二个self_attn，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制. 第三个是feed_froward,  之后我们将传入前馈全连接层实例化对象. 最后一个是置0比率dropout.</li><li>实例化对象的输入参数有2个，x代表来自上一层的输出, mask代表掩码张量.</li><li>它的输出代表经过整个编码层的特征表示.</li></ul><h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>编码器的作用:</p><ul><li>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</li></ul><p><img src="http://121.199.45.168:8001/img/7.png" alt="avatar"></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用Encoder类来实现编码器</span><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""初始化函数的两个参数分别代表编码器层和编码器层的个数"""</span>        super<span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 首先使用clones函数克隆N个编码器层放在self.layers中</span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> clones<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 再初始化一个规范化层, 它将用在编码器的最后面.</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""forward函数的输入和编码器层相同, x代表上一层的输出, mask代表掩码张量"""</span>        <span class="token comment" spellcheck="true"># 首先就是对我们克隆的编码器层进行循环，每次都会得到一个新的x，</span>        <span class="token comment" spellcheck="true"># 这个循环的过程，就相当于输出的x经过了N个编码器层的处理. </span>        <span class="token comment" spellcheck="true"># 最后再通过规范化层的对象self.norm进行处理，最后返回结果. </span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#一层一层的传入的</span>            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#得出来的x是下次的输入</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p>实例化参数:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 第一个实例化参数layer, 它是一个编码器层的实例化对象, 因此需要传入编码器层的参数</span><span class="token comment" spellcheck="true"># 又因为编码器层中的子层是不共享的, 因此需要使用深度拷贝各个对象.</span>size <span class="token operator">=</span> <span class="token number">512</span>head <span class="token operator">=</span> <span class="token number">8</span>d_model <span class="token operator">=</span> <span class="token number">512</span>d_ff <span class="token operator">=</span> <span class="token number">64</span>c <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token comment" spellcheck="true">#两个值多次拷贝</span>attn <span class="token operator">=</span> MultiHeadedAttention<span class="token punctuation">(</span>head<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#注意力机制</span>ff <span class="token operator">=</span> PositionwiseFeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#前馈全连接层</span>dropout <span class="token operator">=</span> <span class="token number">0.2</span>layer <span class="token operator">=</span> EncoderLayer<span class="token punctuation">(</span>size<span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>ff<span class="token punctuation">)</span><span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#子层编码器层</span><span class="token comment" spellcheck="true"># 编码器中编码器层的个数N</span>N <span class="token operator">=</span> <span class="token number">8</span>mask <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>调用:</p><pre class=" language-python"><code class="language-python">en <span class="token operator">=</span> Encoder<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span>en_result <span class="token operator">=</span> en<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>en_result<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>en_result<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></code></pre><p>学习并实现了编码器的类: Encoder</p><ul><li>类的初始化函数参数有两个，分别是layer和N，代表编码器层和编码器层的个数.</li><li>forward函数的输入参数也有两个, 和编码器层的forward相同, x代表上一层的输出, mask代码掩码张量.</li><li>编码器类的输出就是Transformer中编码器的特征提取表示, 它将成为解码器的输入的一部分.</li></ul><h2 id="解码器部分实现"><a href="#解码器部分实现" class="headerlink" title="解码器部分实现"></a>解码器部分实现</h2><p>解码器部分:</p><ul><li>由N个解码器层堆叠而成</li><li>每个解码器层由三个子层连接结构组成</li><li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li><li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li><li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li></ul><p><img src="http://121.199.45.168:8001/img/8.png" alt="avatar"></p><p>说明:</p><ul><li>解码器层中的各个部分，如，多头注意力机制，规范化层，前馈全连接网络，子层连接结构都与编码器中的实现相同. 因此这里可以直接拿来构建解码器层.</li></ul><h3 id="解码器层：有两个掩码，以及源数据掩码张量掩码到不重要的信息和目标数据掩码张量，掩码到未来的信息"><a href="#解码器层：有两个掩码，以及源数据掩码张量掩码到不重要的信息和目标数据掩码张量，掩码到未来的信息" class="headerlink" title="解码器层：有两个掩码，以及源数据掩码张量掩码到不重要的信息和目标数据掩码张量，掩码到未来的信息"></a>解码器层：有两个掩码，以及源数据掩码张量掩码到不重要的信息和目标数据掩码张量，掩码到未来的信息</h3><p>解码器层的作用:</p><ul><li>作为解码器的组成单元, 每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程.</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用DecoderLayer的类实现解码器层</span><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> self_attn<span class="token punctuation">,</span> src_attn<span class="token punctuation">,</span> feed_forward<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，            第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，             第三个是src_attn，多头注意力对象，这里Q!=K=V， 第四个是前馈全连接层对象，最后就是droupout置0比率.        """</span>        super<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 在初始化函数中， 主要就是将这些输入传到类中</span>        self<span class="token punctuation">.</span>size <span class="token operator">=</span> size        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> self_attn        self<span class="token punctuation">.</span>src_attn <span class="token operator">=</span> src_attn        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> feed_forward        <span class="token comment" spellcheck="true"># 按照结构图使用clones函数克隆三个子层连接对象.</span>        self<span class="token punctuation">.</span>sublayer <span class="token operator">=</span> clones<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">(</span>size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""forward函数中的参数有4个，分别是来自上一层的输入x，           来自编码器层的语义存储变量mermory（2，4，512）上一编码的输入， 以及源数据掩码张量和目标数据掩码张量.        """</span>        <span class="token comment" spellcheck="true"># 将memory表示成m方便之后使用</span>        m <span class="token operator">=</span> memory        <span class="token comment" spellcheck="true"># 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，因为是自注意力机制，所以Q,K,V都是x，</span>        <span class="token comment" spellcheck="true"># 最后一个参数是目标数据掩码张量，这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据，</span>        <span class="token comment" spellcheck="true"># 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，</span>        <span class="token comment" spellcheck="true"># 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，同样生成第二个字符或词汇时，</span>        <span class="token comment" spellcheck="true"># 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用.</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#target_mask：目标进行掩码</span>        <span class="token comment" spellcheck="true"># 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k，v是编码层输出memory， </span>        <span class="token comment" spellcheck="true"># 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，而是遮蔽掉对结果没有意义的字符而产生的注意力值，</span>        <span class="token comment" spellcheck="true"># 以此提升模型效果和训练速度. 这样就完成了第二个子层的处理.</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>src_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> m<span class="token punctuation">,</span> m<span class="token punctuation">,</span> source_mask<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#常规注意力机制，</span>        <span class="token comment" spellcheck="true"># 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果.这就是我们的解码器层结构.</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span></code></pre><p>学习并实现了解码器层的类: DecoderLayer</p><ul><li>类的初始化函数的参数有5个, 分别是size，代表词嵌入的维度大小,  同时也代表解码器层的尺寸，第二个是self_attn，多头自注意力对象，也就是说这个注意力机制需要Q=K=V，第三个是src_attn，多头注意力对象，这里Q!=K=V，  第四个是前馈全连接层对象，最后就是droupout置0比率.</li><li>forward函数的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</li><li>最终输出了由编码器输入和目标数据一同作用的特征提取结果.</li></ul><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用类Decoder来实现解码器</span><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N."""</span>        super<span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 首先使用clones方法克隆了N个layer，然后实例化了一个规范化层. </span>        <span class="token comment" spellcheck="true"># 因为数据走过了所有的解码器层后最后要做规范化处理. </span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> clones<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，           source_mask, target_mask代表源数据和目标数据的掩码张量"""</span>        <span class="token comment" spellcheck="true"># 然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，</span>        <span class="token comment" spellcheck="true"># 得出最后的结果，再进行一次规范化返回即可. </span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#一层一层往里面放</span>            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#x：目标数据的嵌入表示，要将x依次经历所有的编码器处理，最后在规范化层memory：编码器的输出张量，source_mask：源码数据的掩码张量，target_mask：目标数据的掩码张量</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p>实例化参数:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 分别是解码器层layer和解码器层的个数N</span>size <span class="token operator">=</span> <span class="token number">512</span>d_model <span class="token operator">=</span> <span class="token number">512</span>head <span class="token operator">=</span> <span class="token number">8</span>d_ff <span class="token operator">=</span> <span class="token number">64</span>dropout <span class="token operator">=</span> <span class="token number">0.2</span>c <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopyattn <span class="token operator">=</span> MultiHeadedAttention<span class="token punctuation">(</span>head<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>ff <span class="token operator">=</span> PositionwiseFeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>layer <span class="token operator">=</span> DecoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>ff<span class="token punctuation">)</span><span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>N <span class="token operator">=</span> <span class="token number">8</span></code></pre><p>输入参数:</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 输入参数与解码器层的输入参数相同</span>x <span class="token operator">=</span> pe_resultmemory <span class="token operator">=</span> en_resultmask <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>source_mask <span class="token operator">=</span> target_mask <span class="token operator">=</span> mask<span class="token comment" spellcheck="true">#掩码张量都表示一样</span></code></pre><p>调用:</p><pre class=" language-python"><code class="language-python">de <span class="token operator">=</span> Decoder<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span>de_result <span class="token operator">=</span> de<span class="token punctuation">(</span>x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>de_result<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>de_result<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></code></pre><ul><li><p>解码器总结:</p><ul><li>学习了解码器的作用:<ul><li>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示.</li></ul></li></ul><hr><ul><li>学习并实现了解码器的类: Decoder<ul><li>类的初始化函数的参数有两个，第一个就是解码器层layer，第二个是解码器层的个数N.</li><li>forward函数中的参数有4个，x代表目标数据的嵌入表示，memory是编码器层的输出，src_mask, tgt_mask代表源数据和目标数据的掩码张量.</li><li>输出解码过程的最终特征表示.</li></ul></li></ul></li></ul><hr><h2 id="输出部分实现"><a href="#输出部分实现" class="headerlink" title="输出部分实现"></a>输出部分实现</h2><ul><li>输出部分包含:<ul><li>线性层</li><li>softmax层</li></ul></li></ul><p><img src="http://121.199.45.168:8001/img/6.png" alt="avatar"></p><h3 id="线性层的作用"><a href="#线性层的作用" class="headerlink" title="线性层的作用"></a>线性层的作用</h3><ul><li>通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.</li></ul><h3 id="softmax层的作用"><a href="#softmax层的作用" class="headerlink" title="softmax层的作用"></a>softmax层的作用</h3><ul><li>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</li></ul><hr><ul><li>线性层和softmax层的代码分析:</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># nn.functional工具包装载了网络层中那些只进行计算, 而没有参数的层</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token comment" spellcheck="true"># 将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构</span><span class="token comment" spellcheck="true"># 因此把类的名字叫做Generator, 生成器类</span><span class="token keyword">class</span> <span class="token class-name">Generator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小."""</span>        super<span class="token punctuation">(</span>Generator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 首先就是使用nn中的预定义线性层进行实例化, 得到一个对象self.project等待使用, </span>        <span class="token comment" spellcheck="true"># 这个线性层的参数有两个, 就是初始化函数传进来的两个参数: d_model, vocab_size</span>        self<span class="token punctuation">.</span>project <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""前向逻辑函数中输入是上一层的输出张量x"""</span>        <span class="token comment" spellcheck="true"># 在函数中, 首先使用上一步得到的self.project对x进行线性变化, </span>        <span class="token comment" spellcheck="true"># 然后使用F中已经实现的log_softmax进行的softmax处理.</span>        <span class="token comment" spellcheck="true"># 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.</span>        <span class="token comment" spellcheck="true"># log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数, </span>        <span class="token comment" spellcheck="true"># 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.</span>        <span class="token keyword">return</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>project<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><ul><li>nn.Linear演示:</li></ul><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> output <span class="token operator">=</span> m<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><hr><blockquote><ul><li>实例化参数:</li></ul></blockquote><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 词嵌入维度是512维</span>d_model <span class="token operator">=</span> <span class="token number">512</span><span class="token comment" spellcheck="true"># 词表大小是1000</span>vocab_size <span class="token operator">=</span> <span class="token number">1000</span></code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 输入x是上一层网络的输出, 我们使用来自解码器层的输出</span>x <span class="token operator">=</span> de_result</code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class=" language-python"><code class="language-python">gen <span class="token operator">=</span> Generator<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>gen_result <span class="token operator">=</span> gen<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>gen_result<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>gen_result<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用EncoderDecoder类来实现编码器-解码器结构</span><span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> source_embed<span class="token punctuation">,</span> target_embed<span class="token punctuation">,</span> generator<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""初始化函数中有5个参数, 分别是编码器对象, 解码器对象,            源数据嵌入函数, 目标数据嵌入函数,  以及输出部分的类别生成器对象        """</span>        super<span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 将参数传入到类中</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder        self<span class="token punctuation">.</span>src_embed <span class="token operator">=</span> source_embed        self<span class="token punctuation">.</span>tgt_embed <span class="token operator">=</span> target_embed        self<span class="token punctuation">.</span>generator <span class="token operator">=</span> generator    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> source<span class="token punctuation">,</span> target<span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""在forward函数中，有四个参数, source代表源数据, target代表目标数据,            source_mask和target_mask代表对应的掩码张量"""</span>        <span class="token comment" spellcheck="true"># 在函数中, 将source, source_mask传入编码函数, 得到结果后,</span>        <span class="token comment" spellcheck="true"># 与source_mask，target，和target_mask一同传给解码函数.</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>self<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>source<span class="token punctuation">,</span> source_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span>                            target<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span>         <span class="token comment" spellcheck="true">#self.encode(source, source_mask)：编码一个张量</span>         <span class="token comment" spellcheck="true">#self.decode(self.encode(source, source_mask), source_mask,</span>                            target<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span>解码    <span class="token keyword">def</span> <span class="token function">encode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> source<span class="token punctuation">,</span> source_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""编码函数, 以source和source_mask为参数"""</span>        <span class="token comment" spellcheck="true"># 使用src_embed对source做处理，原数据的嵌入操作, 然后和source_mask一起传给self.encoder</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>self<span class="token punctuation">.</span>src_embed<span class="token punctuation">(</span>source<span class="token punctuation">)</span><span class="token punctuation">,</span> source_mask<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span> target<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""解码函数, 以memory即编码器的输出，整个编码器的返回结果, source_mask：原数据的掩码张量, target：目标数据, target_mask：目标数据的掩码张量为参数"""</span>        <span class="token comment" spellcheck="true"># 使用tgt_embed对target做处理, 然后和source_mask, target_mask, memory一起传给self.decoder</span>                        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tgt_embed<span class="token punctuation">(</span>target<span class="token punctuation">)</span><span class="token punctuation">,</span> memory<span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#tgt_embed(target)：词嵌入，memory：编码器的输出张量，source_mask：原数据的掩码张量，target_mask：目标数据的掩码张量</span></code></pre><blockquote><ul><li>实例化参数</li></ul></blockquote><pre class=" language-python"><code class="language-python">vocab_size <span class="token operator">=</span> <span class="token number">1000</span>d_model <span class="token operator">=</span> <span class="token number">512</span>encoder <span class="token operator">=</span> en<span class="token comment" spellcheck="true">#编码器：实例化对象</span>decoder <span class="token operator">=</span> de<span class="token comment" spellcheck="true">#解码器：</span>source_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#vocab_size：总词汇，d_model：词嵌入维度</span>target_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>generator <span class="token operator">=</span> gen</code></pre><hr><blockquote><ul><li>输入参数:</li></ul></blockquote><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 假设源数据与目标数据相同, 实际中并不相同</span>source <span class="token operator">=</span> target <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">421</span><span class="token punctuation">,</span> <span class="token number">508</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">491</span><span class="token punctuation">,</span> <span class="token number">998</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">221</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 假设src_mask与tgt_mask相同，实际中并不相同</span>source_mask <span class="token operator">=</span> target_mask <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><hr><blockquote><ul><li>调用:</li></ul></blockquote><pre class=" language-python"><code class="language-python">ed <span class="token operator">=</span> EncoderDecoder<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> source_embed<span class="token punctuation">,</span> target_embed<span class="token punctuation">,</span> generator<span class="token punctuation">)</span>ed_result <span class="token operator">=</span> ed<span class="token punctuation">(</span>source<span class="token punctuation">,</span> target<span class="token punctuation">,</span> source_mask<span class="token punctuation">,</span> target_mask<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>ed_result<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>ed_result<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></code></pre><h3 id="Tansformer模型构建过程的代码分析"><a href="#Tansformer模型构建过程的代码分析" class="headerlink" title="Tansformer模型构建过程的代码分析"></a>Tansformer模型构建过程的代码分析</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">make_model</span><span class="token punctuation">(</span>source_vocab<span class="token punctuation">,</span> target_vocab<span class="token punctuation">,</span> N<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>                d_model<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> d_ff<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> head<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""该函数用来构建模型, 有7个参数，分别是    source_vocab：源数据特征(词汇)总数，    target_vocab：目标数据特征(词汇)总数，    N：编码器和解码器堆叠数，    d_model：词向量映射维度，    d_ff：前馈全连接网络中变换矩阵的维度，    head：多头注意力结构中的多头数，以及    置零比率dropout."""</span>    <span class="token comment" spellcheck="true"># 首先得到一个深度拷贝命令，接下来很多结构都需要进行深度拷贝，</span>    <span class="token comment" spellcheck="true"># 来保证他们彼此之间相互独立，不受干扰.</span>    c <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy    <span class="token comment" spellcheck="true"># 实例化了多头注意力类，得到对象attn</span>    attn <span class="token operator">=</span> MultiHeadedAttention<span class="token punctuation">(</span>head<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 然后实例化前馈全连接类，得到对象ff </span>    ff <span class="token operator">=</span> PositionwiseFeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 实例化位置编码类，得到对象position</span>    position <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 根据结构图, 最外层是EncoderDecoder，在EncoderDecoder中，</span>    <span class="token comment" spellcheck="true"># 分别是编码器层，解码器层，源数据Embedding层和位置编码组成的有序结构，</span>    <span class="token comment" spellcheck="true"># 目标数据Embedding层和位置编码组成的有序结构，以及类别生成器层. </span>    <span class="token comment" spellcheck="true"># 在编码器层中有attention子层以及前馈全连接子层，</span>    <span class="token comment" spellcheck="true"># 在解码器层中有两个attention子层以及前馈全连接层.</span>    model <span class="token operator">=</span> EncoderDecoder<span class="token punctuation">(</span>        Encoder<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>ff<span class="token punctuation">)</span><span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">,</span>        Decoder<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span>                              c<span class="token punctuation">(</span>ff<span class="token punctuation">)</span><span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token comment" spellcheck="true">#编码器子层</span>        nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Embeddings<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> source_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>position<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Embeddings<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> target_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>position<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        Generator<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> target_vocab<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 模型结构完成后，接下来就是初始化模型中的参数，比如线性层中的变换矩阵</span>    <span class="token comment" spellcheck="true"># 这里一但判断参数的维度大于1，则会将其初始化成一个服从均匀分布的矩阵，</span>    <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> p<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform<span class="token punctuation">(</span>p<span class="token punctuation">)</span>    <span class="token keyword">return</span> model</code></pre><ul><li><p>学习并实现了编码器-解码器结构的类: EncoderDecoder</p><ul><li>类的初始化函数传入5个参数, 分别是编码器对象, 解码器对象, 源数据嵌入函数, 目标数据嵌入函数, 以及输出部分的类别生成器对象.</li><li>类中共实现三个函数, forward, encode, decode</li><li>forward是主要逻辑函数, 有四个参数, source代表源数据, target代表目标数据, source_mask和target_mask代表对应的掩码张量.</li><li>encode是编码函数, 以source和source_mask为参数. </li><li>decode是解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数</li></ul><hr></li><li><p>学习并实现了模型构建函数: make_model</p><ul><li>有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，多头注意力结构中的多头数，以及置零比率dropout.</li><li>该函数最后返回一个构建好的模型对象.</li></ul></li></ul><h2 id="模型基本测试运行"><a href="#模型基本测试运行" class="headerlink" title="模型基本测试运行"></a>模型基本测试运行</h2><ul><li>我们将通过一个小的copy任务完成模型的基本测试工作.</li></ul><hr><ul><li>copy任务介绍:<ul><li>任务描述: 针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</li><li>任务意义: copy任务在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力.  </li></ul></li></ul><hr><h3 id="使用copy任务进行模型基本测试的四步曲"><a href="#使用copy任务进行模型基本测试的四步曲" class="headerlink" title="使用copy任务进行模型基本测试的四步曲"></a>使用copy任务进行模型基本测试的四步曲</h3><ul><li>第一步: 构建数据集生成器</li><li>第二步: 获得Transformer模型及其优化器和损失函数</li><li>第三步: 运行模型进行训练和评估</li><li>第四步: 使用模型进行贪婪解码</li></ul><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/6.png' height=px ><p>增加小数点整数不能和folt类型相乘</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Transformer架构解析&quot;&gt;&lt;a href=&quot;#Transformer架构解析&quot; class=&quot;headerlink&quot; title=&quot;Transformer架构解析&quot;&gt;&lt;/a&gt;Transformer架构解析&lt;/h1&gt;&lt;p&gt;Transformer总体架构可分为</summary>
      
    
    
    
    <category term="深度学习" scheme="https://xiaoyvlongoing.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://xiaoyvlongoing.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow代码搭建运用基础</title>
    <link href="https://xiaoyvlongoing.github.io/2020/11/05/TensorFlow%E4%BB%A3%E7%A0%81%E6%90%AD%E5%BB%BA%E8%BF%90%E7%94%A8%E5%9F%BA%E7%A1%80/"/>
    <id>https://xiaoyvlongoing.github.io/2020/11/05/TensorFlow%E4%BB%A3%E7%A0%81%E6%90%AD%E5%BB%BA%E8%BF%90%E7%94%A8%E5%9F%BA%E7%A1%80/</id>
    <published>2020-11-05T06:52:21.000Z</published>
    <updated>2021-11-05T06:54:52.041Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TensorFlow代码搭建运用基础"><a href="#TensorFlow代码搭建运用基础" class="headerlink" title="TensorFlow代码搭建运用基础"></a>TensorFlow代码搭建运用基础</h1><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from pyitcast.transformer_utils import <strong>get_std_opt</strong></td><td>该工具用于获得标准的针对Transformer模型的优化器，该标准优化器基于Adam优化器, 使其对序列到序列的任务更有效.</td></tr><tr><td>from pyitcast.transformer_utils import <strong>LabelSmoothing</strong></td><td>标签平滑工具包, 该工具用于标签平滑, 标签平滑的作用就是小幅度的改变原有标签值的值域</td></tr><tr><td>from pyitcast.transformer_utils import <strong>SimpleLossCompute</strong></td><td>导入损失计算工具包, 该工具能够使用标签平滑后的结果进行损失的计算, 损失的计算方法可以认为是交叉熵损失函数.</td></tr><tr><td></td><td></td></tr><tr><td>from pyitcast.transformer_utils import <strong>run_epoch</strong></td><td>该工具将对模型使用给定的损失函数计算方法进行单轮参数更新. # 并打印每轮参数更新的损失结果.</td></tr><tr><td>from pyitcast.transformer_utils import <strong>greedy_decode</strong></td><td>贪婪解码的方式是每次预测都选择概率最大的结果作为输出,</td></tr></tbody></table><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from pyitcast.transformer import <strong>TransformerModel</strong></td><td>数据集</td></tr><tr><td></td><td></td></tr><tr><td><strong>定义数据类型以及用于转换为Tensor的指令</strong>。字段类对可以由张量表示的常见文本处理数据类型进行建模。  它包含一个Vocab对象，该对象定义了字段元素及其对应的数字表示形式的可能值的集合。  Field对象还保存其他与应如何对数据类型进行数字化有关的参数，例如令牌化方法和应产生的Tensor类型。 如果在数据集中的两个列之间共享字段（例如，在QA数据集中的问题和答案），则它们将具有共享的词汇表。</td><td></td></tr><tr><td>TEXT = torchtext.data.Field(<strong>tokenize=get_tokenizer</strong>(“切分的预料句”),<strong>init_token=</strong>‘<sos>起始’, <strong>eos_token=</strong>‘<eos>结束’,<strong>lower=True</strong>)</td><td><strong>tokenize</strong> –用于使用此字段将字符串标记为顺序示例的函数。 如果为“ spacy”，则使用SpaCy标记器。 如果将不可序列化的函数作为参数传递，则该字段将无法序列化。 默认值：string.split。<strong>init_token</strong> –使用该字段的每个示例之前都将添加一个令牌，如果没有初始令牌，则为None。 默认值：无。<strong>eos_token</strong> –使用此字段将附加到每个示例的令牌，或无则表示无句尾令牌。 默认值：无。</td></tr><tr><td></td><td></td></tr><tr><td>train_txt, val_txt, test_txt  = torchtext.datasets.WikiText2.splits（TEXT）</td><td>WikiText2：数据文本库，splits：分割，并切分为对应训练文本, 验证文本，测试文本, 并对这些文本施加刚刚创建的语料域</td></tr><tr><td></td><td></td></tr><tr><td><strong>WikiText2数据集</strong>：这是使用数据集的最简单方法，并假定字段，词汇表和迭代器参数具有通用默认值。</td><td></td></tr><tr><td><code>torchtext.datasets.``WikiText2</code>（ <em>path</em> ， <em>text_field</em> ， <em>newline_eos = True</em> ， <em>encoding =’utf-8’</em> ， *** kwargs* ）：为WikiText-2数据集的拆分创建迭代器对象。</td><td>参数： <strong>batch_size</strong> –批次大小。<strong>bptt_len</strong> –随时间反向传播的序列长度。<strong>设备</strong> –用于在其上创建批处理的设备。 对于CPU使用-1，对于当前活动的GPU设备使用无，<strong>root</strong> –数据集的zip存档将被扩展到的根目录； 因此，将在其wikitext-2子目录中存储数据文件的目录，<strong>wv_type，wv_dim</strong> （ <em>wv_dir</em> <em>，</em> ）–传递给文本字段的Vocab构造函数。 单词向量可以通过train.dataset.fields [‘text’]。vocab.vectors访问。 <strong>keyword arguments</strong> （ <em>剩余</em> ）–传递给splits方法</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><code>splits</code>（ <em>text_field</em> ， <em>root =’。data’</em> ， <em>train =’wiki.train.tokens’</em> ， <em>validation =’wiki.valid.tokens’</em> ， <em>test =’wiki.test.tokens’</em> ， *** kwargs* ）** <strong>数据集的分割</strong></td><td><strong>text_field</strong> –将用于文本数据的字段。                     <strong>root</strong> –数据集的zip存档将被扩展到的根目录； 因此，将在其wikitext-2子目录中存储数据文件的目录。                                                         <strong>train</strong> –火车数据的文件名。 默认值：“ wiki.train.tokens”。                                                   <strong>validation</strong>-验证数据的文件名，或“无”不加载验证集。 默认值：“ wiki.valid.tokens”。                                            <strong>test</strong> –测试数据的文件名，或“无”不加载测试集。 默认值：“ wiki.test.tokens”</td></tr><tr><td></td><td></td></tr><tr><td>build_vocab（<strong>train_txt：数据集</strong>，<strong>max_size=10000</strong>) #将词汇表中频率前10000的单词取出来，<strong>min_freq=10</strong>) #将词汇表中频率大于等于10的单词取出来）</td><td>build_vocab方法中这样可以使用vocab对象的stoi方法统计文本共包含的不重复词汇总数.</td></tr><tr><td></td><td></td></tr></tbody></table><p>新增函数</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>numericalize（）</td><td>方法将单词映射成对应的连续数字</td></tr><tr><td></td><td></td></tr><tr><td><code>torch.``narrow</code>（ *<em>input</em>, <em>dim</em>, <em>start</em>, <em>length</em> ）</td><td><strong>input</strong>（ <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>张量</em> </a>）–要 的张量 缩小  <strong>dim</strong> （ <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em> </a>）– 的尺寸 缩小  <strong>start</strong> （ <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em> </a>）–起始尺寸  <strong>length</strong> （ <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em> </a>）–到最终尺寸的距离</td></tr><tr><td>data = data.narrow(0, 0, nbatch * bsz)</td><td>之后使用narrow方法对不规整的剩余数据进行删除,     # 第一个参数是代表横轴删除还是纵轴删除, 0为横轴，1为纵轴     # 第二个和第三个参数代表保留开始轴到结束轴的数值.类似于切片</td></tr><tr><td></td><td></td></tr><tr><td><code>contiguous</code>(<em>memory_format=torch.contiguous_format</em>)连续 的内存张量，用了转置后要用</td><td><strong>memory_format</strong> （ <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.memory_format"><code>torch.memory_format</code></a>（可选）–返回的Tensor所需的内存格式。  默认：  <code>torch.contiguous_format</code>.</td></tr><tr><td></td><td>返回一个 连续 的内存张量，其中包含与以下内容相同的数据 <code>self</code>张量。  如果<br/><code>self</code> 张量已经是指定的内存格式，此函数返回<br/><code>self</code> 张量。</td></tr></tbody></table><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>裁剪参数可迭代的梯度范数。 范数是在所有梯度上一起计算的，就好像它们被串联到单个矢量中一样。  渐变就地修改，</td><td>使用nn自带的clip_grad_norm_方法进行梯度规范化, 防止出现梯度消失或爆炸</td></tr><tr><td><code>torch.nn.utils.``clip_grad_norm_</code>（ <em>parameters</em> ， <em>max_norm</em> ， <em>norm_type = 2.0</em></td><td><strong>parameters</strong>（ <em>Iterable</em> <em>[* <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em> </a></em>]或* <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em> </a>）–张量或单个Tensor的可迭代量，将对梯度进行归一化<strong>max_norm</strong> （ <a href="https://docs.python.org/3/library/functions.html#float"><em>float</em> </a><em>或</em> <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em> </a>）–渐变的最大范数<strong>norm_type</strong> （ <a href="https://docs.python.org/3/library/functions.html#float"><em>float</em> </a><em>或</em> <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em> </a>）–使用的p范数的类型。 可 <code>&#39;inf&#39;</code> 为无穷范数</td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;TensorFlow代码搭建运用基础&quot;&gt;&lt;a href=&quot;#TensorFlow代码搭建运用基础&quot; class=&quot;headerlink&quot; title=&quot;TensorFlow代码搭建运用基础&quot;&gt;&lt;/a&gt;TensorFlow代码搭建运用基础&lt;/h1&gt;&lt;table&gt;
&lt;</summary>
      
    
    
    
    <category term="深度学习" scheme="https://xiaoyvlongoing.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://xiaoyvlongoing.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>seaborn绘制统计图形</title>
    <link href="https://xiaoyvlongoing.github.io/2020/11/05/seaborn%E7%BB%98%E5%88%B6%E7%BB%9F%E8%AE%A1%E5%9B%BE%E5%BD%A2/"/>
    <id>https://xiaoyvlongoing.github.io/2020/11/05/seaborn%E7%BB%98%E5%88%B6%E7%BB%9F%E8%AE%A1%E5%9B%BE%E5%BD%A2/</id>
    <published>2020-11-05T06:50:50.000Z</published>
    <updated>2021-11-05T06:55:32.871Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-1seaborn绘制统计图形"><a href="#1-1seaborn绘制统计图形" class="headerlink" title="1.1seaborn绘制统计图形"></a>1.1seaborn绘制统计图形</h2><h4 id="绘制单变量分布"><a href="#绘制单变量分布" class="headerlink" title="绘制单变量分布"></a>绘制单变量分布</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>seaborn.<strong>set</strong>()</td><td>获取默认绘图</td></tr><tr><td>seaborn.<strong>distplot</strong>( a,                  bins=None, hist=True, kde=True, rug=False, fit=None, color=None)</td><td>(1) a：表示要观察的数据，可以是 Series、一维数组或列表。                                                                               (2) bins：用于控制条形的数量 。                                                                                            (3) hist：接收布尔类型，表示是否绘制(标注)直方图。                                                               (4) kde：接收布尔类型，表示是否绘制高斯核密度估计曲线。                                                                                  (5) rug：接收布尔类型，表示是否在支持的轴方向上绘制rugplot。</td></tr><tr><td></td><td></td></tr><tr><td><strong>设置面板风格主题</strong></td><td></td></tr><tr><td>sns.set_style(“”)</td><td>设置面板风格主题</td></tr><tr><td>“whitegrid”</td><td>白色网格</td></tr><tr><td>“darkgrid”</td><td>灰色网格</td></tr></tbody></table><h4 id="标签类别数量的统计"><a href="#标签类别数量的统计" class="headerlink" title="标签类别数量的统计"></a>标签类别数量的统计</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>sns.<strong>countplot</strong>(data.<strong>target</strong>)</td><td>查看类别数量的柱状图统计</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="绘制双变量分布"><a href="#绘制双变量分布" class="headerlink" title="绘制双变量分布"></a>绘制双变量分布</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>seaborn.<strong>jointplot</strong>(x, y, data=None,                                                                                                              kind=’scatter’, stat_func=None, color=None,                                                  ratio=5, space=0.2, dropna=True)</td><td>(1) kind：表示绘制图形的类型，  默认不写为散点图                                            (2) stat_func：用于计算有关关系的统计量并标注图。                                        (3) color：表示绘图元素的颜色。                                                                          (4) size：用于设置图的大小(正方形)。                                                                   (5) ratio：表示中心图与侧边图的比例。该参数的值越大，则中心图的占比会越大。                                                                                                                (6) space：用于设置中心图与侧边图的间隔大小。</td></tr><tr><td><strong>kind绘制的类型有：</strong></td><td></td></tr><tr><td>kind = “<strong>hex</strong>”</td><td>二维直方图</td></tr><tr><td>kind = “<strong>kde</strong>”</td><td>核密度估计图</td></tr><tr><td></td><td></td></tr></tbody></table><p><strong>程序</strong>：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns<span class="token comment" spellcheck="true"># 创建DataFrame对象</span>dataframe_obj <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"x": np.random.randn(500),"y": np.random.randn(500)&amp;#125;)</span><span class="token comment" spellcheck="true"># 绘制散布图</span>sns<span class="token punctuation">.</span>jointplot<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token string">"x"</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token string">"y"</span><span class="token punctuation">,</span> data<span class="token operator">=</span>dataframe_obj，kind<span class="token operator">=</span>“”<span class="token punctuation">)</span></code></pre><h2 id="二维的散点图"><a href="#二维的散点图" class="headerlink" title="二维的散点图"></a>二维的散点图</h2><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>seaborn.<strong>lmplot</strong>()</td><td><strong>绘制二维散点图自行完成回归拟合：</strong>                                                                                                        x, y 分别代表横纵坐标的列名,                                                                                                           data= 是关联到数据集,                                                                                                                        hue=*代表按照 species即花的类别分类显示什么颜色表示什么类型,                                                                                                  fit_reg=是否进行线性拟合。</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="绘制成对的双变量分布："><a href="#绘制成对的双变量分布：" class="headerlink" title="绘制成对的双变量分布："></a>绘制成对的双变量分布：</h3><h3 id="如直接进行读文件：把对应的数据文件导入的seaborn-data文件中才能进行读取里面的文件进行操作："><a href="#如直接进行读文件：把对应的数据文件导入的seaborn-data文件中才能进行读取里面的文件进行操作：" class="headerlink" title="如直接进行读文件：把对应的数据文件导入的seaborn-data文件中才能进行读取里面的文件进行操作："></a>如直接进行读文件：把对应的数据文件导入的seaborn-data文件中才能进行读取里面的文件进行操作：</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>直接进行读文件：</strong></td><td></td></tr><tr><td>变量 = sns.<strong>load_dataset</strong>(“数据文档名”)</td><td>直接进行读文件：把对应的数据文件导入的<strong>seaborn</strong>-<strong>data</strong>文件中才能进行读取里面的文件进行操作</td></tr><tr><td>sns.<strong>pairplot</strong>(变量)</td><td>绘制多个成对的双变量分布</td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="2-1用分类数据进行绘图"><a href="#2-1用分类数据进行绘图" class="headerlink" title="2.1用分类数据进行绘图"></a>2.1用分类数据进行绘图</h2><h4 id="类别散点图"><a href="#类别散点图" class="headerlink" title="类别散点图"></a>类别散点图</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>seaborn.<strong>stripplot</strong>(x=None, y=None, hue=None, data=None, order=None, hue_order=None, jitter=False)</td><td><strong>绘制类别散点图</strong>：                                                                                                                  (1) x（可以写入行标），y（可以写入行标），                                       hue：用于绘制长格式数据的输入。                                                      (2) data：用于绘制的数据集。如果x和y不存在，则它将作为宽格式，否则将作为长格式。                                                                                                                       (3) jitter：表示抖动的程度(仅沿类別轴)。当很多数据点重叠时，可以指定抖动的数量或者设为Tue使用默认值。</td></tr><tr><td>sns.<strong>swarmplot</strong>(x=”day”, y=”total_bill”, data=tips)</td><td><strong>绘制类别散点图</strong>：数据不会重叠清晰观察数据的分布情况</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="类别内的数据分布"><a href="#类别内的数据分布" class="headerlink" title="类别内的数据分布"></a>类别内的数据分布</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>绘制箱型图</strong></td><td>是一种用作显示一组数据分散情况资料的统计图。因形状如箱子而得名。<strong>它能显示出一组数据的最大值、最小值、中位数、及上下四分位数。</strong></td></tr><tr><td>seaborn.<strong>boxplot</strong>(x=None, y=None, hue=None, data=None, orient=None, color=None,  saturation=0.75, width=0.8)</td><td><strong>箱型图</strong>：                                                                                                                    (1) palette：用于设置不同级别色相的颜色变量。—- palette=[“r”,”g”,”b”,”y”]                                                                                                         (2) saturation：用于设置数据显示的颜色饱和度。—- 使用小数表示</td></tr><tr><td></td><td></td></tr><tr><td><strong>绘制提琴图</strong></td><td></td></tr><tr><td>sns.<strong>violinplot</strong>(x=””, y=””, data=)</td><td><strong>提琴图</strong>：</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="类别内的统计估计"><a href="#类别内的统计估计" class="headerlink" title="类别内的统计估计"></a><strong>类别内的统计估计</strong></h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>统计条形图</strong></td><td></td></tr><tr><td>sns.<strong>barplot</strong>(x=””,y=””,data=)</td><td>统计条形图</td></tr><tr><td>sns.<strong>pointplot</strong>(x=””,y=””,data=)</td><td>绘制点图</td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-1seaborn绘制统计图形&quot;&gt;&lt;a href=&quot;#1-1seaborn绘制统计图形&quot; class=&quot;headerlink&quot; title=&quot;1.1seaborn绘制统计图形&quot;&gt;&lt;/a&gt;1.1seaborn绘制统计图形&lt;/h2&gt;&lt;h4 id=&quot;绘制单变量分布&quot;&gt;</summary>
      
    
    
    
    <category term="python高级" scheme="https://xiaoyvlongoing.github.io/categories/python%E9%AB%98%E7%BA%A7/"/>
    
    
    <category term="python高级" scheme="https://xiaoyvlongoing.github.io/tags/python%E9%AB%98%E7%BA%A7/"/>
    
  </entry>
  
  <entry>
    <title>K近邻</title>
    <link href="https://xiaoyvlongoing.github.io/2020/11/05/K%E8%BF%91%E9%82%BB/"/>
    <id>https://xiaoyvlongoing.github.io/2020/11/05/K%E8%BF%91%E9%82%BB/</id>
    <published>2020-11-05T06:47:55.000Z</published>
    <updated>2021-11-05T06:55:04.351Z</updated>
    
    <content type="html"><![CDATA[<h2 id="k紧邻算法：k表示邻居"><a href="#k紧邻算法：k表示邻居" class="headerlink" title="k紧邻算法：k表示邻居"></a>k紧邻算法：k表示邻居</h2><h4 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from sklearn.neighbors import <strong>KNeighborsClassifier</strong></td><td>导入k紧邻包</td></tr><tr><td>from <strong>sklearn</strong>.<strong>model_selection</strong> import <strong>train_test_split</strong></td><td><strong>划分数据集</strong>，划分训练集和测试集</td></tr><tr><td>from <strong>sklearn</strong>.<strong>preprocessing</strong> import <strong>MinMaxScaler</strong></td><td><strong>导入归一化</strong>的包，形成（0,1）区间里面数值</td></tr><tr><td>from sklearn.preprocessing import <strong>StandardScaler</strong></td><td><strong>生成标准化</strong>可以预先生成规则，即生成<strong>训练集的均值与标准差</strong>，然后利用训练集的均值和标准差去分别标准化训练集和测试集！</td></tr><tr><td>from sklearn.<strong>model_selection</strong>  import  **GridSearchCV  **</td><td><strong>交叉验证网格搜索（模型选择与调优）API：</strong></td></tr><tr><td>from sklearn.model_selection import <strong>KFold</strong>,**StratifiedKFold  **</td><td><strong>交叉验证法：</strong>将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，得到n_splits个结果</td></tr><tr><td>from   sklearn.linear_model  import  <strong>LinearRegression</strong>(fit_intercept=True)</td><td><strong>正规方程</strong>，fit_intercept=True是否计算偏置</td></tr><tr><td>from   sklearn.linear_model  import   <strong>SGDRegressor</strong>（）</td><td><strong>随机梯度下降</strong></td></tr><tr><td>from sklearn.linear_model import <strong>Ridge</strong>, <strong>ElasticNet</strong>, <strong>Lasso，RidgeCV</strong></td><td><strong>正则化线性模型</strong>：复杂度或者异常数据点太多减少这个特征的影响</td></tr><tr><td>from sklearn.externals import <strong>joblib</strong></td><td><strong>模型的保存和加载</strong></td></tr><tr><td>from sklearn.linear_model import  <strong>LogisticRegression</strong></td><td><strong>逻辑回归</strong></td></tr><tr><td>from  sklearn.metrics  import    <strong>classification_report</strong></td><td><strong>精确率和召回率分类评估</strong></td></tr><tr><td>from sklearn.metrics import <strong>roc_auc_score</strong></td><td><strong>AUC</strong> <strong>评估</strong></td></tr><tr><td>from sklearn.tree  import   <strong>DecisionTreeClassifier</strong></td><td><strong>决策树</strong></td></tr><tr><td>from   sklearn.feature_extraction   import   <strong>DictVectorizer</strong></td><td><strong>字典的提取</strong>：主要解决离散型数据</td></tr><tr><td>from sklearn.tree import <strong>DecisionTreeRegressor</strong></td><td><strong>回归决策树</strong>：主要解决连续型数据</td></tr><tr><td>from imblearn.over_sampling import <strong>RandomOverSampler</strong></td><td><strong>过采样方法</strong></td></tr><tr><td>from imblearn.under_sampling import <strong>RandomUnderSampler</strong></td><td><strong>欠采样方法</strong>：                                                                                                   去除一些多数类中的样本使得正例、反例数目接近，然后再进行学习</td></tr><tr><td>from sklearn.feature_extraction.text  import <strong>CountVectorizer</strong>(stop_words=[])</td><td><strong>文本特征提取（英文）</strong></td></tr><tr><td>from sklearn.feature_extraction.text import  <strong>TfidfVectorizer</strong></td><td><strong>Tf-idf文本特征提取</strong>个词或短语出现的概率</td></tr><tr><td>from sklearn.ensemble  import  <strong>RandomForestClassifier</strong></td><td><strong>随机森林</strong></td></tr><tr><td>from sklearn.preprocessing import <strong>OneHotEncoder</strong></td><td>实现<strong>one=hot</strong>表示，就是把标签里面的对应的特征用最大概率等于1描述出来，有几个参与的特征就会有分出来几个对应的数，这些数里面都有对应的概率，取概率最大的让它等于1，要导入这个包</td></tr><tr><td>from sklearn.metrics import <strong>log_loss</strong></td><td>导入<strong>log</strong> <strong>计算方式</strong></td></tr><tr><td>from sklearn.preprocessing import <strong>LabelEncoder</strong></td><td><strong>把标签值转化为数值</strong>，也要用上.fit_transform进行数据的转化</td></tr><tr><td>from sklearn.ensemble import <strong>AdaBoostClassifier</strong></td><td>AdaBoost：分类器</td></tr><tr><td>from sklearn.cluster  import  <strong>KMeans</strong>(n_clusters=8)</td><td><strong>聚类</strong>：参数:                                                                                                             n_clusters:开始的聚类中心数量  整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。</td></tr><tr><td>from sklearn.metrics import <strong>calinski_harabaz_score</strong></td><td><strong>评价聚类效果</strong></td></tr><tr><td>from  sklearn.feature_selection  import  <strong>VarianceThreshold</strong></td><td><strong>删除所有低方差特征</strong></td></tr><tr><td>from scipy.stats import **pearsonr **</td><td><strong>皮尔逊相关系数</strong>：变量之间相关关系密切程度的统计指标                                                                                            x : (N,) array_like                                                                                         y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value)</td></tr><tr><td>from scipy.stats import <strong>spearmanr</strong></td><td><strong>斯皮尔曼相关系数</strong>：反映变量之间相关关系密切程度的统计指标</td></tr><tr><td>from  sklearn.decomposition  import  <strong>PCA</strong>(n_components=None)</td><td><strong>将数据分解为较低维数空间</strong>，                                                              <strong>n_components:</strong>                                                                                 <strong>小数：表示保留百分之多少的信息</strong>                                                          <strong>整数：减少到多少特征</strong></td></tr><tr><td>from  sklearn.naive_bayes   import  <strong>MultinomialNB</strong></td><td><strong>朴素贝叶斯算法</strong>：进行文本的分类</td></tr><tr><td>from sklearn import <strong>svm</strong></td><td><strong>支持向量机</strong></td></tr><tr><td>from hmmlearn import <strong>hmm</strong></td><td>HMM模型</td></tr><tr><td>from sklearn.model_selection import <strong>StratifiedShuffleSplit</strong></td><td>提供分层抽样功能，确保每个标签对应的样本的比例</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="K近邻的pai含义"><a href="#K近邻的pai含义" class="headerlink" title="K近邻的pai含义"></a>K近邻的pai含义</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from sklearn.neighbors import <strong>KNeighborsClassifier</strong></td><td>导入k紧邻包</td></tr><tr><td><strong>KNeighborsClassifier</strong>(n_neighbors=5，algorithm=’auto’)</td><td><strong>n_neighbors</strong>：int,可选（默认= 5），k_neighbors查询默认使用的邻居数</td></tr><tr><td><strong>algorithm=</strong>{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}使用哪个算法：</td><td>使用的有：brute是蛮力搜索，kd_tree，构造kd树，ball tree是为了克服kd树高维</td></tr><tr><td>默认参数为<strong>auto</strong>，可以理解为算法自己决定合适的搜索算法</td><td>为算法自己决定合适的搜索算法</td></tr><tr><td><strong>brute</strong>是蛮力搜索</td><td>也就是线性扫描，当训练集很大时，计算非常耗时。</td></tr><tr><td>kd_tree，构造kd树</td><td>构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。</td></tr><tr><td>ball_tree是为了克服kd树高维</td><td>为了克服kd树高维失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>fit</strong>（）</td><td>简单来说，就是求得训练集X的均值啊，方差啊，最大值啊，最小值啊这些训练集X固有的属性。可以理解为一个训练过程</td></tr><tr><td><strong>.predict()</strong></td><td>predict()方法：进行<strong>预测值</strong>，返回值是数值，表示样本属于每一个类别的概率，我们可以使用numpy.argmax()方法找到样本以最大概率所属的类别作为样本的预测标签。</td></tr></tbody></table><pre class=" language-python"><code class="language-python">训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_testx_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">,</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">22</span><span class="token punctuation">)</span></code></pre><h1 id="鸢尾花种类预测–数据集"><a href="#鸢尾花种类预测–数据集" class="headerlink" title="鸢尾花种类预测–数据集"></a>鸢尾花种类预测–数据集</h1><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>sklearn数据集返回值介绍</td><td>load和fetch返回的数据类型datasets.base.Bunch(字典格式)                                                   data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组 target：标签数组，是 n_samples 的一维 numpy.ndarray 数组 DESCR：数据描述 feature_names：特征名,新闻数据，手写数字、回归数据集没有 target_names：标签名</td></tr><tr><td>from <strong>sklearn</strong>.<strong>datasets</strong> import <strong>load_iris</strong></td><td><strong>sklearn</strong>.<strong>datasets</strong>:加载获取流行数据集                                                                              <strong>datasets</strong>.load*_*()  :获取小规模数据集，数据包含在datasets里 datasets.fetch_*(data_home=None)  获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/                                                                                                          <strong>load_iris</strong>:获取鸢尾花数据集</td></tr><tr><td>datasets*.<strong>load_*</strong>()  :获取小规模数据集，数据包含在datasets里 datasets.<strong>fetch_</strong>*(data_home=None)  获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/</td><td></td></tr><tr><td>sklearn.datasets.<strong>fetch_20newsgroups</strong>(data_home=None,subset=‘train’)获取大型数据集</td><td>subset：’train’或者’test’，’all’，可选，选择要加载的数据集。                                          训练集的“训练”，测试集的“测试”，两者的“全部”</td></tr></tbody></table><h2 id="数据集的划分训练的函数"><a href="#数据集的划分训练的函数" class="headerlink" title="数据集的划分训练的函数"></a>数据集的划分训练的函数</h2><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from <strong>sklearn</strong>.<strong>model_selection</strong> import <strong>train_test_split</strong>：</td><td><strong>train_test_split</strong>是sklearn中用于划分数据集，即将原始数据集划分成测试集和训练集两部分的函数。</td></tr><tr><td><strong>训练集特征值，测试特征值，训练目标值，，测试目标值，                 x_train, x_test, y_train, y_test      =    train_test_split</strong>(train_data,train_target,test_size=0.3,random_state=5)</td><td><strong>train_data</strong>：待划分样本数据<strong>train_target</strong>：待划分样本数据的结果（标签）                              <strong>test_size</strong>：测试数据占样本数据的比例，若整数则样本数量<strong>random_state</strong>：<strong>设置随机数种子</strong>，保证每次都是同一个随机数。若为0或不填，则每次得到数据都不一样</td></tr><tr><td>.target</td><td>查看标签类型</td></tr><tr><td>.data</td><td>查看所有的数据值</td></tr></tbody></table><h2 id="数据的特征预处理函数，特征处理"><a href="#数据的特征预处理函数，特征处理" class="headerlink" title="数据的特征预处理函数，特征处理"></a>数据的特征预处理函数，特征处理</h2><h4 id="数据的归一化："><a href="#数据的归一化：" class="headerlink" title="数据的归一化："></a>数据的归一化：</h4><p>归一化的转换公式：</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/EM%E7%AE%97%E6%B3%95-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/21.png' height=px ><h3 id="适合小数据型"><a href="#适合小数据型" class="headerlink" title="适合小数据型"></a>适合小数据型</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>数据的归一化</strong>：归一化的数值范围：<strong>feature_range=</strong>(0,1)：数值0到1之前的小数，可以设置成其他的数值，在这里是设置成0到1</td><td>（(当前值—最小值)/最大值—最小值）×  这里是0,1的范围所有（1—0）× 0</td></tr><tr><td>from <strong>sklearn</strong>.<strong>preprocessing</strong> import <strong>MinMaxScaler</strong></td><td><strong>导入归一化的包</strong></td></tr><tr><td>变量 = <strong>MinMaxScaler</strong> (feature_range=(0,1)… )</td><td>归一化的数值范围：<strong>feature_range=</strong>(0,1)</td></tr><tr><td></td><td></td></tr><tr><td>变量.<strong>fit_transform</strong>（）：是fit和transform的组合，既包括了训练又包含了转换。</td><td><strong>fit_transform</strong>():两个进行合成的                                                                                          <strong>fit</strong>：计算均值和标准差                                                                                 <strong>transform</strong>：相应的标准化的数据转换                                                               先拟合数据，在进行标准化，对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。</td></tr><tr><td><strong>fit</strong>(),<strong>transform</strong>()：不一样的定义，在独立同分布的时候要对其进行分开写入如<strong>特征降维</strong>：</td><td><strong>符合独立同分布的时候要分开写</strong>，                                                                              <strong>fit()：</strong>是对应训练集的里面的值，                                                                  <strong>transform()：</strong>对应着的是验证集里面的值</td></tr><tr><td>transform()</td><td>相应的标准化的数据转换</td></tr></tbody></table><h2 id="标准化：适合现在嘈杂大数据的场景"><a href="#标准化：适合现在嘈杂大数据的场景" class="headerlink" title="标准化：适合现在嘈杂大数据的场景"></a>标准化：适合现在嘈杂大数据的场景</h2><h4 id="定义：通过对原始的数据进行变换把数据变换到均值为0，标准差为1范围内"><a href="#定义：通过对原始的数据进行变换把数据变换到均值为0，标准差为1范围内" class="headerlink" title="定义：通过对原始的数据进行变换把数据变换到均值为0，标准差为1范围内"></a>定义：通过对原始的数据进行变换把数据变换到均值为0，标准差为1范围内</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/EM%E7%AE%97%E6%B3%95-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/20.png' height=px ><p><strong>对于标准化来说</strong>：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>标准化：</strong>                      .<strong>StandardScaler</strong>（）</td><td>可以预先生成规则，即生成<strong>训练集的均值与标准差</strong>，然后利用训练集的均值和标准差去分别标准化训练集和测试集！</td></tr><tr><td>StandardScaler.<strong>fit_transform</strong>(X)</td><td><strong>fit_transform</strong>():两个进行合成的                                                                                          <strong>fit</strong>：计算均值和标准差                                                                                    <strong>transform</strong>：相应的标准化的数据转换                                                                            X:numpy array格式的数据[n_samples,n_features]，                                                        返回值：转换后的形状相同的array</td></tr><tr><td>标准后变量的.<strong>mean_</strong></td><td>每一列特征的平均值</td></tr><tr><td>标准后变量的.<strong>var_</strong></td><td>每一列特征的方差</td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="交叉验证和网格搜索：可以选择出来最优的模型参数，如学习率，和跌代多少次"><a href="#交叉验证和网格搜索：可以选择出来最优的模型参数，如学习率，和跌代多少次" class="headerlink" title="交叉验证和网格搜索：可以选择出来最优的模型参数，如学习率，和跌代多少次"></a>交叉验证和网格搜索：可以选择出来最优的模型参数，如学习率，和跌代多少次</h2><p><strong>交叉验证是什么</strong>：将数据分成几份，其中<strong>一份做验证集</strong>，再经过几组的测试，每次都更换不同的验证集，经过几组的模型结果，<strong>取平均值最终结果</strong>，但是<strong>测试集没有分组</strong>，<strong>能在训练的模型中得到的结果更加稳定</strong>，但是<strong>不能提高模型的准确率</strong>。</p><p><strong>网格搜索：</strong>就是调整超参数，提高模型的<strong>准确率</strong>。</p><p><strong>网格搜索是什么：</strong>对模型预设几组超参数组合，<strong>每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型，</strong></p><p><strong>超参数：</strong>能手动<strong>指定参数都叫超参数</strong>，超参数能进行组合，能组合成最好模型，如果超参数多了，模型也跟着变多，网格搜索变多复杂，数据的计算那里也变的跟高了</p><p><strong>超参数与交叉验证进行计算有多少次</strong>：交叉验证的几组与超参数输入的值进行相乘，</p><table><thead><tr><th>函数</th><th>函数的性质</th></tr></thead><tbody><tr><td>StandardScaler（）</td><td>可以预先生成规则，即生成<strong>训练集的均值与标准差</strong>，然后利用训练集的均值和标准差去分别标准化训练集和测试集！</td></tr><tr><td><strong>交叉验证网格搜索（模型选择与调优）API</strong></td><td></td></tr><tr><td>sklearn.<strong>model_selection</strong>.<strong>GridSearchCV</strong>(estimator, param_grid=None,cv=None)</td><td><strong>交叉验证网格搜索（模型选择与调优）API：</strong>                            对估计器的指定参数值进行详尽搜索：                       <strong>estimator</strong>：<strong>传入估计器的对象</strong>，进行交叉验证，估计器对象，训练器                                   **param_grid{‘learning_rate’: [0.01, 0.1, 1]，“n_neighbors”:[1,3,5]} **      形成的每一轮学习率和迭代，和整体数据的迭代次数                                                                             <strong>cv</strong>：指定几折交叉验证                                                       *<em>fit（X_train, y_train))<em>*：输入训练数据                                                               <strong>score</strong>：准确率                                                                             结果分析：                                                                 ~~best<em>score__:在交叉验证中验证的最好结果</em> ~~best</em>_stimator*：可以选出最优的参数模型数，如多少次的迭代，和学习率打印出来                                                        ~~cv</em>results*:每次交叉验证后的验证集准确率结果和训练集准确率结果</td></tr><tr><td>.score（x_test,y_test）</td><td>准确率，衡量指标</td></tr><tr><td>被网格搜索后的变量.best_score_</td><td>在交叉验证中验证中可以选出最优的参数模型数，如多少次的迭代，和学习率打印出来</td></tr><tr><td>被网格搜索后的变量.best_estimator_</td><td>最好的参数模型</td></tr><tr><td>被网格搜索后的变量.cv_results_</td><td>每次交叉验证后的准确率结果</td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;k紧邻算法：k表示邻居&quot;&gt;&lt;a href=&quot;#k紧邻算法：k表示邻居&quot; class=&quot;headerlink&quot; title=&quot;k紧邻算法：k表示邻居&quot;&gt;&lt;/a&gt;k紧邻算法：k表示邻居&lt;/h2&gt;&lt;h4 id=&quot;导入包&quot;&gt;&lt;a href=&quot;#导入包&quot; class=&quot;he</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xiaoyvlongoing.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://xiaoyvlongoing.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>CV深度学习基础一</title>
    <link href="https://xiaoyvlongoing.github.io/2020/11/05/CV%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B8%80/"/>
    <id>https://xiaoyvlongoing.github.io/2020/11/05/CV%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B8%80/</id>
    <published>2020-11-05T03:00:01.000Z</published>
    <updated>2021-11-05T08:43:19.102Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1神经网络运用的"><a href="#1神经网络运用的" class="headerlink" title="1神经网络运用的"></a><strong>1</strong>神经网络运用的</h2><table><thead><tr><th>函数</th><th>函数表达作用</th></tr></thead><tbody><tr><td>random.seed(n)</td><td>如果使用相同的n值，则随机数生成函数每次生成的随机数序列都相同</td></tr><tr><td>np.random.seed(n)</td><td>以按顺序产生一组固定的数组，如果使用相同的n值，则每次生成的随机数都相同</td></tr><tr><td>numpy.array([])</td><td>创建一个数组</td></tr><tr><td>relu函数</td><td>激活函数通常指代以<a href="https://baike.baidu.com/item/%E6%96%9C%E5%9D%A1%E5%87%BD%E6%95%B0">斜坡函数</a>及其变种为代表的非线性函数</td></tr><tr><td>dot()</td><td>计算的是我们经常计算的矩阵乘法，设A(2 * 3), B(3 * 4), 那么dot(A, B)就表示两个矩阵相乘，得到的结果是一个2 * 4的矩阵。</td></tr><tr><td>Model（）</td><td><strong>模型：</strong>就是要显示、保存、创建、更新和删除的对象。</td></tr><tr><td>optimize（）</td><td>每一次的w，b更新都通过optimize函数取做</td></tr><tr><td>summary（）</td><td>函数是描述性统计分析，对于连续型变量</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="tf-where使用：检索索引-True元素"><a href="#tf-where使用：检索索引-True元素" class="headerlink" title="tf.where使用：检索索引 True元素"></a>tf.where使用：检索索引 <code>True</code>元素</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.where（condition, x=None, y=None, name=None）</td><td></td></tr><tr><td>返回其中的元素 <code>condition</code>是 <code>True</code>（多路复用 <code>x</code>和 <code>y</code>).</td><td></td></tr><tr><td></td><td></td></tr></tbody></table><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/1.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/2.png' height=px ><h4 id="tf-math-equal：按元素返回-x-y-的真值"><a href="#tf-math-equal：按元素返回-x-y-的真值" class="headerlink" title="tf.math.equal：按元素返回 (x == y) 的真值"></a>tf.math.equal：按元素返回 (x == y) 的真值</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.math.equal(     x, y, name=None )</td><td>X，y：一种 <a href="https://tensorflow.google.cn/api_docs/python/tf/Tensor"><code>tf.Tensor</code></a>或者 <a href="https://tensorflow.google.cn/api_docs/python/tf/sparse/SparseTensor"><code>tf.sparse.SparseTensor</code></a>或者 [<code>tf.IndexedSlices</code>]</td></tr><tr><td></td><td></td></tr></tbody></table><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/3.png' height=px ><h3 id="tf-compat-v1-scatter-update-：对变量进行更新。"><a href="#tf-compat-v1-scatter-update-：对变量进行更新。" class="headerlink" title="tf.compat.v1.scatter_update ：对变量进行更新。"></a>tf.compat.v1.scatter_update ：对变量进行更新。</h3><h5 id="函数函数信息tf-compat-v1-scatter-update-ref-indices-updates-use-locking-True-name-None"><a href="#函数函数信息tf-compat-v1-scatter-update-ref-indices-updates-use-locking-True-name-None" class="headerlink" title="函数函数信息tf.compat.v1.scatter_update(     ref, indices, updates, use_locking=True, name=None )"></a>函数函数信息tf.compat.v1.scatter_update(     ref, indices, updates, use_locking=True, name=None )</h5><table><thead><tr><th><code>ref</code></th><th>一种 <code>Variable</code>.</th></tr></thead><tbody><tr><td><code>indices</code></td><td>一种 <code>Tensor</code>.  必须是以下类型之一： <code>int32</code>,  <code>int64</code>.  第一维的索引张量 <code>ref</code>.</td></tr><tr><td><code>updates</code></td><td>一种 <code>Tensor</code>.  必须具有相同的类型 <code>ref</code>.  要存储的更新值的张量 <code>ref</code>.</td></tr><tr><td><code>use_locking</code></td><td>一个可选的 <code>bool</code>.  默认为 <code>True</code>.  如果为 True，则赋值将受锁保护；  否则行为是未定义的，但可能会表现出较少的争用。</td></tr><tr><td><code>name</code></td><td>操作的名称（可选）。</td></tr></tbody></table><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">import</span> tensorflow<span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>eager <span class="token keyword">as</span> tfetf<span class="token punctuation">.</span>enable_eager_execution<span class="token punctuation">(</span><span class="token punctuation">)</span>ref <span class="token operator">=</span> tfe<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>initial_value<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>indices <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>updates <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">98</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>update <span class="token operator">=</span> tf<span class="token punctuation">.</span>scatter_update<span class="token punctuation">(</span>ref<span class="token punctuation">,</span> indices<span class="token punctuation">,</span> updates<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>update<span class="token punctuation">)</span>打印<span class="token operator">&lt;</span>tf<span class="token punctuation">.</span>Variable <span class="token string">''</span> shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span> dtype<span class="token operator">=</span>int32<span class="token punctuation">,</span> numpy<span class="token operator">=</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">98</span><span class="token punctuation">,</span>  <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span>  <span class="token number">0</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">,</span>   <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">></span></code></pre><h4 id="tf-stop-gradient-：停止梯度计算。"><a href="#tf-stop-gradient-：停止梯度计算。" class="headerlink" title="tf.stop_gradient ：停止梯度计算。"></a>tf.stop_gradient ：停止梯度计算。</h4><pre class=" language-python"><code class="language-python">tf<span class="token punctuation">.</span>stop_gradient<span class="token punctuation">(</span>    input<span class="token punctuation">,</span> name<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><h4 id="tf-ragged-stack：进行拼接成"><a href="#tf-ragged-stack：进行拼接成" class="headerlink" title="tf.ragged.stack：进行拼接成"></a>tf.ragged.stack：进行拼接成</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/4.png' height=px ><h2 id="TensorFlow基础函数简介"><a href="#TensorFlow基础函数简介" class="headerlink" title="TensorFlow基础函数简介"></a>TensorFlow基础函数简介</h2><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.constant（）</td><td>创建张量</td></tr><tr><td>变量.numpy（），np.array（变量）</td><td>转换成numpy的类型</td></tr><tr><td>tf.Variable（）</td><td>变量是一种特殊的张量，形状是不可变，但可以更改其中的参数</td></tr><tr><td>tf. data.datasets</td><td>对于大型数据集或者要进行跨设备训练时使用来进行数据输入。</td></tr><tr><td></td><td></td></tr><tr><td>keras.utils.plot_model(model, to_file=’model.png’, show_shapes=False, show_layer_names=True, rankdir=’TB’, expand_nested=False, dpi=96)</td><td>将 Keras 模型转换为 dot 格式并保存到文件中：                                              <strong>参数：</strong>                                                                                                                 <strong>model</strong>: 一个 Keras 模型实例                                                                         <strong>to_file</strong>: 绘制图像的文件名                                                                              <strong>show_shapes</strong>: 是否显示尺寸信息                                             <strong>show_layer_names</strong>: 是否显示层的名称                                                    <strong>rankdir</strong>: 传递给 PyDot 的 <code>rankdir</code> 参数，一个指定绘图格式的字符串：’TB’ 创建一个垂直绘图；’LR’ 创建一个水平绘图                                                         <strong>expand_nested</strong>: 是否扩展嵌套模型为聚类                                                                                                                             <strong>dpi</strong>: 点 DPI</td></tr><tr><td>tf.ones（shape, dtype=tf.dtypes.float32, name=None）</td><td><strong>shape：</strong>整数列表、整数元组或int32类型的一维张量。                                     <strong>dtype ：</strong>结果张量中元素的可选数据类型。默认值为tf.32浮动                           <strong>name：</strong>名字                                                                                                          <strong>案例：</strong>tf.ones([2, 3], tf.int32) ==&gt; [[1, 1, 1], [1, 1, 1]]</td></tr><tr><td>tf.data.Dataset.from_tensor_slices（）</td><td><strong>创建一个数据集</strong>,<strong>其元素是给定张量的片段</strong>.                             <strong>tensors</strong>：张量的一种嵌套结构,在第0维度中各有相同的大小.                                                                                                 <strong>返回</strong>：                                                                                                       返回一个 Dataset.</td></tr><tr><td></td><td></td></tr><tr><td>torch.utils.data.TensorDataset(data_tensor, target_tensor)</td><td></td></tr></tbody></table><h1 id="神经网络构造代码重要函数："><a href="#神经网络构造代码重要函数：" class="headerlink" title="神经网络构造代码重要函数："></a>神经网络构造代码重要函数：</h1><h3 id="1：导入tf-keras"><a href="#1：导入tf-keras" class="headerlink" title="1：导入tf.keras"></a>1：导入tf.keras</h3><p>使用 <code>tf.keras</code>，首先需要在代码开始时导入<code>tf.keras</code></p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import tensorflow as tf</td><td></td></tr><tr><td>from tensorflow import keras</td><td></td></tr></tbody></table><h3 id="2-数据输入"><a href="#2-数据输入" class="headerlink" title="2.数据输入"></a>2.数据输入</h3><p>对于小的数据集，可以直接使用numpy格式的数据进行训练、评估模型，对于大型数据集或者要进行跨设备训练时使用tf.data.datasets来进行数据输入。</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.data.datasets</td><td>对于大型数据集或者要进行跨设备训练时使用来进行数据输入。</td></tr><tr><td></td><td></td></tr><tr><td><strong>.to_categorical</strong></td><td>将类向量（整数）转换为二进制类矩阵。</td></tr><tr><td>eras.utils.to_categorical(y, num_classes=None, dtype=’float32’)</td><td><strong>参数</strong>:                                                                                                                                 <strong>y</strong>: 需要转换成矩阵的类矢量(从 0 到 num_classes 的整数                                                                                                                            <strong>num_classes</strong>: 总类别数                                                                                                                           <strong>dtype</strong>: 字符串，输入所期望的数据类型 (<code>float32</code>, <code>float64</code>, <code>int32</code>…)</td></tr><tr><td>返回</td><td>输入的二进制矩阵表示。</td></tr><tr><td></td><td></td></tr><tr><td>tf.keras.datasets.mnist.load_data(     path=’mnist.npz’ )</td><td>path：本地缓存数据集的路径（相对于~/.keras/datasets）</td></tr></tbody></table><h3 id="3-模型构建-：https-keras-io-zh-getting-started-sequential-model-guide"><a href="#3-模型构建-：https-keras-io-zh-getting-started-sequential-model-guide" class="headerlink" title="3.模型构建    ：https://keras.io/zh/getting-started/sequential-model-guide/"></a>3.模型构建    ：<a href="https://keras.io/zh/getting-started/sequential-model-guide/">https://keras.io/zh/getting-started/sequential-model-guide/</a></h3><ul><li>简单模型使用Sequential进行构建</li><li>复杂模型使用函数式编程来构建</li><li>自定义layers</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 利用sequential方式构建模型</span>model <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>  <span class="token comment" spellcheck="true"># 隐藏层1，激活函数是relu,输入大小有input_shape指定</span>  Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># 隐藏层2，激活函数是relu</span>  Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 输出层</span>  Dense<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">"softmax"</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#通过Sequential构建</span><span class="token comment" spellcheck="true"># 导入相关的工具包</span><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow <span class="token keyword">import</span> keras<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> layers<span class="token comment" spellcheck="true"># 定义一个Sequential模型，包含3层</span>model <span class="token operator">=</span> keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    <span class="token punctuation">[</span>        <span class="token comment" spellcheck="true"># 第一层：激活函数为relu,权重初始化为he_normal</span>        layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span>                     kernel_initializer<span class="token operator">=</span><span class="token string">"he_normal"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"layer1"</span><span class="token punctuation">,</span>input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token comment" spellcheck="true"># 第二层：激活函数为relu,权重初始化为he_normal</span>        layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span>                     kernel_initializer<span class="token operator">=</span><span class="token string">"he_normal"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"layer2"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token comment" spellcheck="true"># 第三层（输出层）：激活函数为sigmoid,权重初始化为he_normal</span>        layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"sigmoid"</span><span class="token punctuation">,</span>                     kernel_initializer<span class="token operator">=</span><span class="token string">"he_normal"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"layer3"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    name<span class="token operator">=</span><span class="token string">"my_Sequential"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#function API构建</span><span class="token comment" spellcheck="true"># 导入工具包</span><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token comment" spellcheck="true"># 定义模型的输入</span>inputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name <span class="token operator">=</span> <span class="token string">"input"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第一层：激活函数为relu，其他默认</span>x <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span>name <span class="token operator">=</span> <span class="token string">"layer1"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第二层：激活函数为relu，其他默认</span>x <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span>name <span class="token operator">=</span> <span class="token string">"layer2"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 第三层（输出层）：激活函数为sigmoid</span>outputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"sigmoid"</span><span class="token punctuation">,</span>name <span class="token operator">=</span> <span class="token string">"layer3"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用Model来创建模型，指明输入和输出</span>model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>inputs<span class="token punctuation">,</span> outputs<span class="token operator">=</span>outputs<span class="token punctuation">,</span>name<span class="token operator">=</span><span class="token string">"my_model"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#model的子类构建</span><span class="token comment" spellcheck="true"># 导入工具包</span><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token comment" spellcheck="true"># 定义model的子类</span><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 在init方法中定义网络的层结构</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 第一层：激活函数为relu,权重初始化为he_normal</span>        self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span>                     kernel_initializer<span class="token operator">=</span><span class="token string">"he_normal"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"layer1"</span><span class="token punctuation">,</span>input_shape<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 第二层：激活函数为relu,权重初始化为he_normal</span>        self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">,</span>                     kernel_initializer<span class="token operator">=</span><span class="token string">"he_normal"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"layer2"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 第三层（输出层）：激活函数为sigmoid,权重初始化为he_normal</span>        self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"sigmoid"</span><span class="token punctuation">,</span>                     kernel_initializer<span class="token operator">=</span><span class="token string">"he_normal"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"layer3"</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 在call方法中万完成前向传播</span>    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 实例化模型</span>model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 设置一个输入，调用模型（否则无法使用summay()）</span>x <span class="token operator">=</span> tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>y <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><table><thead><tr><th><strong>函数</strong></th><th><strong>函数性质</strong></th></tr></thead><tbody><tr><td><strong>用于配置训练模型</strong></td><td></td></tr><tr><td><strong>Sequential():用于配置训练模型</strong></td><td>顺序模型是多个网络层的线性堆叠。                                                            你可以通过将网络层实例的列表传递给 <code>Sequential</code> 的构造器，来创建一个 <code>Sequential</code> 模型：</td></tr><tr><td>model = <strong>Sequential</strong>([     Dense(32, input_shape=(784,)),     Activation(‘relu’),     Dense(10),     Activation(‘softmax’), ])</td><td>([Dense（第一层神经元个数，input_shape：输入指定的大小)，Activation(‘relu’)：激活函数，                                                           Dense（第二层神经元个数，Activation(‘relu’)：激活函数，。。。])</td></tr><tr><td></td><td></td></tr><tr><td><strong>模型构建</strong></td><td></td></tr><tr><td>tf.keras.layers.<strong>Dense</strong>(     units, activation=None, use_bias=True, kernel_initializer=’glorot_uniform’,     bias_initializer=’zeros’)</td><td><strong>主要参数</strong>：                                                                                                                      <strong>units</strong>: 当前层中包含的神经元个数                                                                          <strong>Activation</strong>: 激活函数，relu,sigmoid等                                           <strong>use_bias</strong>: 是否使用偏置，默认使用偏置                                                             <strong>Kernel_initializer</strong>: 权重的初始化方式，默认是Xavier初始化<strong>bias_initializer</strong>: 偏置的初始化方式，默认为0</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>用Input构建神经网络</strong></td><td></td></tr><tr><td>keras.engine.input_layer.<strong>Input</strong>()</td><td><code>Input()</code> 用于实例化 Keras 张量。Keras 张量是底层后端(Theano, TensorFlow 或 CNTK)的张量对象，我们增加了一些特性，使得能够通过了解模型的输入和输出来构建 Keras 模型。                                            <strong>参数</strong>                                                                                                               <strong>shape</strong>: 一个尺寸元组（整数），不包含批量大小。例如，<code>shape=(32,)</code> 表明期望的输入是按批次的 32 维向量                                    <strong>batch_shape</strong>: 一个尺寸元组（整数），包含批量大小。例如，<code>batch_shape=(10, 32)</code> 表明期望的输入是 10 个 32 维向量。<code>batch_shape=(None, 32)</code> 表明任意批次大小的 32 维向量                                                                      <strong>name</strong>: 一个可选的层的名称的字符串。在一个模型中应该是唯一的（不可以重用一个名字两次）。如未提供，将自动生成                                                                          <strong>dtype</strong>: 输入所期望的数据类型，字符串表示 (<code>float32</code>, <code>float64</code>, <code>int32</code>…)                                                                                                  <strong>sparse</strong>: 一个布尔值，指明需要创建的占位符是否是稀疏的                                     <strong>tensor</strong>: 可选的可封装到 <code>Input</code> 层的现有张量。如果设定了，那么这个层将不会创建占位符张量                                                                         <strong>返回</strong> 一个张量</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>激活函数</strong></td><td>激活函数可决定层中每个节点的输出形状。这些非线性关系很重要，如果没有它们，模型将等同于单个层。激活函数有很多，但隐藏层通常使用</td></tr><tr><td><strong>隐藏层使用最多的激活函数：</strong>ReLU</td><td></td></tr><tr><td>keras.layers.<strong>ReLU</strong>(max_value=None, negative_slope=0.0, threshold=0.0)</td><td>使用默认值时，它返回逐个元素的 <code>max(x，0)</code>。                                                    <strong>否则</strong>                                                                                                                                   如果 <code>x &gt;= max_value</code>，返回 <code>f(x) = max_value</code>，                                         如果 <code>threshold &lt;= x &lt; max_value</code>，返回 <code>f(x) = x</code>,                                  否则，返回 <code>f(x) = negative_slope * (x - threshold)</code>。                                 <strong>参数：</strong>                                                                                                                <strong>max_value</strong>: 浮点数，最大的输出值                                        <strong>negative_slope</strong>: float &gt;= 0. 负斜率系数                                        <strong>threshold</strong>: float。”thresholded activation” 的阈值</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>输出层使用最多的函数：Softmax</strong></td><td></td></tr><tr><td>keras.layers.Softmax(axis=-1)</td><td><strong>axis</strong>: 整数，应用 softmax 标准化的轴。</td></tr><tr><td></td><td></td></tr><tr><td>模型编译</td><td></td></tr></tbody></table><h4 id="数据的转换"><a href="#数据的转换" class="headerlink" title="数据的转换"></a>数据的转换</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.cast（x, dtype, name=None）</td><td>第一个参数 x:   待转换的数据（张量）                                                                                                      第二个参数 dtype： 目标数据类型                                                                                                           第三个参数 name： 可选参数，定义操作的名称</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="tf-boolean-mask-函数的使用"><a href="#tf-boolean-mask-函数的使用" class="headerlink" title="tf.boolean_mask()函数的使用"></a><strong>tf.boolean_mask()函数的使用</strong></h4><table><thead><tr><th>函数</th><th>函数的性质</th></tr></thead><tbody><tr><td>使用tf.boolean_mask用来过滤概率值比较低的锚盒</td><td></td></tr><tr><td><strong>tf.boolean_mask(<strong>a,b</strong>)</strong></td><td>1：使a (m维)矩阵，仅保留与b中“True”元素同下标的部分，                                  2：一个参数b为滤波器掩模，生成掩模要用到逻辑表达式（&gt;或者&lt;）生成布尔值</td></tr><tr><td></td><td></td></tr></tbody></table><p>例子：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tfa <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> b <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_max<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>c<span class="token operator">=</span>  a <span class="token operator">></span><span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"a="</span><span class="token operator">+</span>str<span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"b="</span><span class="token operator">+</span>str<span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"c="</span><span class="token operator">+</span>str<span class="token punctuation">(</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>c<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>d<span class="token operator">=</span>tf<span class="token punctuation">.</span>boolean_mask<span class="token punctuation">(</span>a<span class="token punctuation">,</span>c<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"d="</span><span class="token operator">+</span>str<span class="token punctuation">(</span>d<span class="token punctuation">.</span>eval<span class="token punctuation">(</span>session<span class="token operator">=</span>sess<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span>shape<span class="token punctuation">)</span></code></pre><p>运行结果如下：</p><pre class=" language-python"><code class="language-python">a<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>b<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">.</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">.</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span>c<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token boolean">False</span>  <span class="token boolean">True</span><span class="token punctuation">]</span>  <span class="token punctuation">[</span> <span class="token boolean">True</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token boolean">True</span>  <span class="token boolean">True</span><span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token boolean">False</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span>d<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">.</span> <span class="token number">4</span><span class="token punctuation">.</span> <span class="token number">6</span><span class="token punctuation">.</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span></code></pre><h3 id="4-训练与评估"><a href="#4-训练与评估" class="headerlink" title="4.训练与评估"></a>4.训练与评估</h3><ul><li>配置训练过程：</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 配置优化方法，损失函数和评价指标</span>model<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>              loss<span class="token operator">=</span><span class="token string">'categorical_crossentropy'</span><span class="token punctuation">,</span>              metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><table><thead><tr><th>函数</th><th>函数向性质</th></tr></thead><tbody><tr><td>adam</td><td>优化器。</td></tr><tr><td>keras.optimizers.<strong>Adam</strong>(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)</td><td>**参数 **                                                                                                                                            <strong>lr</strong>: float &gt;= 0. 学习率。                                                                                                           <strong>beta_1</strong>: float, 0 &lt; beta &lt; 1. 通常接近于 1。                                                                      **beta_2**: float, 0 &lt; beta &lt; 1. 通常接近于 1。                                                                                                 **epsilon**: float &gt;= 0. 模糊因子. 若为 <code>None</code>, 默认为 <code>K.epsilon()</code>。                           <strong>decay</strong>: float &gt;= 0. 每次参数更新后学习率衰减值。                                                                                  <strong>amsgrad</strong>: boolean. 是否应用此算法的 AMSGrad 变种，来自论文 “On the Convergence of Adam and Beyond”。</td></tr><tr><td></td><td><strong>lr</strong>: float &gt;= 0. 学习率。 <strong>beta_1</strong>: float, 0 &lt; beta &lt; 1. 通常接近于 1。 **beta_2**: float, 0 &lt; beta &lt; 1. 通常接近于 1。 **epsilon**: float &gt;= 0. 模糊因子. 若为 <code>None</code>, 默认为 <code>K.epsilon()</code>。 <strong>decay</strong>: float &gt;= 0. 每次参数更新后学习率衰减值。 <strong>amsgrad</strong>: boolean. 是否应用此算法的 AMSGrad 变种，来自论文 “On the Convergence of Adam and Beyond”。</td></tr><tr><td><strong>model.compile</strong></td><td></td></tr><tr><td></td><td><strong>方法用于在配置训练模型时，告知训练时用的优化器、损失函数和准确率评测标准</strong>：</td></tr><tr><td><strong>model.compile</strong>=(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)</td><td><strong>参数</strong>                                                                                                                <strong>optimizer</strong>: 字符串（优化器名）或者优化器对象。详见 <a href="https://keras.io/optimizers">optimizers</a>                                                                                       1：“sgd”   或者   tf.optimizers.SGD 梯度优化方法                                                                                         2:“adagrad”  或者  tf.keras.optimizers.Adagrad                                                                     3:”adadelta”  或者  tf.keras.optimizers.Adadelta                                                           4:“adam”  或者  tf.keras.optimizers.Adam                                                                                                                               <strong>loss</strong>: 字符串（目标函数名）或目标函数。详见 <a href="https://keras.io/losses">losses</a>。如果模型具有多个输出，则可以通过传递损失函数的字典或列表，在每个输出上使用不同的损失。模型将最小化的损失值将是所有单个损失的总和                                                                                                                                1：categorical_crossentropy：交叉熵损失                                                                                                1：”mse” 或者 tf.keras.losses.MeanSquaredError():  均方误差      2：”sparse_categorical_crossentropy”  或者                                                                                              <strong>metrics</strong>: 在训练和测试期间的模型评估标准。通常你会使用 <code>metrics = [&#39;accuracy&#39;]</code>。要为多输出模型的不同输出指定不同的评估标准，还可以传递一个字典，如 <code>metrics = &#123;&#39;output_a&#39;：&#39;accuracy&#39;&#125; </code>                                                                                     <strong>loss_weights</strong>: 指定标量系数（Python浮点数）的可选列表或字典，用于加权不同模型输出的损失贡献。模型将要最小化的损失值将是所有单个损失的加权和，由 <code>loss_weights</code> 系数加权。如果是列表，则期望与模型的输出具有 1:1 映射。如果是张量，则期望将输出名称（字符串）映射到标量系数 。                                                                                                                             <strong>sample_weight_mode</strong>: 如果你需要执行按时间步采样权重（2D 权重），请将其设置为 <code>temporal</code>。默认为 <code>None</code>，为采样权重（1D）。如果模型有多个输出，则可以通过传递 mode 的字典或列表，以在每个输出上使用不同的 <code>sample_weight_mode</code>。                                                                                  <strong>weighted_metrics</strong>: 在训练和测试期间，由 sample_weight 或 class_weight 评估和加权的度量标准列表。                                                <strong>target_tensors</strong>: 默认情况下，Keras 将为模型的目标创建一个占位符，在训练过程中将使用目标数据。相反，如果你想使用自己的目标张量（反过来说，Keras 在训练期间不会载入这些目标张量的外部 Numpy 数据），您可以通过 <code>target_tensors</code> 参数指定它们。它应该是单个张量（对于单输出 Sequential 模型）。                                                                                            *<strong>*kwargs</strong>: 当使用 Theano/CNTK 后端时，这些参数被传入 <code>K.function</code>。当使用 TensorFlow 后端时，这些参数被传递到 <code>tf.Session.run</code></td></tr><tr><td><strong>异常</strong></td><td><strong>ValueError</strong>:  如果 <code>optimizer</code>, <code>loss</code>, <code>metrics</code> 或 <code>sample_weight_mode</code> 这些参数不合法</td></tr></tbody></table><ul><li><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 指明训练数据集，训练epoch,批次大小和验证集数据</span>model<span class="token punctuation">.</span>fit<span class="token operator">/</span>fit_generator<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>                         batch_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>          validation_data<span class="token operator">=</span>val_dataset<span class="token punctuation">,</span>          <span class="token punctuation">)</span></code></pre><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>model. fit</strong></td><td></td></tr><tr><td><strong>model. fit</strong>(x=None, y=None, batch_size=None, epochs=1, verbose=1, <br/>callbacks=None, validation_split=0.0, validation_data=None, <br/>shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0,                 <br/>steps_per_epoch=None, validation_steps=None)</td><td><strong>以给定数量的轮次（数据集上的迭代）训练模型:</strong>                                                                      <strong>x: 训练数据</strong>的 Numpy 数组（如果模型只有一个输入）， 或者是 Numpy 数组的列表（如果模型有多个输入）。如果模型中的输入层被命名，你也可以传递一个字典，将输入层名称映射到 Numpy 数组。 如果从本地框架张量馈送（例如 TensorFlow 数据张量）数据，x 可以是 None（默认）    。                                                                                                         <strong>y: 目标（训练标签）</strong>数据的 Numpy 数组（如果模型只有一个输出）， 或者是 Numpy 数组的列表（如果模型有多个输出）。如果模型中的输出层被命名，你也可以传递一个字典，将输出层名称映射到 Numpy 数组。 如果从本地框架张量馈送（例如 TensorFlow 数据张量）数据，y 可以是 None（默认）。                                                                                                    <strong>batch_size:</strong> 整数或 None。每次梯度更新的样本数。如果未指定，默认为 32。                                                                                                                                <strong>epochs:</strong> 整数。训练模型迭代轮次。一个轮次是在整个 x 和 y 上的一轮迭代。 请注意，与 initial_epoch 一起，epochs 被理解为 「最终轮次」。模型并不是训练了 epochs 轮，而是到第 epochs 轮停止训练  。                                                                                                               <strong>verbose:</strong> 0, 1 或 2。日志显示模式。 0 = 安静模式, 1 = 进度条, 2 = 每轮一行。                                                                                                                              <strong>callbacks:</strong> 一系列的 <a href="http://www.zzvips.com/article/76569.html">keras</a>.callbacks.Callback 实例。一系列可以在训练时使用的回调函数。 详见 callbacks。                                                                       <strong>validation_split:</strong> 0 和 1 之间的浮点数。用作验证集的训练数据的比例。 模型将分出一部分不会被训练的验证数据，并将在每一轮结束时评估这些验证数据的误差和任何其他模型指标。 验证数据是混洗之前 x 和y 数据的最后一部分样本中。                                                                                                           <strong>validation_data:</strong> 元组 (x_val，y_val) 或元组 (x_val，y_val，val_sample_weights)，用来评估损失，以及在每轮结束时的任何模型度量指标。 模型将不会在这个数据上进行训练。这个参数会覆盖 validation_split。<strong>shuffle:</strong> 布尔值（是否在每轮迭代之前混洗数据）或者 字符串 (batch)。 batch 是处理 HDF5 数据限制的特殊选项，它对一个 batch 内部的数据进行混洗。 当 steps_per_epoch 非 None 时，这个参数无效。                                       <strong>class_weight:</strong> 可选的字典，用来映射类索引（整数）到权重（浮点）值，用于加权损失函数（仅在训练期间）。 这可能有助于告诉模型 「更多关注」来自代表性不足的类的样本。                                                                             <strong>sample_weight:</strong> 训练样本的可选 Numpy 权重数组，用于对损失函数进行加权（仅在训练期间）。 您可以传递与输入样本长度相同的平坦（1D）Numpy 数组（权重和样本之间的 1:1 映射）， 或者在时序数据的情况下，可以传递尺寸为 (samples, sequence_length) 的 2D 数组，以对每个样本的每个时间步施加不同的权重。 在这种情况下，你应该确保在 compile() 中指定 sample_weight_mode=”temporal”。                                                            <strong>initial_epoch:</strong> 整数。开始训练的轮次（有助于恢复之前的训练）。<strong>steps_per_epoch:</strong> 整数或 None。 在声明一个轮次完成并开始下一个轮次之前的总步数（样品批次）。 使用 TensorFlow 数据张量等输入张量进行训练时，默认值 None 等于数据集中样本的数量除以 batch 的大小，如果无法确定，则为 1。                                                                                                  <strong>validation_steps:</strong> 只有在指定了 steps_per_epoch 时才有用。停止前要验证的总步数（批次样本）。</td></tr><tr><td><strong>返回</strong></td><td><strong>返回：</strong>                                                                                                                                      一个 History 对象。其 History.history 属性是连续 epoch 训练损失和评估值，以及验证集损失和评估值的记录（如果适用）。</td></tr><tr><td><strong>异常</strong></td><td>RuntimeError: 如果模型从未编译。                                                                 ValueError: 在提供的输入数据与模型期望的不匹配的情况下。</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>fit_generator</strong></td><td></td></tr><tr><td><strong>model.fit_generator</strong>下面函数解义</td><td>使用 Python 生成器（或 Sequence 实例）逐批生成的数据，按批次训练模型。                                                                                                                                     生成器与模型并行运行，以提高效率。 例如，这可以让你在 CPU 上对图像进行实时数据增强，以在 GPU 上训练模型。                                            keras.utils.Sequence 的使用可以保证数据的顺序， 以及当 use_multiprocessing=True 时 ，保证每个输入在每个 epoch 只使用一次。</td></tr><tr><td><strong>fit_generator</strong>(generator, steps_per_epoch=None, epochs=1, verbose=1, <br/>callbacks=None, validation_data=None, validation_steps=None, <br/>class_weight=None, max_queue_size=10, workers=1, <br/>use_multiprocessing=False, shuffle=True, initial_epoch=0)</td><td><strong>generator:</strong> 一个生成器，或者一个 Sequence (keras.utils.Sequence) 对象的实例， 以在使用多进程时避免数据的重复。 生成器的输出应该为以下之一：一个 (inputs, targets) 元组，一个 (inputs, targets, sample_weights) 元组。这个元组（生成器的单个输出）组成了单个的 batch。 因此，这个元组中的所有数组长度必须相同（与这一个 batch 的大小相等）。 不同的 batch 可能大小不同。 例如，一个 epoch 的最后一个 batch 往往比其他 batch 要小， 如果数据集的尺寸不能被 batchsize 整除。 生成器将无限地在数据集上循环。当运行到第 steps_per_epoch 时，记一个 epoch 结束。<strong>steps_per_epoch:</strong> 在声明一个 epoch 完成并开始下一个 epoch 之前从 generator 产生的总步数（批次样本）。 它通常应该等于你的数据集的样本数量除以批量大小。 对于 Sequence，它是可选的：如果未指定，将使用len(generator) 作为步数。                                                                                <strong>epochs:<strong>整数。训练模型的迭代总轮数。一个 epoch 是对所提供的整个数据的一轮迭代，如 steps_per_epoch 所定义。注意，与 initial_epoch 一起使用，epoch 应被理解为「最后一轮」。模型没有经历由 epochs 给出的多次迭代的训练，而仅仅是直到达到索引 epoch 的轮次。                                              <strong>verbose:</strong> 0, 1 或 2。日志显示模式。 0 = 安静模式, 1 = 进度条, 2 = 每轮一行</strong>callbacks:</strong> keras.callbacks.Callback 实例的列表。在训练时调用的一系列回调函数                                                                                                              <strong>validation_data:</strong> 它可以是以下之一：验证数据的生成器或 Sequence 实例一个 (inputs, targets) 元组一个 (inputs, targets, sample_weights) 元组在每个 epoch 结束时评估损失和任何模型指标。该模型不会对此数据进行训练。<strong>validation_steps:</strong> 仅当 validation_data 是一个生成器时才可用。 在停止前 generator 生成的总步数（样本批数）。 对于 Sequence，它是可选的：如果未指定，将使用 len(generator) 作为步数。                                              <strong>class_weight:</strong> 可选的将类索引（整数）映射到权重（浮点）值的字典，用于加权损失函数（仅在训练期间）。 这可以用来告诉模型「更多地关注」来自代表性不足的类的样本max_queue_size: 整数。生成器队列的最大尺寸。 如未指定，max_queue_size 将默认为 10workers: 整数。使用的最大进程数量，如果使用基于进程的多线程。 如未指定，workers 将默认为 1。如果为 0，将在主线程上执行生成器                                                                  **use_multiprocessing:**布尔值。如果 True，则使用基于进程的多线程。 如未指定， use_multiprocessing 将默认为 False。 请注意，由于此实现依赖于多进程，所以不应将不可传递的参数传递给生成器，因为它们不能被轻易地传递给子进程                                                                                                             <strong>shuffle:</strong> 是否在每轮迭代之前打乱 batch 的顺序。 只能与 Sequence (keras.utils.Sequence) 实例同用                                                                          <strong>initial_epoch:</strong> 开始训练的轮次（有助于恢复之前的训练）</td></tr><tr><td><strong>返回</strong></td><td>一个 History 对象。其 History.history 属性是连续 epoch 训练损失和评估值，以及验证集损失和评估值的记录（如果适用）。</td></tr><tr><td><strong>总结</strong></td><td>在使用fit函数的时候，需要有batch_size，但是在使用fit_generator时需要有steps_per_epoch</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><ul><li><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 指明评估数据集和批次大小</span>model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span></code></pre><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>model.evaluate</strong></td><td><strong>指明评估数据集和批次大小</strong></td></tr><tr><td><strong>model.evaluate</strong>(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)</td><td>**x:**Numpy测试数据数组（如果模型只有一个输入），或者Numpy数组列表（如果模型有多个输入）。如果命名了模型中的输入层，还可以将映射输入名称的字典传递给Numpy数组。如果从框架本机张量（例如，TensorFlow数据张量）馈送，则x可以是None（默认值）。                                                                  <strong>y：</strong>目标（标签）数据的Numpy数组（如果模型有一个输出），或者Numpy数组的列表（如果模型有多个输出）。如果命名了模型中的输出层，还可以将映射输出名称的字典传递给Numpy数组。如果从框架本机张量（例如，TensorFlow数据张量）馈送，则y可以是None（默认值）。                                             <strong>batch_size：</strong>整数或无。每个评估步骤的样本数。如果未指定，批次大小将默认为32。                                                                                                           <strong>verbose：</strong>0或1。详细模式。0=静音，1=进度条。                                                                <strong>sample_weight：</strong>测试样本的可选加权数组，用于加权损失函数。您可以传递一个与输入样本长度相同的平面（1D）Numpy数组（权重和样本之间的1:1映射），或者在时间数据的情况下，可以传递一个具有形状（样本、序列长度）的2D数组，以便对每个样本的每个时间步应用不同的权重。在这种情况下，您应该确保在compile（）中指定sample\u weight\u mode=“temporal”。                                                                               <strong>sample_weight：</strong>整数或无。在声明评估轮已完成之前的总步骤数（批样本）。忽略，默认值为“无”。</td></tr><tr><td></td><td></td></tr></tbody></table><ul><li><h4 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h4></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 对新的样本进行预测</span>model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span></code></pre><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>model.predict</strong></td><td><strong>对新的样本进行预测</strong></td></tr><tr><td><strong>model.predict</strong>(x, batch_size=None, verbose=0, steps=None)</td><td><strong>x</strong>: 输入数据，Numpy 数组（或者 Numpy 数组的列表，如果模型有多个输出）                                                                                                                                             <strong>batch_size</strong>: 整数。如未指定，默认为 32。                                                                                      <strong>verbose</strong>: 日志显示模式，0 或 1。                                                                                                      <strong>steps</strong>: 声明预测结束之前的总步数（批次样本）。默认值 <code>None</code>。</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="5-回调函数（callbacks）"><a href="#5-回调函数（callbacks）" class="headerlink" title="5.回调函数（callbacks）"></a>5.回调函数（callbacks）</h3><h4 id="回调函数使用"><a href="#回调函数使用" class="headerlink" title="回调函数使用"></a>回调函数使用</h4><p>回调函数是一个函数的合集，会在训练的阶段中所使用。你可以使用回调函数来查看训练模型的内在状态和统计。你可以传递一个列表的回调函数（作为 <code>callbacks</code> 关键字参数）到 <code>Sequential</code> 或 <code>Model</code> 类型的 <code>.fit()</code> 方法。在训练时，相应的回调函数的方法就会被在各自的阶段被调用。</p><p>回调函数用在模型训练过程中，来控制模型训练行为，可以自定义回调函数，也可使用<strong>tf.keras.callbacks</strong> 内置的 callback ：</p><p><strong>ModelCheckpoint</strong>：定期保存 checkpoints。 <strong>LearningRateScheduler</strong>：动态改变学习速率。 EarlyStopping：当验证集上的性能不再提高时，终止训练。 TensorBoard：使用 TensorBoard 监测模型的状态。</p><table><thead><tr><th><strong>函数</strong></th><th><strong>函数性质</strong></th></tr></thead><tbody><tr><td><strong>回调函数</strong></td><td></td></tr><tr><td><strong>keras.callbacks.Callback()</strong></td><td><strong>回调函数</strong>：用来组建新的回调函数的抽象基类。</td></tr><tr><td><strong>keras.callbacks.Callback()</strong></td><td><strong>属性</strong>                                                                                      <strong>params</strong>: 字典。训练参数，(例如，verbosity, batch size, number of epochs…)                                                         <strong>model</strong>: <code>keras.models.Model</code> 的实例。指代被训练模型被回调函数作为参数的 <code>logs</code> 字典，它会含有于当前批量或训练轮相关数据的键目前，<code>Sequential</code> 模型类的 <code>.fit()</code> 方法会在传入到回调函数的 <code>logs</code> 里面包含以下的数据                                                                        <strong>on_epoch_end</strong>: 包括 <code>acc</code> 和 <code>loss</code> 的日志， 也可以选择性的包括 <code>val_loss</code>（如果在 <code>fit</code> 中启用验证），和 <code>val_acc</code>（如果启用验证和监测精确值）<strong>on_batch_begin</strong>: 包括 <code>size</code> 的日志，在当前批量内的样本数量<strong>on_batch_end</strong>: 包括 <code>loss</code> 的日志，也可以选择性的包括 <code>acc</code>（如果启用监测精确值）</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>在每个训练期之后保存模型</strong></td><td></td></tr><tr><td><strong>ModelCheckpoint：</strong></td><td>在每个训练期之后保存模型。                                                            如果 <strong>filepath</strong><code>是</code>weights.{epoch:02d}-{val_loss:.2f}.hdf5`，<br/>那么模型被保存的的文件名就会有训练轮数和验证损失。</td></tr><tr><td><strong>keras.callbacks.ModelCheckpoint</strong>(filepath, monitor=’val_loss’, verbose=0, save_best_only=False, save_weights_only=False, mode=’auto’, period=1)</td><td><strong>参数：</strong>                                                                                  <strong>filepath</strong>:  字符串，保存模型的路径。                               <strong>monitor</strong>: 被监测的数据。                                                        <strong>verbose</strong>: 详细信息模式，0 或者 1 。     <strong>save_best_only</strong>: 如果 <code>save_best_only=True</code>， 被监测数据的最佳模型就不会被覆盖。                                                                                 <strong>mode</strong>: {auto, min, max} 的其中之一。 如果 <code>save_best_only=True</code>，那么是否覆盖保存文件的决定就取决于被监测数据的最大或者最小值。 对于 <code>val_acc</code>，模式就会是 <code>max</code>，而对于 <code>val_loss</code>，模式就需要是 <code>min</code>，等等。 在 <code>auto</code> 模式中，方向会自动从被监测的数据的名字中判断出来。                                                         <strong>save_weights_only</strong>: 如果 True，那么只有模型的权重会被保存 (<code>model.save_weights(filepath)</code>)， 否则的话，整个模型会被保存 (<code>model.save(filepath)</code>) 。                        <strong>period</strong>: 每个检查点之间的间隔（训练轮数）</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>学习速率定时器。</strong></td><td></td></tr><tr><td><strong>keras.callbacks.LearningRateScheduler</strong>(schedule, verbose=0)</td><td><strong>参数</strong>                                                                       <strong>schedule</strong>: 一个函数，接受轮索引数作为输入（整数，从 0 开始迭代）然后返回一个学习速率作为输出（浮点数）                                                          <strong>verbose</strong>: 整数。 0：安静，1：更新信息</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>Tensorboard 基本可视化。提供的一个可视化工具。</strong></td><td></td></tr><tr><td>这个回调函数为 Tensorboard :编写一个日志这样你可以<strong>可视化测试和训练的标准评估的动态图像</strong>，也可以可视化模型中不同层的激活值直方图。</td><td></td></tr><tr><td><strong>keras.callbacks.TensorBoard</strong>(log_dir=’./logs’, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq=’epoch’)</td><td><strong>参数:    **                                                                     <strong>log_dir</strong>: 用来保存被 TensorBoard 分析的日志文件的文件名                                                                                                         <strong>histogram_freq</strong>: 对于模型中各个层计算激活值和模型权重直方图的频率（训练轮数中）。如果设置成 0 ，直方图不会被计算。对于直方图可视化的验证数据（或分离数据）一定要明确的指出</strong>write_graph**: 是否在 TensorBoard 中可视化图像。如果 write_graph 被设置为 True，日志文件会变得非常大                                                    <strong>write_grads</strong>: 是否在 TensorBoard  中可视化梯度值直方图。<code>histogram_freq</code> 必须要大于 0 。        <strong>batch_size</strong>: 用以直方图计算的传入神经元网络输入批的大小                                                <strong>write_images</strong>: 是否在 TensorBoard 中将模型权重以图片可视化                                        <strong>embeddings_freq</strong>: 被选中的嵌入层会被保存的频率（在训练轮中）                   <strong>embeddings_layer_names</strong>: 一个列表，会被监测层的名字。如果是 None 或空列表，那么所有的嵌入层都会被监测                               <strong>embeddings_metadata</strong>: 一个字典，对应层的名字到保存有这个嵌入层元数据文件的名字。查看 <a href="https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional">详情</a>关于元数据的数据格式。以防同样的元数据被用于所用的嵌入层，字符串可以被传入          <strong>embeddings_data</strong>: 要嵌入在 <code>embeddings_layer_names</code> 指定的层的数据。Numpy 数组（如果模型有单个输入）或 Numpy 数组列表（如果模型有多个输入）。<a href="https://www.tensorflow.org/programmers_guide/embedding">Learn ore about embeddings</a> 。                                                     <strong>update_freq</strong>: <code>&#39;batch&#39;</code> 或 <code>&#39;epoch&#39;</code> 或 整数。当使用 <code>&#39;batch&#39;</code> 时，在每个 batch 之后将损失和评估值写入到 TensorBoard 中。同样的情况应用到 <code>&#39;epoch&#39;</code> 中。如果使用整数，例如 <code>10000</code>，这个回调会在每 10000 个样本之后将损失和评估值写入到 TensorBoard 中。注意，频繁地写入到 TensorBoard 会减缓你的训练</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="6-模型的保存和恢复：在官方网址上搜索https-keras-io-zh-getting-started-faq-3"><a href="#6-模型的保存和恢复：在官方网址上搜索https-keras-io-zh-getting-started-faq-3" class="headerlink" title="6.模型的保存和恢复：在官方网址上搜索https://keras.io/zh/getting-started/faq/#_3"></a><strong>6.模型的保存和恢复</strong>：在官方网址上搜索<a href="https://keras.io/zh/getting-started/faq/#_3">https://keras.io/zh/getting-started/faq/#_3</a></h3><ul><li><strong>只保存参数</strong></li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 只保存模型的权重</span>model<span class="token punctuation">.</span>save_weights<span class="token punctuation">(</span><span class="token string">'./my_model'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 加载模型的权重</span>model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span><span class="token string">'my_model'</span><span class="token punctuation">)</span></code></pre><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>model.save_weights（” “）</td><td>只保存模型的权重</td></tr><tr><td>model.load_weights(“”)</td><td>加载模型的权重</td></tr><tr><td></td><td></td></tr></tbody></table><ul><li><strong>保存整个模型</strong></li></ul><pre><code># 保存模型架构与权重在h5文件中model.save(&#39;my_model.h5&#39;)# 加载模型：包括架构和对应的权重model = keras.models.load_model(&#39;my_model.h5&#39;)</code></pre><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>model.save（“.h5”）</td><td>保存模型架构与权重在h5文件中</td></tr><tr><td>keras.models.load_model(“.h5”)</td><td>加载模型：包括架构和对应的权重</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="TensorFlow算数加法"><a href="#TensorFlow算数加法" class="headerlink" title="TensorFlow算数加法"></a><strong>TensorFlow算数加法</strong></h4><table><thead><tr><th><strong>函数</strong></th><th><strong>函数性质</strong></th></tr></thead><tbody><tr><td><strong>tf.add()</strong></td><td>**张量和 **</td></tr><tr><td><strong>tf.multiply()</strong></td><td><strong>张量的乘法,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 对应元素相乘</strong></td></tr><tr><td><strong>tf.matmul()</strong></td><td><strong>计算乘法，对应的矩阵相乘</strong></td></tr><tr><td><strong>tf.reduce_max()</strong></td><td><strong>最大值</strong></td></tr><tr><td><strong>tf.reduce_sum()  # 求和                                                                                                                                           tf.reduce_mean() # 平均值                                                                                                                          tf.reduce_max()  # 最大值                                                                                                                       tf.reduce_min()  # 最小值                                                                                                                                       tf.argmax() # 最大值的索引                                                                                                                                tf.argmin() # 最小值的索引</strong></td><td></td></tr><tr><td><strong>变量.dtype</strong></td><td><strong>类型</strong></td></tr><tr><td><strong>tf.argmin(变量)</strong></td><td><strong>最小值的索引</strong></td></tr><tr><td><strong>tf.argmax(变量)</strong></td><td><strong>最大值的索引</strong></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="keras："><a href="#keras：" class="headerlink" title="keras："></a><strong>keras：</strong></h3><table><thead><tr><th><strong>函数</strong></th><th><strong>函数性质</strong></th></tr></thead><tbody><tr><td><strong>activations</strong></td><td><strong>激活函数</strong>：加上点可以调用激活函数</td></tr><tr><td><strong>applications</strong></td><td><strong>预训练网络模块</strong></td></tr><tr><td><strong>Callbacks</strong></td><td><strong>在模型训练期间被调用</strong></td></tr><tr><td><strong>datasets</strong></td><td><strong>tf.keras数据集模块，包括boston_housing，cifar10，fashion_mnist，imdb ，mnist</strong></td></tr><tr><td><strong>ayers</strong></td><td><strong>Keras层API</strong></td></tr><tr><td><strong>losses</strong></td><td><strong>各种损失函数</strong></td></tr><tr><td><strong>metircs</strong></td><td><strong>各种评价指标</strong></td></tr><tr><td><strong>models</strong></td><td><strong>模型创建模块，以及与模型相关的API</strong></td></tr><tr><td><strong>optimizers</strong></td><td><strong>优化方法</strong></td></tr><tr><td><strong>preprocessing</strong></td><td><strong>Keras数据的预处理模块</strong></td></tr><tr><td><strong>regularizers</strong></td><td><strong>正则化，L1,L2等</strong></td></tr><tr><td><strong>utils</strong></td><td><strong>辅助功能实现</strong></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.keras.losses</td><td>可以调用下面损失函数</td></tr><tr><td>CategoricalCrossentropy()</td><td>用于多交叉熵损失</td></tr><tr><td>BinaryCrossentropy()</td><td>用于二分类的交叉熵损失</td></tr><tr><td>MeanAbsoluteError()</td><td>MAE损失，称为L1 Loss，平均绝对误差，绝对误差作为距离</td></tr><tr><td>MeanSquaredError()</td><td>L2 loss，或欧氏距离，它以误差的平方和作为距离</td></tr><tr><td>Huber()</td><td>Smooth L1损失函数，上个损失的合并</td></tr></tbody></table><h2 id="查看网络结构图"><a href="#查看网络结构图" class="headerlink" title="查看网络结构图"></a>查看网络结构图</h2><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>.summary()</td><td>查看网络结构图的神经元多少</td></tr><tr><td>.plot_model(model,show_shapes=True)</td><td>查看网络结构的用的模型</td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="激活函数：是线性的映射转换成非线性的映射的过程，这些单调上升的非线性映射做成激活函数，在不同的任务中，总有一个在该任务中表示最好的激活函数"><a href="#激活函数：是线性的映射转换成非线性的映射的过程，这些单调上升的非线性映射做成激活函数，在不同的任务中，总有一个在该任务中表示最好的激活函数" class="headerlink" title="激活函数：是线性的映射转换成非线性的映射的过程，这些单调上升的非线性映射做成激活函数，在不同的任务中，总有一个在该任务中表示最好的激活函数"></a>激活函数：是线性的映射转换成非线性的映射的过程，这些单调上升的非线性映射做成激活函数，在不同的任务中，总有一个在该任务中表示最好的激活函数</h2><p>激活函数会对模型的学习有一定的帮助，</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/5.png' height=px ><h4 id="1：Sigmoid（激活输出层）激活函数，用于到二分类的输出层，会造成梯度消失，梯度饱和"><a href="#1：Sigmoid（激活输出层）激活函数，用于到二分类的输出层，会造成梯度消失，梯度饱和" class="headerlink" title="1：Sigmoid（激活输出层）激活函数，用于到二分类的输出层，会造成梯度消失，梯度饱和"></a>1：Sigmoid（激活输出层）激活函数，用于到二分类的输出层，会造成梯度消失，梯度饱和</h4><p>以0.5为中间形成概率值</p><p>sigmoid函数反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。</p><h4 id="2：tanh-双曲正切曲线-：以0位中心，只要用于隐藏层，tanh两侧的导数也为0，同样会造成梯度消失。梯度饱和"><a href="#2：tanh-双曲正切曲线-：以0位中心，只要用于隐藏层，tanh两侧的导数也为0，同样会造成梯度消失。梯度饱和" class="headerlink" title="2：tanh(双曲正切曲线)：以0位中心，只要用于隐藏层，tanh两侧的导数也为0，同样会造成梯度消失。梯度饱和"></a>2：tanh(双曲正切曲线)：以0位中心，只要用于隐藏层，tanh两侧的导数也为0，同样会造成梯度消失。梯度饱和</h4><p>迭代次数较少，收敛速度更快，因为在0为中间了，将梯度进行了有区分</p><h4 id="3-RELU（多适用于隐藏层）：小于0的都设置为0，大于0的时候会保持梯度不衰减，如果到小于0的时候会出现神经元死亡，计算量小"><a href="#3-RELU（多适用于隐藏层）：小于0的都设置为0，大于0的时候会保持梯度不衰减，如果到小于0的时候会出现神经元死亡，计算量小" class="headerlink" title="3.RELU（多适用于隐藏层）：小于0的都设置为0，大于0的时候会保持梯度不衰减，如果到小于0的时候会出现神经元死亡，计算量小"></a>3.RELU（多适用于隐藏层）：小于0的都设置为0，大于0的时候会保持梯度不衰减，如果到小于0的时候会出现神经元死亡，计算量小</h4><p>Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，神经元为负数时会造成死亡，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。计算量小，速度快斜率就是他的本身，可以用于部分深层的网络训练</p><h3 id="4：LeakReLu（隐藏层）：是RELU升级版，可以解决了负向梯度消失的问题，"><a href="#4：LeakReLu（隐藏层）：是RELU升级版，可以解决了负向梯度消失的问题，" class="headerlink" title="4：LeakReLu（隐藏层）：是RELU升级版，可以解决了负向梯度消失的问题，"></a>4：LeakReLu（隐藏层）：是RELU升级版，可以解决了负向梯度消失的问题，</h3><h4 id="5：SoftMax：-激活输出层：多分类问题中）：将网络输出的logits通过softmax函数，就映射成为-0-1-的值，选取概率最大（也就是值对应最大的）接点，作为我们的预测目标类别。"><a href="#5：SoftMax：-激活输出层：多分类问题中）：将网络输出的logits通过softmax函数，就映射成为-0-1-的值，选取概率最大（也就是值对应最大的）接点，作为我们的预测目标类别。" class="headerlink" title="5：SoftMax：(激活输出层：多分类问题中）：将网络输出的logits通过softmax函数，就映射成为(0,1)的值，选取概率最大（也就是值对应最大的）接点，作为我们的预测目标类别。"></a>5：SoftMax：(激活输出层：多分类问题中）：<strong>将网络输出的logits通过softmax函数，就映射成为(0,1)的值</strong>，<strong>选取概率最大（也就是值对应最大的）接点，作为我们的预测目标类别。</strong></h4><h3 id="6：elu-隐藏层-：全程可以求导"><a href="#6：elu-隐藏层-：全程可以求导" class="headerlink" title="6：elu(隐藏层)：全程可以求导"></a>6：elu(隐藏层)：全程可以求导</h3><h1 id="初始化：对权重进行初始化，b可以先设置为0"><a href="#初始化：对权重进行初始化，b可以先设置为0" class="headerlink" title="初始化：对权重进行初始化，b可以先设置为0"></a>初始化：对权重进行初始化，b可以先设置为0</h1><h3 id="激活函数设置的小一些对tanh和sigmoid中的激活函数计算的形成的梯度就会大一些，更新就会快一些，是有利的"><a href="#激活函数设置的小一些对tanh和sigmoid中的激活函数计算的形成的梯度就会大一些，更新就会快一些，是有利的" class="headerlink" title="激活函数设置的小一些对tanh和sigmoid中的激活函数计算的形成的梯度就会大一些，更新就会快一些，是有利的"></a>激活函数设置的小一些对tanh和sigmoid中的激活函数计算的形成的梯度就会大一些，更新就会快一些，是有利的</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>**Xavier 正态分布初始化器： **</td><td></td></tr><tr><td>tf.keras.initializers.glorot_normal(seed=None)</td><td>**Xavier 正态分布初始化器： **                                                                     seed：一个 Python 整数。作为随机发生器的种子。</td></tr><tr><td>initializer = tf.keras.initializers.glorot_normal() # 采样得到权重值                                                                                                   values = initializer(shape=(9, 1))                                                print(values)</td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>标准化Xavier初始化</strong></td><td></td></tr><tr><td>keras.initializers.glorot_uniform(seed=None)</td><td><strong>seed</strong>: 一个 Python 整数。作为随机发生器的种子。</td></tr><tr><td></td><td></td></tr><tr><td><strong>He 正态分布初始化器：</strong></td><td></td></tr><tr><td>keras.initializers.he_normal(seed=None)</td><td><strong>seed</strong>: 一个 Python 整数。作为随机发生器的种子。</td></tr><tr><td></td><td></td></tr><tr><td><strong>标准化的he初始化</strong>，He 均匀方差缩放初始化器</td><td></td></tr><tr><td>keras.initializers.he_uniform(seed=None)</td><td><strong>seed</strong>: 一个 Python 整数。作为随机发生器的种子。</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="1：随机初始化：随机初始化从均值为0，标准差是1的高斯分布中取样，使用一些很小的值对参数W进行初始化"><a href="#1：随机初始化：随机初始化从均值为0，标准差是1的高斯分布中取样，使用一些很小的值对参数W进行初始化" class="headerlink" title="1：随机初始化：随机初始化从均值为0，标准差是1的高斯分布中取样，使用一些很小的值对参数W进行初始化"></a><strong>1：随机初始化：随机初始化从均值为0，标准差是1的高斯分布中取样，使用一些很小的值对参数W进行初始化</strong></h4><p>确定第一次取值范围，w，让w随机取高斯分布中的值，</p><h4 id="2：标准初始化："><a href="#2：标准初始化：" class="headerlink" title="2：标准初始化："></a><strong>2：标准初始化：</strong></h4><p><strong>有多少d取值范围在这里面取值：(-1/√d,1/√d)均匀分布中生成当前神经元的权重</strong></p><h4 id="4：标准化Xavier初始化"><a href="#4：标准化Xavier初始化" class="headerlink" title="4：标准化Xavier初始化"></a>4：标准化Xavier初始化</h4><p><strong>正态化Xavier初始化（glorot_normal）</strong>：它从以 0 为中心，标准差为 <code>stddev = sqrt(2 / (fan_in + fan_out))</code> 的正态分布中抽取样本， 其中 <code>fan_in</code> 是输入神经元的个数， <code>fan_out</code> 是输出的神经元个数。</p><p><strong>标准化Xavier初始化（glorot_uniform）</strong>：<code>sqrt(6 / (fan_in + fan_out))</code>， 其中 <code>fan_in</code> 是输入神经元的个数， <code>fan_out</code> 是输出的神经元个数。</p><h4 id="5：b标准化he初始化"><a href="#5：b标准化he初始化" class="headerlink" title="5：b标准化he初始化"></a>5：b标准化he初始化</h4><p><strong>正态化的he初始化（he_normal）</strong>：He 正态分布初始化是以 0 为中心，标准差为 <code>stddev = sqrt(2 / fan_in)</code> 的截断正态分布中抽取样本， 其中 <code>fan_in</code>是输入神经元的个数</p><p><strong>标准化的he初始化（he_uniform）</strong>：其中 <code>limit</code> 是 <code>sqrt(6 / fan_in)</code>， 其中 <code>fan_in</code> 输入神经元的个数</p><h1 id="设置层的数量和尺寸："><a href="#设置层的数量和尺寸：" class="headerlink" title="设置层的数量和尺寸："></a>设置层的数量和尺寸：</h1><p>设置网络的层数，小型网络的损失过高，最后的损失值将展现出多变性，拟合能力有上限，大型的损失较小，会有很多不同的方法来解决。</p><h1 id="在线性分类任务中使用线性分类损失"><a href="#在线性分类任务中使用线性分类损失" class="headerlink" title="在线性分类任务中使用线性分类损失"></a>在线性分类任务中使用线性分类损失</h1><h2 id="线性分类损失值："><a href="#线性分类损失值：" class="headerlink" title="线性分类损失值："></a>线性分类损失值：</h2><h4 id="在多分类中使用softmax激活函数，使用CategoricalCrossentropy-多分类交叉熵损失函数："><a href="#在多分类中使用softmax激活函数，使用CategoricalCrossentropy-多分类交叉熵损失函数：" class="headerlink" title="在多分类中使用softmax激活函数，使用CategoricalCrossentropy()多分类交叉熵损失函数："></a>在多分类中使用softmax激活函数，使用CategoricalCrossentropy()多分类交叉熵损失函数：</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/6.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/7.png' height=px ><h3 id="在二分类中使用sigmoid激活函数，使用BinaryCrossentropy-二分类的交叉熵损失函数："><a href="#在二分类中使用sigmoid激活函数，使用BinaryCrossentropy-二分类的交叉熵损失函数：" class="headerlink" title="在二分类中使用sigmoid激活函数，使用BinaryCrossentropy()二分类的交叉熵损失函数："></a>在二分类中使用sigmoid激活函数，使用BinaryCrossentropy()二分类的交叉熵损失函数：</h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/8.png' height=px ><h1 id="在回归任务中使用以下损失函数有几种"><a href="#在回归任务中使用以下损失函数有几种" class="headerlink" title="在回归任务中使用以下损失函数有几种"></a>在回归任务中使用以下损失函数有几种</h1><h3 id="MAE损失也被称为L1-Loss，是以绝对误差作为距离"><a href="#MAE损失也被称为L1-Loss，是以绝对误差作为距离" class="headerlink" title="MAE损失也被称为L1 Loss，是以绝对误差作为距离"></a>MAE损失也被称为L1 Loss，是以绝对误差作为距离</h3><p>是在0处不可导就会造成跳过最小值，稀疏性：就是凸函数的斜率与L1的斜率相交较少，为了惩罚较大的值，因此常常将其作为正则项添加到其他loss中作为约束。L1 loss的最大问题是<strong>梯度在零点不平滑，导致会跳过极小值，不可导</strong></p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/9.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/10.png' height=px ><h3 id="MSE损失为L2-loss，或欧氏距离，它以误差的平方和作为距离"><a href="#MSE损失为L2-loss，或欧氏距离，它以误差的平方和作为距离" class="headerlink" title="MSE损失为L2 loss，或欧氏距离，它以误差的平方和作为距离"></a>MSE损失为L2 loss，或欧氏距离，它以误差的平方和作为距离</h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/11.png' height=px ><p>L2 loss也常常作为正则项。当预测值与目标值相差很大时, <strong>梯度容易爆炸，因为后面的斜率太大会造成爆炸</strong></p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/12.png' height=px ><h3 id="smooth-L1-损失"><a href="#smooth-L1-损失" class="headerlink" title="smooth L1 损失"></a>smooth L1 损失</h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/13.png' height=px ><p>在[-1,1]之间实际上就是L2损失，这样解决了L1的不光滑问题，在[-1,1]区间外，实际上就是L1损失，<strong>这样就解决了离群点梯度爆炸的问题。通常在目标检测中使用该损失函数。</strong></p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/14.png' height=px ><h1 id="深度学习的优化方法，解决鞍点问题和梯度消失及梯度爆炸问题：降低训练损失，只关注最小化目标函数上的表现，"><a href="#深度学习的优化方法，解决鞍点问题和梯度消失及梯度爆炸问题：降低训练损失，只关注最小化目标函数上的表现，" class="headerlink" title="深度学习的优化方法，解决鞍点问题和梯度消失及梯度爆炸问题：降低训练损失，只关注最小化目标函数上的表现，"></a>深度学习的优化方法，解决鞍点问题和梯度消失及梯度爆炸问题：降低训练损失，只关注最小化目标函数上的表现，</h1><h3 id="经过神经网络大数据集运算，经常出现鞍点问题"><a href="#经过神经网络大数据集运算，经常出现鞍点问题" class="headerlink" title="经过神经网络大数据集运算，经常出现鞍点问题"></a><strong>经过神经网络大数据集运算，经常出现鞍点问题</strong></h3><h3 id="海森矩阵，鞍点："><a href="#海森矩阵，鞍点：" class="headerlink" title="海森矩阵，鞍点："></a>海森矩阵，鞍点：</h3><h4 id="多元素上和大型网络中会遇到鞍点问题，"><a href="#多元素上和大型网络中会遇到鞍点问题，" class="headerlink" title="多元素上和大型网络中会遇到鞍点问题，"></a>多元素上和大型网络中会遇到鞍点问题，</h4><p>当函数的海森矩阵在梯度为零的位置上的特征值全为正时，该函数得到局部最小值。</p><p>当函数的海森矩阵在梯度为零的位置上的特征值全为负时，该函数得到局部最大值。</p><p>当函数的海森矩阵在梯度为零的位置上的特征值有正有负时，该函数得到鞍点</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/15.png' height=px ><h1 id="梯度消失和梯度爆炸，鞍点，和梯度停止更新"><a href="#梯度消失和梯度爆炸，鞍点，和梯度停止更新" class="headerlink" title="梯度消失和梯度爆炸，鞍点，和梯度停止更新"></a>梯度消失和梯度爆炸，鞍点，和梯度停止更新</h1><h3 id="梯度消失：网络效果不好，梯度值接近为0，并且损失值一直不下降"><a href="#梯度消失：网络效果不好，梯度值接近为0，并且损失值一直不下降" class="headerlink" title="梯度消失：网络效果不好，梯度值接近为0，并且损失值一直不下降"></a>梯度消失：网络效果不好，梯度值接近为0，并且损失值一直不下降</h3><h3 id="梯度爆炸：权重值和损失值打印出来是nan（很大）那就是梯度爆炸"><a href="#梯度爆炸：权重值和损失值打印出来是nan（很大）那就是梯度爆炸" class="headerlink" title="梯度爆炸：权重值和损失值打印出来是nan（很大）那就是梯度爆炸"></a>梯度爆炸：权重值和损失值打印出来是nan（很大）那就是梯度爆炸</h3><p>为什么会造成我们的损失函数难优化，其实有个原因就是因为激活函数存在使得函数计算梯度时候遇到梯度消失问题。在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸</strong>或者<strong>梯度消失</strong></p><h3 id="优化的选用：如果数据集小些的时候就直接全阶训练，如果数据量很大的时候就优先用mini-batch梯度下降，要以2的幂次方进行训练，效果更好些，得出的结果可能会在最小值周围来回摆动，再经过学习率退火可以达最小值。"><a href="#优化的选用：如果数据集小些的时候就直接全阶训练，如果数据量很大的时候就优先用mini-batch梯度下降，要以2的幂次方进行训练，效果更好些，得出的结果可能会在最小值周围来回摆动，再经过学习率退火可以达最小值。" class="headerlink" title="优化的选用：如果数据集小些的时候就直接全阶训练，如果数据量很大的时候就优先用mini-batch梯度下降，要以2的幂次方进行训练，效果更好些，得出的结果可能会在最小值周围来回摆动，再经过学习率退火可以达最小值。"></a>优化的选用：如果数据集小些的时候就直接全阶训练，如果数据量很大的时候就优先用mini-batch梯度下降，要以2的幂次方进行训练，效果更好些，得出的结果可能会在最小值周围来回摆动，再经过学习率退火可以达最小值。</h3><h3 id="优化方法：批梯度下降，SGD算法的优化，初始化参数和学习率衰减"><a href="#优化方法：批梯度下降，SGD算法的优化，初始化参数和学习率衰减" class="headerlink" title="优化方法：批梯度下降，SGD算法的优化，初始化参数和学习率衰减"></a>优化方法：批梯度下降，SGD算法的优化，初始化参数和学习率衰减</h3><p>当一个神经元损失值时就是一个维度时的只找一个最小点，对于多个神经元时就形成了多维的就形成最小值形成了山貌，</p><h3 id="mini-batch梯度下降，每次处理固定大小的数据集"><a href="#mini-batch梯度下降，每次处理固定大小的数据集" class="headerlink" title="mini-batch梯度下降，每次处理固定大小的数据集"></a>mini-batch梯度下降，每次处理固定大小的数据集</h3><p>选择一个合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。</p><h4 id="mini-batch大小选则：大小选择要以2的幂次方运行的要快一些，因为计算机是以二进制计算"><a href="#mini-batch大小选则：大小选择要以2的幂次方运行的要快一些，因为计算机是以二进制计算" class="headerlink" title="mini-batch大小选则：大小选择要以2的幂次方运行的要快一些，因为计算机是以二进制计算"></a>mini-batch大小选则：大小选择要以2的幂次方运行的要快一些，因为计算机是以二进制计算</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/16.png' height=px ><h1 id="动量算法，解决鞍点：引用计算梯度指数加权平均数的计算方法做梯度更新对学习做调节，让梯度下降更平滑一些更快找到最小值"><a href="#动量算法，解决鞍点：引用计算梯度指数加权平均数的计算方法做梯度更新对学习做调节，让梯度下降更平滑一些更快找到最小值" class="headerlink" title="动量算法，解决鞍点：引用计算梯度指数加权平均数的计算方法做梯度更新对学习做调节，让梯度下降更平滑一些更快找到最小值"></a>动量算法，解决鞍点：引用计算梯度指数加权平均数的计算方法做梯度更新对学习做调节，让梯度下降更平滑一些更快找到最小值</h1><h3 id="在随机梯度下降（SGD）中加入指数加权平均数来更新参数的梯度"><a href="#在随机梯度下降（SGD）中加入指数加权平均数来更新参数的梯度" class="headerlink" title="在随机梯度下降（SGD）中加入指数加权平均数来更新参数的梯度"></a>在随机梯度下降（SGD）中加入指数加权平均数来更新参数的梯度</h3><h3 id="1：优化算法：动量下降，动量太大无法找到最小值，只能在最小值附近徘徊"><a href="#1：优化算法：动量下降，动量太大无法找到最小值，只能在最小值附近徘徊" class="headerlink" title="1：优化算法：动量下降，动量太大无法找到最小值，只能在最小值附近徘徊"></a>1：优化算法：动量下降，动量太大无法找到最小值，只能在最小值附近徘徊</h3><p>部分的梯度下降，训练速度快，但丢失了向量化带来的计算加速，会有很多噪声，需要减少学习率，成本函数总体趋势向全局最小值靠近，但永远不会收敛，而且一直在最小值附近波动，</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 导入相应的工具包</span><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token comment" spellcheck="true"># 实例化优化方法：SGD </span>opt <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 定义要调整的参数</span>var <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 定义损失函数：无参但有返回值</span>loss <span class="token operator">=</span> <span class="token keyword">lambda</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>var <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2.0</span>  <span class="token comment" spellcheck="true"># 计算梯度，并对参数进行更新，步长为 `- learning_rate * grad`</span>opt<span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> <span class="token punctuation">[</span>var<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 展示参数更新结果</span>var<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)</td><td>**参数          **                                                                                                          <strong>lr</strong>: float &gt;= 0. 学习率                                                                             <strong>momentum</strong>: float &gt;= 0. 参数，用于加速 SGD 在相关方向上前进，并抑制震荡                                                                                                                                 <strong>decay</strong>: float &gt;= 0. 每次参数更新后学习率衰减值                                               <strong>nesterov</strong>: boolean. 是否使用 Nesterov 动量,True</td></tr><tr><td>tf.keras.optimizers.SGD(     learning_rate=0.01, momentum=0.0, nesterov=False, name=’SGD’, **kwargs )</td><td><strong>learning_rate</strong>：学习率默认0.01                                                                     <strong>momentum</strong>：浮点超参数&gt;=0，加速相关方向的梯度下降并抑制振荡。默认为0，即香草梯度下降。布尔型。                                                                 <strong>nesterov</strong>：是否应用Nesterov动量。默认为False，根据动量项<strong>预先估计</strong>的参数，在Momentum的基础上进一步加快收敛，提高响应性</td></tr><tr><td></td><td></td></tr><tr><td>tf.keras.optimizers.<strong>Adagrad</strong>(     learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07,     name=’Adagrad’, **kwargs )</td><td><strong>learning_rate</strong>：学习率，                                                          <strong>initial_accumulator_value</strong>：对应调整学习率衰减的方法，                                    <strong>epsilon：</strong>防止分母为0</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><p>在进行模型训练时，有三个基础的概念：</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/17.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/18.png' height=px ><h4 id="动量是引用历史的值通过权重计算分配给现在的值，只不过越远取的值越小，引入β作为偏差修正指数，让下降的更平滑，加速收敛，下降更快"><a href="#动量是引用历史的值通过权重计算分配给现在的值，只不过越远取的值越小，引入β作为偏差修正指数，让下降的更平滑，加速收敛，下降更快" class="headerlink" title="动量是引用历史的值通过权重计算分配给现在的值，只不过越远取的值越小，引入β作为偏差修正指数，让下降的更平滑，加速收敛，下降更快"></a>动量是引用历史的值通过权重计算分配给现在的值，只不过越远取的值越小，引入β作为偏差修正指数，让下降的更平滑，加速收敛，下降更快</h4><h4 id="动量与梯度的关系：当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。"><a href="#动量与梯度的关系：当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。" class="headerlink" title="动量与梯度的关系：当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。"></a>动量与梯度的关系：<strong>当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。</strong></h4><h4 id="下面公式通过分段函数进行添加β值和之前历史的算出的值（St历史加权平均），来调节批量梯度下降的波动，"><a href="#下面公式通过分段函数进行添加β值和之前历史的算出的值（St历史加权平均），来调节批量梯度下降的波动，" class="headerlink" title="下面公式通过分段函数进行添加β值和之前历史的算出的值（St历史加权平均），来调节批量梯度下降的波动，"></a>下面公式通过分段函数进行添加β值和之前历史的算出的值（St历史加权平均），来调节批量梯度下降的波动，</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/19.png' height=px ><p>其中β越大利用的指数加权平均就越大β，β越大相当于求取平均利用的天数越多<strong>，曲线自然就会越平滑而且越滞后。这些系数被称作</strong>偏差修正（Bias Correction）</p><p>动量算法是解决鞍点，让梯度向量更平滑一些，梯度的步数小些，</p><p>使用加权平均值来进行平滑，考虑前期已经计算的数值，对当下计算的印象，这个是累加</p><h4 id="动量控制学习率衰减"><a href="#动量控制学习率衰减" class="headerlink" title="动量控制学习率衰减"></a>动量控制学习率衰减</h4><p>很多时候我们要对学习率（learning rate）进行衰减，下面的代码示范了如何每30个epoch按10%的速率衰减：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">adjust_learning_rate</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>​    <span class="token triple-quoted-string string">"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""</span>​    lr <span class="token operator">=</span> args<span class="token punctuation">.</span>lr <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">0.1</span> <span class="token operator">**</span> <span class="token punctuation">(</span>epoch <span class="token operator">//</span> <span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">)</span>​    <span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>​        param_group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr</code></pre><h2 id="什么是param-groups"><a href="#什么是param-groups" class="headerlink" title="什么是param_groups?"></a>什么是param_groups?</h2><p>optimizer通过param_group来管理参数组.param_group中保存了参数组及其对应的学习率,动量等等.所以我们可以通过更改param_group[‘lr’]的值来更改对应参数组的学习率。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 有两个`param_group`即,len(optim.param_groups)==2</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>​                <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;'params': model.base.parameters()&amp;#125;,</span>​                <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;'params': model.classifier.parameters(), 'lr': 1e-3&amp;#125;</span>​            <span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span> \#一个参数组optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token punctuation">.</span><span class="token number">9</span><span class="token punctuation">)</span></code></pre><h2 id="2：AdaGrad"><a href="#2：AdaGrad" class="headerlink" title="2：AdaGrad"></a>2：AdaGrad</h2><h3 id="算法会使用一个小批量随机梯度，每一个小批量的梯度就是历史的梯度，影响的是每个元素学习率，早期降的快但解不佳，后期学习率就小"><a href="#算法会使用一个小批量随机梯度，每一个小批量的梯度就是历史的梯度，影响的是每个元素学习率，早期降的快但解不佳，后期学习率就小" class="headerlink" title="算法会使用一个小批量随机梯度，每一个小批量的梯度就是历史的梯度，影响的是每个元素学习率，早期降的快但解不佳，后期学习率就小"></a><strong>算法会使用一个小批量随机梯度</strong>，每一个小批量的梯度就是历史的梯度，影响的是每个元素学习率，早期降的快但解不佳，后期学习率就小</h3><p>这里的元素是指这批出来的梯度，第一次出来的梯度，s0式每个元素的初始化为0</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/20.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/21.png' height=px ><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)</td><td>是一种具有特定参数学习率的优化器，它根据参数在训练期间的更新频率进行自适应调整。参数接收的更新越多，更新越小。                                  <strong>参数：</strong>                                                                                                                            <strong>lr</strong>: float &gt;= 0. 学习率                                                                                <strong>epsilon</strong>: float &gt;= 0. 若为 <code>None</code>, 默认为 <code>K.epsilon()</code>                         <strong>decay</strong>: float &gt;= 0. 每次参数更新后学习率衰减值.</td></tr><tr><td></td><td></td></tr><tr><td><strong>AdaGrad算法会使用一个小批量随机梯度</strong></td><td></td></tr><tr><td>tf.keras.optimizers.<strong>Adagrad</strong>(     learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07,     name=’Adagrad’, **kwargs )</td><td><strong>learning_rate</strong>：学习率，                                                          <strong>initial_accumulator_value</strong>：对应ϵ调整学习率衰减的方法，                                    <strong>epsilon：</strong>防止分母为0</td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="3：RMSProp-将这些梯度按元素平方做指数加权移动平均，也是对参数的学习率调节如例子对应下式可以把后面一项成为权重项"><a href="#3：RMSProp-将这些梯度按元素平方做指数加权移动平均，也是对参数的学习率调节如例子对应下式可以把后面一项成为权重项" class="headerlink" title="3：RMSProp :将这些梯度按元素平方做指数加权移动平均，也是对参数的学习率调节如例子对应下式可以把后面一项成为权重项"></a>3：RMSProp :将这些梯度按元素平方做指数加权移动平均，也是对参数的学习率调节如例子对应下式<img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/22.png' height=px >可以把后面一项成为权重项</h2><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/23.png' height=px ><table><thead><tr><th>函数</th><th>函数的性质</th></tr></thead><tbody><tr><td>tf.keras.optimizers.RMSprop(     learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,     name=’RMSprop’, **kwargs )</td><td>learning_rate：学习率rho：</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="4：Adam：将-Momentum-和-RMSProp-算法结合在一起。Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。为了修正dw梯度"><a href="#4：Adam：将-Momentum-和-RMSProp-算法结合在一起。Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。为了修正dw梯度" class="headerlink" title="4：Adam：将 Momentum 和 RMSProp 算法结合在一起。Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。为了修正dw梯度"></a>4：Adam：将 Momentum 和 RMSProp 算法结合在一起。Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。为了修正dw梯度</h2><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/24.png' height=px ><p>假设用每一个 mini-batch 计算 dW、db，第t次迭代时，对学习率和梯度的计算都做了优化</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.keras.optimizers.<strong>Adam</strong>(     learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,     name=’Adam’, **kwargs )</td><td>learning_rate：学习率，                                                                                               beta_1：个浮点值或一个常量浮点张量，或一个不带参数并返回实际值的可调用函数。一阶矩估计的指数衰减率。默认为0.9，                                                    beta_2：二阶矩估计的指数衰减率。默认为0.999。                                                                                          epsilon（ε）：数值稳定性的一个小常数，默认为1e-7，作者建议写成1e-8。                                                                                            amsgrad（）：是否应用Adam及beyond算法的收敛性一文中的AMSGrad变量。默认为False。</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h1 id="Adam与SGD对比："><a href="#Adam与SGD对比：" class="headerlink" title="Adam与SGD对比："></a>Adam与SGD对比：</h1><p>Adam可能不会收敛，因为可能在训练后期引起学习率的震荡，导致模型无法收敛，学习率不一定会是单调递减的</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/25.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/26.png' height=px ><h2 id="学习率退火：防止后期学习率过高，跳过最小值-一般用在大型网络中使用学习率退火，小型的就调一个合适的学习率就行"><a href="#学习率退火：防止后期学习率过高，跳过最小值-一般用在大型网络中使用学习率退火，小型的就调一个合适的学习率就行" class="headerlink" title="学习率退火：防止后期学习率过高，跳过最小值,一般用在大型网络中使用学习率退火，小型的就调一个合适的学习率就行"></a>学习率退火：防止后期学习率过高，跳过最小值,一般用在大型网络中使用学习率退火，小型的就调一个合适的学习率就行</h2><p>一般情况下学习率都会随着训练而变化，如果学习率过高，会造成loss的振荡，但是如果学习率减小的过快，又会造成收敛变慢的情况。</p><h3 id="1-分段常数衰减，随着分段值的不同范围衰减学习率的大小，根于迭代的总数来划分准确的值"><a href="#1-分段常数衰减，随着分段值的不同范围衰减学习率的大小，根于迭代的总数来划分准确的值" class="headerlink" title="1 分段常数衰减，随着分段值的不同范围衰减学习率的大小，根于迭代的总数来划分准确的值"></a>1 分段常数衰减，随着分段值的不同范围衰减学习率的大小，根于迭代的总数来划分准确的值</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>对数据进进行分组，不同的组分到的学习率不同</td><td></td></tr><tr><td>tf.keras.optimizers.schedules.<strong>PiecewiseConstantDecay</strong>(boundaries, values)</td><td>boundaries：设置分段更新的step值，                                           Values: 针对不用分段的学习率值</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="2：指数衰减"><a href="#2：指数衰减" class="headerlink" title="2：指数衰减"></a>2：指数衰减</h3><p>!<img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/27.png' height=px >t表示迭代次数，α0,k是超参数</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.keras.optimizers.schedules.<strong>ExponentialDecay</strong>(initial_learning_rate, decay_steps,decay_rate)</td><td>initial_learning_rate: 初始学习率，α0，                                          decay_steps: k值                              decay_rate: 指数的底</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="3-1-t衰减"><a href="#3-1-t衰减" class="headerlink" title="3 1/t衰减"></a>3 1/t衰减</h3><p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/28.png' height=px >t表示迭代次数，α0,k是超参数</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate, decay_steps,                                                decay_rate)</td><td>Initial_learning_rate: 初始学习率，α0，         decay_step/decay_steps: k值</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h1 id="深度学习正则化（L1-L2），把对应的损失值的式子变优化，减少过拟合"><a href="#深度学习正则化（L1-L2），把对应的损失值的式子变优化，减少过拟合" class="headerlink" title="深度学习正则化（L1,L2），把对应的损失值的式子变优化，减少过拟合"></a>深度学习正则化（L1,L2），把对应的损失值的式子变优化，减少过拟合</h1><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/29.png' height=px ><h2 id="正则化，即在成本函数中加入一个正则化项-惩罚项-，惩罚模型的复杂度，防止网络过拟合"><a href="#正则化，即在成本函数中加入一个正则化项-惩罚项-，惩罚模型的复杂度，防止网络过拟合" class="headerlink" title="正则化，即在成本函数中加入一个正则化项(惩罚项)，惩罚模型的复杂度，防止网络过拟合"></a><strong>正则化</strong>，<strong>即在成本函数中加入一个正则化项(惩罚项)，惩罚模型的复杂度，防止网络过拟合</strong></h2><h3 id="偏差与方差：是解释学习算法泛化型的一种重要的工具"><a href="#偏差与方差：是解释学习算法泛化型的一种重要的工具" class="headerlink" title="偏差与方差：是解释学习算法泛化型的一种重要的工具"></a>偏差与方差：是解释学习算法泛化型的一种重要的工具</h3><p>泛化误差可分解为偏差、方差与噪声，<strong>泛化性能</strong>是由<strong>学习算法的能力</strong>、<strong>数据的充分性（是不是数据较多）</strong>以及<strong>学习任务本身的难度（这个问题很难解决）</strong>所共同决定的。</p><ul><li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong></li><li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong></li><li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望<strong>泛化误差的下界</strong>，即刻画了<strong>学习问题本身的难度</strong>。</li></ul><h3 id="那么偏差、方差与我们的数据集划分到底有什么关系呢？"><a href="#那么偏差、方差与我们的数据集划分到底有什么关系呢？" class="headerlink" title="那么偏差、方差与我们的数据集划分到底有什么关系呢？"></a>那么偏差、方差与我们的数据集划分到底有什么关系呢？</h3><ul><li>1、训练集的错误率较小，而验证集/测试集的错误率较大，<strong>说明模型存在较大方差，可能出现了过拟合</strong></li><li>2、训练集和测试集的错误率都较大，且两者相近，<strong>说明模型存在较大偏差，可能出现了欠拟合</strong></li><li>3、训练集和测试集的错误率都较小，且两者相近，说明方差和偏差都较小，这个模型效果比较好。</li></ul><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p><strong>对于高方差，有以下几种方式：</strong></p><ul><li>获取更多的数据，使得训练能够包含所有可能出现的情况</li><li><strong>正则化（Regularization）</strong></li><li>寻找更合适的网络结构</li></ul><p>对于高偏差，有以下几种方式：</p><ul><li>扩大网络规模，如添加隐藏层或者神经元数量</li><li>寻找合适的网络架构，使用更大的网络结构，如AlexNet</li><li>训练时间更长一些</li></ul><p>不断尝试，直到找到低偏差、低方差的框架。</p><h4 id="数据集的划分"><a href="#数据集的划分" class="headerlink" title="数据集的划分"></a>数据集的划分</h4><p>损失函数中添加正则化，使权重值减少，复杂度或者异常数据点太多对整体数据的结果有很大影响值，就要减少这个特征的影响，</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.keras.regularizers.L1(l1=0.01)</td><td>L1正则化的添加</td></tr><tr><td>tf.keras.regularizers.L2(l2=0.01)</td><td>L2正则化的添加</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="模型复不复杂是根据权重大不大有关，如果权重都小了神经网络就变稀疏了"><a href="#模型复不复杂是根据权重大不大有关，如果权重都小了神经网络就变稀疏了" class="headerlink" title="模型复不复杂是根据权重大不大有关，如果权重都小了神经网络就变稀疏了"></a>模型复不复杂是根据权重大不大有关，如果权重都小了神经网络就变稀疏了</h4><h3 id="L2：防止模型过分依赖特定的几项特征，削弱对其影响太大的某些特征"><a href="#L2：防止模型过分依赖特定的几项特征，削弱对其影响太大的某些特征" class="headerlink" title="L2：防止模型过分依赖特定的几项特征，削弱对其影响太大的某些特征"></a><strong>L2</strong>：防止模型过分依赖特定的几项特征，削弱对其影响太大的某些特征</h3><h3 id="如果w就是特别的大就是就是会形成过拟合"><a href="#如果w就是特别的大就是就是会形成过拟合" class="headerlink" title="如果w就是特别的大就是就是会形成过拟合"></a>如果w就是特别的大就是就是会形成过拟合</h3><h3 id="在损失函数后面添加一项，那么梯度下降是要减少损失函数的大小，对于L1和L2来讲就是减少正则项的大小就是来减少权重的大小，当λ变大，w就会减小，然后Z就会变的小，"><a href="#在损失函数后面添加一项，那么梯度下降是要减少损失函数的大小，对于L1和L2来讲就是减少正则项的大小就是来减少权重的大小，当λ变大，w就会减小，然后Z就会变的小，" class="headerlink" title="在损失函数后面添加一项，那么梯度下降是要减少损失函数的大小，对于L1和L2来讲就是减少正则项的大小就是来减少权重的大小，当λ变大，w就会减小，然后Z就会变的小，"></a>在损失函数后面添加一项，那么梯度下降是要减少损失函数的大小，对于L1和L2来讲就是减少正则项的大小就是来减少权重的大小，当λ变大，w就会减小，然后Z就会变的小，</h3><p>这里的λ是正则化参数，它是一个需要优化的超参数。L2正则化又称为权重衰减，因为其导致权重趋向于0（但不全是0）</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/30.png' height=px ><h4 id="L1：防止特征过多模型过于复杂，可以使得其中一些W的值直接为0，删除这个特征的影响"><a href="#L1：防止特征过多模型过于复杂，可以使得其中一些W的值直接为0，删除这个特征的影响" class="headerlink" title="L1：防止特征过多模型过于复杂，可以使得其中一些W的值直接为0，删除这个特征的影响"></a><strong>L1</strong>：防止特征过多模型过于复杂，可以使得其中一些W的值直接为0，删除这个特征的影响</h4><p>我们惩罚权重矩阵的绝对值。其中，λ 为正则化参数，是超参数，不同于L2，权重值可能被减少到0.因此，L1对于压缩模型很有用。其它情况下，一般选择优先选择L2正则化。</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/31.png' height=px ><p>下面更新<img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/32.png' height=px ></p><h2 id="2-Dropout对神经元素进行删减，解决过拟合（里面的参数设置0-5到0-8最为合适），添加会更加训练的时间，"><a href="#2-Dropout对神经元素进行删减，解决过拟合（里面的参数设置0-5到0-8最为合适），添加会更加训练的时间，" class="headerlink" title="2.Dropout对神经元素进行删减，解决过拟合（里面的参数设置0.5到0.8最为合适），添加会更加训练的时间，"></a>2.Dropout对神经元素进行删减，解决过拟合（里面的参数设置0.5到0.8最为合适），添加会更加训练的时间，</h2><p>在每个迭代过程中<strong>，随机选择某些节点，并且删除前向和后向连接</strong>，因此，每个迭代过程都会有不同的节点组合，从而导致不同的输出，这可以看成机器学习中的集成方法（ensemble technique）。集成模型一般优于单一模型，因为它们可以捕获更多的随机性。相似地，dropout使得神经网络模型优于正常的模型。</p><ul><li>1、训练过程<ul><li>1、神经元随机失效，概率为P</li><li>2、并且在神经元存在且工作的状态下，权重才会更新，权重更新的越多理论上会变得更大，是因为本来有100个权重，经过dropout后变成了80（如设置0.8）理论上这80个权重就会变大但是现实不是变大的，反而会变小，与学习迭代的次数成正比，由每一轮每个神经学到的特征就不一样的，每个神经元变的更加独立了，泛化型变强了。</li></ul></li><li>2、测试过程<ul><li>1、神经元随机失效，概率为0</li><li>2、所有的神经元都会参与计算，大于训练时候的任意一个模型的计算量</li></ul></li><li>3、模型过程伪代码过程</li></ul><h3 id="随机进行失活："><a href="#随机进行失活：" class="headerlink" title="随机进行失活："></a>随机进行失活：</h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/33.png' height=px ><h3 id="上图的解释：这样就是每个神经学到的特征就不一样的，每个神经元变的更加独立了"><a href="#上图的解释：这样就是每个神经学到的特征就不一样的，每个神经元变的更加独立了" class="headerlink" title="上图的解释：这样就是每个神经学到的特征就不一样的，每个神经元变的更加独立了"></a>上图的解释：这样就是每个神经学到的特征就不一样的，每个神经元变的更加独立了</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.keras.layers.<strong>Dropout</strong>(     rate, noise_shape=None, seed=None, **kwargs )</td><td><strong>rate</strong>：在0和1之间浮动。要删除的输入单位的分数。对应的是要丢掉的比例， <strong>noise_shape</strong>：1D整数张量，表示将与输入相乘的二进制丢失掩码的形状，                                            <strong>seed</strong>：用作随机种子的Python整数</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="dropout注意事项"><a href="#dropout注意事项" class="headerlink" title="dropout注意事项"></a>dropout注意事项</h2><p>1：dropout会增加训练的时间                         </p><p>2：因为dropout在梯度下降中引入了大量的<strong>噪声导致梯度相互抑制（因为每一次学习到的不一样所有就会产生噪声）</strong>，因此学习速率要增加10-100倍。另外一个减少噪声的方法是用momentum，momentum对于标准网络一般采用0.9，对于dropout网络一般是0.95-0.99。两个方法可以同时采用</p><p>3：防止学习过快导致网络增长太大，一般给隐藏层权重的norm一个上限c，c一般取值3-4dropout和max-normalization、large decaying learning rates and high momentum等组合起来效果更好</p><p>4：Dropout Rate：一般取值0.5-0.8之间，drop比例p越小，要求隐含层n越大，训练也会越慢</p><p>5：调试时候使用技巧：</p><ul><li>先确定网络没问题，再打开dropout训练测试：dropout 的缺点是成本函数无法被明确定义，因为每次会随机消除一部分神经元，所以参数也无法确定具体哪一些，在反向传播的时候带来计算上的麻烦，也就无法保证当前<strong>网络是否损失函数下降的</strong>。如果要使用droupout，会先关闭这个参数，保证损失函数是单调下降的，确定网络没有问题，再次打开droupout才会有效。</li></ul><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/34.png' height=px ><h2 id="神经网络调优"><a href="#神经网络调优" class="headerlink" title="神经网络调优"></a>神经网络调优</h2><p>算法层面：</p><ul><li>学习率：α</li><li>β1,β2,ϵ: Adam 优化算法的超参数，常设为 0.9、0.999、</li><li>λ：正则化网络参数，</li></ul><p>网络层面：</p><ul><li>hidden units：各隐藏层神经元个数</li><li>layers：神经网络层数</li></ul><p>合理的参数设置</p><ul><li>学习率α\alphaα：0.0001、0.001、0.01、0.1，跨度稍微大一些。</li><li>算法参数β\betaβ， 0.999、0.9995、0.998等，尽可能的选择接近于1的值</li></ul><h2 id="3-提前停止：损失值不变或者变大进行停止"><a href="#3-提前停止：损失值不变或者变大进行停止" class="headerlink" title="3.提前停止：损失值不变或者变大进行停止"></a>3.提前停止：损失值不变或者变大进行停止</h2><p>当验证集的性能越来越差时或者性能不再提升，则立即停止对该模型的训练。 这被称为提前停止。</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.keras.callbacks.<strong>EarlyStopping</strong>(     monitor=’val_loss’,  patience=5 )</td><td><strong>monitor</strong>参数表示监测量，这里val_loss表示验证集损失，                  <strong>patience</strong>参数设置的数量，当在这个过程性能无提升时会停止训练</td></tr><tr><td></td><td></td></tr><tr><td>也可以设置丢弃的概率</td><td></td></tr><tr><td>self.keras.layers.Dropout(rete=0.2)</td><td>设置了丢弃的概率</td></tr><tr><td>x=self.dropout(x,training=True)</td><td>模型输入的时候，training=True才会启用</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h1 id="批标准化-主要为了解决数据分布的变化，为了防止梯度消失问题和梯度爆炸问题，加速了能更快找到最小值，"><a href="#批标准化-主要为了解决数据分布的变化，为了防止梯度消失问题和梯度爆炸问题，加速了能更快找到最小值，" class="headerlink" title="批标准化:主要为了解决数据分布的变化，为了防止梯度消失问题和梯度爆炸问题，加速了能更快找到最小值，"></a>批标准化:主要为了解决数据分布的变化，为了防止梯度消失问题和梯度爆炸问题，加速了能更快找到最小值，</h1><h4 id="为了规范x的"><a href="#为了规范x的" class="headerlink" title="为了规范x的"></a>为了规范x的</h4><h3 id="对每一批次的输入特征和输出的做标准化处理"><a href="#对每一批次的输入特征和输出的做标准化处理" class="headerlink" title="对每一批次的输入特征和输出的做标准化处理"></a>对每一批次的输入特征和输出的做标准化处理</h3><h3 id="就是对数据进行批标准化，使得数据满足均值为0，方差为1的正态分布，后面是有两个超参数进行调参，经过调参后不一定会符合正态分布的，其主要作用是缓解DNN训练中的梯度消失、爆炸现象，加快模型的训练速度"><a href="#就是对数据进行批标准化，使得数据满足均值为0，方差为1的正态分布，后面是有两个超参数进行调参，经过调参后不一定会符合正态分布的，其主要作用是缓解DNN训练中的梯度消失、爆炸现象，加快模型的训练速度" class="headerlink" title="就是对数据进行批标准化，使得数据满足均值为0，方差为1的正态分布，后面是有两个超参数进行调参，经过调参后不一定会符合正态分布的，其主要作用是缓解DNN训练中的梯度消失、爆炸现象，加快模型的训练速度"></a>就是对数据进行批标准化，使得数据满足均值为0，方差为1的正态分布，后面是有两个超参数进行调参，经过调参后不一定会符合正态分布的，其主要作用是缓解DNN训练中的梯度消失、爆炸现象，加快模型的训练速度</h3><p>在大型网络中BN是相当于每一层都有，后面的不进行标准化</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/35.png' height=px ><h2 id="BN层对单个神经元进行，网络训练时一个-mini-batch-的数据来计算该神经元的均值，方差归一化并重构"><a href="#BN层对单个神经元进行，网络训练时一个-mini-batch-的数据来计算该神经元的均值，方差归一化并重构" class="headerlink" title="BN层对单个神经元进行，网络训练时一个 mini-batch 的数据来计算该神经元的均值，方差归一化并重构"></a>BN层对单个神经元进行，网络训练时一个 mini-batch 的数据来计算该神经元的均值，方差归一化并重构</h2><h3 id="最好在中间的每一层就加上BN层，统计一下均值和方差，表示数据的差异很大的时候，膜的向量会很大，整体的让他等量的变大或变小进行重构，标准化，重构，让传播更可控一些，"><a href="#最好在中间的每一层就加上BN层，统计一下均值和方差，表示数据的差异很大的时候，膜的向量会很大，整体的让他等量的变大或变小进行重构，标准化，重构，让传播更可控一些，" class="headerlink" title="最好在中间的每一层就加上BN层，统计一下均值和方差，表示数据的差异很大的时候，膜的向量会很大，整体的让他等量的变大或变小进行重构，标准化，重构，让传播更可控一些，"></a>最好在中间的每一层就加上BN层，统计一下均值和方差，表示数据的差异很大的时候，膜的向量会很大，整体的让他等量的变大或变小进行重构，标准化，重构，让传播更可控一些，</h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/36.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/37.png' height=px ><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>批量标准化层</td><td></td></tr><tr><td>keras.layers.<strong>BatchNormalization</strong>(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer=’zeros’, gamma_initializer=’ones’, moving_mean_initializer=’zeros’, moving_variance_initializer=’ones’, beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)</td><td><strong>参数</strong>                                                                                               <strong>axis</strong>: 整数，需要标准化的轴（通常是特征轴）。例如，在 <code>data_format=&quot;channels_first&quot;</code> 的 <code>Conv2D</code> 层之后，&lt;br在 <code>BatchNormalization</code> 中设置 <code>axis=1 </code>           <strong>momentum</strong>: 移动均值和移动方差的动量                      <strong>epsilon</strong>: 增加到方差的小的浮点数，以避免除以零        <strong>center</strong>: 如果为 True，把 <code>beta</code> 的偏移量加到标准化的张量上。如果为 False， <code>beta</code> 被忽略                                                                              <strong>scale</strong>: 如果为 True，乘以 <code>gamma</code>。如果为 False，<code>gamma</code> 不使用。当下一层为线性层（或者例如 <code>nn.relu</code>），这可以被禁用，因为缩放将由下一层完成                                  <strong>beta_initializer</strong>: beta 权重的初始化方法       <strong>gamma_initializer</strong>: 伽马 权重的初始化方法              <strong>moving_mean_initializer</strong>: 移动均值的初始化方法        <strong>moving_variance_initializer</strong>: 移动方差的初始化方法<strong>beta_regularizer</strong>: 可选的 beta 权重的正则化方法     <strong>gamma_regularizer</strong>: 可选的 gamma 权重的正则化方 法<strong>beta_constraint</strong>: 可选的 beta 权重的约束方法     <strong>gamma_constraint</strong>: 可选的 gamma 权重的约束方法</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><p>输入均值在靠近0的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。因此，需要用 γ 和 β 对标准化后的结果做进一步处理</p><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/38.png' height=px ><p>上式的引入两个超参数γ和β也是跟着初始化的，也运用到了梯度下降公式中（如同W和b一样），所以可以用各种更新的γ和β的值，如同更新神经网络的权重一样</p><h3 id="为什么批标准化能够是优化过程变得简单"><a href="#为什么批标准化能够是优化过程变得简单" class="headerlink" title="为什么批标准化能够是优化过程变得简单"></a>为什么批标准化能够是优化过程变得简单</h3><p><strong>协变量偏移</strong>，那么有一个解释叫做 <strong>在网络当中数据的分布会随着不同数据集改变</strong> 。这是网络中存在的问题。那我们一起来看一下数据本身分布是在这里会有什么问题。</p><ul><li><strong>1、Batch Normalization的作用就是减小Internal Covariate Shift（协变量偏移） 所带来的影响，让模型变得更加健壮，鲁棒性（Robustness）更强。</strong></li><li><strong>2 、Batch Normalization 的作用，使得均值和方差保持固定（由每一层γ和β决定），不同层学习到不同的分布状态</strong></li><li><strong>3、因此后层的学习变得更容易一些。Batch Normalization 减少了各层 W 和 b 之间的耦合性，让各层更加独立，实现自我训练学习的效果</strong></li></ul><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/39.png' height=px ><h3 id="网址图形可视化"><a href="#网址图形可视化" class="headerlink" title="网址图形可视化"></a>网址图形可视化</h3><p>pip install tensorboard：进入网址可视化</p><p>tensorboard –logdir=graph ：找到对应的文件名打印出来网址进行复制就可以看了</p><h2 id="数据的增强：也是为了模型的泛化性，让网络能学到更多特征的能力"><a href="#数据的增强：也是为了模型的泛化性，让网络能学到更多特征的能力" class="headerlink" title="数据的增强：也是为了模型的泛化性，让网络能学到更多特征的能力"></a>数据的增强：也是为了模型的泛化性，让网络能学到更多特征的能力</h2><p>数据增强类别：</p><ul><li>离线增强。预先进行所有必要的变换，从根本上增加数据集的规模（例如，通过翻转所有图像，保存后数据集数量会增加2倍）。</li><li>在线增强，或称为动态增强。可通过对即将输入模型的小批量数据的执行相应的变化，这样同一张图片每次训练被随机执行一些变化操作，相当于不同的数据集了。</li></ul><h2 id="几何变换，颜色的变换"><a href="#几何变换，颜色的变换" class="headerlink" title="几何变换，颜色的变换"></a>几何变换，颜色的变换</h2><p>几何变换类即对图像进行几何变换，包括<strong>翻转，旋转，裁剪，变形，缩放</strong>等各类操作</p><p>常见的包括<strong>噪声、模糊、颜色变换、擦除、填充</strong>等等</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tf.image.random_flip_left_right</td><td>翻转</td></tr><tr><td></td><td></td></tr><tr><td>keras.preprocessing.image.ImageDataGenerator(                                                                                         rotation_range=0, #整数。随机旋转的度数范围。                width_shift_range=0.0, #浮点数、宽度平移                height_shift_range=0.0, #浮点数、高度平移                brightness_range=None, # 亮度调整                shear_range=0.0, # 裁剪                                                                 zoom_range=0.0, #浮点数 或 [lower, upper]。随机缩放范围                                                                              horizontal_flip=False, # 左右翻转                vertical_flip=False, # 垂直翻转                                                     rescale=None # 尺度调整             )</td><td>数据增强</td></tr><tr><td></td><td></td></tr><tr><td>读取文件并数据进行增强</td><td></td></tr><tr><td>def flow_from_directory(self,<br/>                        directory: Any,<br/>                        target_size: Tuple[int, int] = (256, 256),<br/>                        color_mode: str = ‘rgb’,<br/>                        classes: Any = None,<br/>                        class_mode: str = ‘categorical’,<br/>                        batch_size: int = 32,<br/>                        shuffle: bool = True,<br/>                        seed: Any = None,<br/>                        save_to_dir: Any = None,<br/>                        save_prefix: str = ‘’,<br/>                        save_format: str = ‘png’,<br/>                        follow_links: bool = False,<br/>                        subset: Any = None,<br/>                        interpolation: str = ‘nearest’) -&gt; DirectoryIterator</td><td>读取文件并数据进行增强                                                      目录 （<strong>directory</strong>）：字符串，目标目录的路径。每个类应该包含一个子目录。生成器中将包含每个子目录树中的任何PNG、JPG、BMP、PPM或TIF图像。                                                                                                 目标大小（<strong>target_size</strong>）：整数的元组（高度、宽度）。默认值：（256，256）。将调整找到的所有图像的尺寸。<br/>    颜色_模式（<strong>color_mode</strong>）：“灰度”、“rgb”、“rgba”之一。默认值：“rgb”。是否将图像转换为具有1、3或4个通道。&lt;br/                         类列表（*<em>classes**）：类子目录的可选列表。（例如，“狗”、“猫”）。默认值：无。如果没有提供，将根据目录下的子目录名称/结构自动推断类列表，其中每个子目录将被视为不同的类（将映射到标签索引的类的顺序将是字母数字）。包含从类名到类索引的映射的字典可以通过属性class_indexes获得。<br/>    类模式：确定返回的标签数组的类型：“分类（“categorical”）”、“二进制（“binary”）”、“稀疏（“sparse”）”、“输入（“input”）”、“无（None）”模式之一。默认值：“分类”。<br/><br/>        “categorial”则是2维one-hot编码标签；<br/>        “binary”则是一维二进制标签；<br/>        “sparse”是1维整数标签；<br/>        “input”是与输入图像相同的图像（主要用于自动编码器）；<br/>        None，则不会返回任何标签（生成器将只生成一批图像数据，与model.predict_generator（）一起使用非常有用）。请注意，在类模式为“无”的情况下，数据仍然需要驻留在目录的子目录中，以便它正常工作。<br/><br/>    批次大小（batch_size）：数据批次的大小（默认值：32）。<br/>    随机播放（shuffle）：是否随机播放数据（默认值：True）。如果设置为False，则按字母数字顺序对数据进行排序。<br/>    种子（seed）：可选的随机种子，用于洗牌和转换。<br/>    保存路径（save_to_dir）:none或str（默认值：none）。这允许您选择指定一个目录，将生成的增强图片保存到该目录（对于可视化所做的操作很有用）。<br/>    保存前缀（save_prefix）: Str. Prefix用于保存图片的文件名（仅当设置了save_to_dir时才相关）。<br/>    保存格式（save_format）：“png”、“jpeg”之一（仅当设置了save_to_dir时才相关）。默认值：“png”。<br/>    跟随链接（follow_links）：是否跟随类子目录内的符号链接（默认值：False）。<br/>    子集（subset）：如果在ImageDataGenerator中设置了验证分割（validation_split），则为数据的子集（训练集”training” 或验证集”validation”）。<br/>    插值（interpolation）：当目标大小与加载的图像大小不同时，用于对图像重新采样的插值方法。支持的方法有最近邻”nearest”、双线性”bilinear”和双三次”bicubic”。如果安装了PIL版本1.1.3或更高版本，还支持“lanczos”。如果安装了PIL 3.4.0或更高版本，还支持“Box”和“Hamming”。默认情况下，使用“nearest”。<br/><br/>返回值<br/><br/>一个产生（x，y）元组的目录迭代器（DirectoryIterator）。<br/>其中x是包含一批（batch_size，</em> target_size，channels）类型的图像的numpy数组，y是对应标签的numpy数组。<br/></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><pre class=" language-python"><code class="language-python">random_brightness<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：以随机因素调整图像的亮度。random_contrast<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：通过随机因素调整图像的对比度。random_crop<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：将张量随机裁剪为给定大小。random_flip_left_right<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：随机水平翻转图像（从左到右）。random_flip_up_down<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：垂直（上下颠倒）随机翻转图像。random_hue<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：通过随机因素调整RGB图像的色调。random_jpeg_quality<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：随机更改jpeg编码质量，以产生jpeg噪声。random_saturation<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：通过随机因子调整RGB图像的饱和度。resize<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：调整大小images以size使用指定的method。resize_with_crop_or_pad<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：裁剪和<span class="token operator">/</span>或将图像填充到目标宽度和高度。resize_with_pad<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：调整图像大小并将其填充到目标宽度和高度。rgb_to_grayscale<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：将一个或多个图像从RGB转换为灰度。rgb_to_hsv<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：将一个或多个图像从RGB转换为HSV。rgb_to_yiq<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：将一个或多个图像从RGB转换为YIQ。rgb_to_yuv<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>：将一个或多个图像从RGB转换为YUV。</code></pre><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/40.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/41.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/42.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/43.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/44.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/45.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/46.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/47.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/56.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/48.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/49.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/50.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/51.png' height=px ><h3 id="步骤7"><a href="#步骤7" class="headerlink" title="步骤7"></a>步骤7</h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/52.png' height=px ><h3 id=""><a href="#" class="headerlink" title=""></a><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/53.png' height=px ></h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/53.png' height=px ><h3 id="步骤9求出特征与均值的差平方函数的导数"><a href="#步骤9求出特征与均值的差平方函数的导数" class="headerlink" title="步骤9求出特征与均值的差平方函数的导数"></a>步骤9求出特征与均值的差平方函数的导数</h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/54.png' height=px ><img src='https://boes.oss-cn-beijing.aliyuncs.com/cv%E5%9F%BA%E7%A1%801/55.png' height=px >]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1神经网络运用的&quot;&gt;&lt;a href=&quot;#1神经网络运用的&quot; class=&quot;headerlink&quot; title=&quot;1神经网络运用的&quot;&gt;&lt;/a&gt;&lt;strong&gt;1&lt;/strong&gt;神经网络运用的&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数&lt;/th&gt;</summary>
      
    
    
    
    <category term="CV" scheme="https://xiaoyvlongoing.github.io/categories/CV/"/>
    
    
    <category term="CV" scheme="https://xiaoyvlongoing.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>线性回归，逻辑回归</title>
    <link href="https://xiaoyvlongoing.github.io/2020/11/04/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>https://xiaoyvlongoing.github.io/2020/11/04/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id>
    <published>2020-11-04T08:54:11.000Z</published>
    <updated>2021-11-05T06:34:57.373Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1：线性回归：属于预测模型"><a href="#1：线性回归：属于预测模型" class="headerlink" title="1：线性回归：属于预测模型"></a>1：线性回归：属于预测模型</h2><p><strong>评估线性损失的时</strong>：最小二乘法</p><h3 id="1-1：线性回归：是利用一个或者多个特征值（自变量）和目标值（因变量）之间的进行建模的一种分析方式"><a href="#1-1：线性回归：是利用一个或者多个特征值（自变量）和目标值（因变量）之间的进行建模的一种分析方式" class="headerlink" title="1.1：线性回归：是利用一个或者多个特征值（自变量）和目标值（因变量）之间的进行建模的一种分析方式"></a>1.1：线性回归：是利用一个或者多个特征值（自变量）和目标值（因变量）之间的进行建模的一种分析方式</h3><ul><li><h4 id="线性回归：就是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式"><a href="#线性回归：就是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式" class="headerlink" title="线性回归：就是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式"></a>线性回归：就是利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式</h4></li><li><p><strong>特点：</strong>如果自变量只有一个为单变量回归，多个的话为多元回归</p><p>下面公式w为权重值</p></li></ul><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/1.png' height=px ><h3 id="1-2：非线性关系："><a href="#1-2：非线性关系：" class="headerlink" title="1.2：非线性关系："></a>1.2：非线性关系：</h3><ul><li>下边的公式为非线性关系</li></ul><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/2.png' height=px ><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>线性回归函数</strong></td><td></td></tr><tr><td>sklearn.linear_model.<strong>LinearRegression</strong>(fit_intercept=True)</td><td><strong>线性回归</strong>，fit_intercept=True是否计算偏置</td></tr><tr><td>LinearRegression.<strong>coef_</strong></td><td><strong>回归（权重值）系数：</strong>是多少</td></tr><tr><td>estimator.<strong>predict</strong>([[特征值,目标值]])</td><td><strong>形成的模型可以预测的结果：</strong>是多少</td></tr><tr><td>estimator.<strong>intercept_</strong></td><td>线性回规的偏置</td></tr></tbody></table><h3 id="1-3：损失函数：（（权重-输入数据）—-真实数据）-2"><a href="#1-3：损失函数：（（权重-输入数据）—-真实数据）-2" class="headerlink" title="1.3：损失函数：（（权重*输入数据）—  真实数据）**2"></a>1.3：损失函数：（（权重*输入数据）—  真实数据）**2</h3><h3 id="纯误差：-权重-输入数据）—-真实数据"><a href="#纯误差：-权重-输入数据）—-真实数据" class="headerlink" title="纯误差：                              权重*输入数据）—  真实数据"></a>纯误差：                              权重*输入数据）—  真实数据</h3><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/3.png' height=px ><h3 id="1-4：均方误差"><a href="#1-4：均方误差" class="headerlink" title="1.4：均方误差"></a>1.4：均方误差</h3><ul><li><h4 id="公式："><a href="#公式：" class="headerlink" title="公式："></a>公式：<img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/4.png' height=px ></h4></li></ul><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>sklearn.metrics.<strong>mean_squared_error</strong>(y_true, y_pred)</td><td><strong>均方误差回归损失：</strong>                y_true:真实值                             y_pred:预测值                               return:浮点数结果</td></tr><tr><td>mean_absolute_error(y_valid, y_pre)</td><td>平均绝对值误差</td></tr></tbody></table><h2 id="2：-正规方程：直接求出最小值（适合小型数据集）"><a href="#2：-正规方程：直接求出最小值（适合小型数据集）" class="headerlink" title="2： 正规方程：直接求出最小值（适合小型数据集）"></a>2： 正规方程：直接求出最小值（适合小型数据集）</h2><h3 id="正规方程-–-一蹴而就"><a href="#正规方程-–-一蹴而就" class="headerlink" title="正规方程 – 一蹴而就"></a>正规方程 – 一蹴而就</h3><ul><li>利用矩阵的逆,转置进行一步求解</li><li>只是适合样本和特征比较少的情况</li></ul><h3 id="2-1：优化算法公式："><a href="#2-1：优化算法公式：" class="headerlink" title="2.1：优化算法公式："></a>2.1：优化算法公式：</h3><ul><li><h3 id="正规方程：直接求出最小公式"><a href="#正规方程：直接求出最小公式" class="headerlink" title="正规方程：直接求出最小公式"></a>正规方程：直接求出最小公式</h3><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/5.png' height=px ></li></ul></li><li><h3 id="推导公式："><a href="#推导公式：" class="headerlink" title="推导公式："></a>推导公式：</h3><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/6.png' height=px ></li></ul></li></ul><h2 id="3-1：梯度下降（适合大型数据集）"><a href="#3-1：梯度下降（适合大型数据集）" class="headerlink" title="3.1：梯度下降（适合大型数据集）"></a>3.1：梯度下降（适合大型数据集）</h2><h3 id="梯度下降定义："><a href="#梯度下降定义：" class="headerlink" title="梯度下降定义："></a>梯度下降定义：</h3><ul><li><h4 id="在单变量的函数中，代表着函数在某个给定点的切线的斜率；"><a href="#在单变量的函数中，代表着函数在某个给定点的切线的斜率；" class="headerlink" title="在单变量的函数中，代表着函数在某个给定点的切线的斜率；"></a>在单变量的函数中，代表着函数在某个给定点的切线的斜率；</h4></li><li><h4 id="在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；"><a href="#在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；" class="headerlink" title="在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；"></a>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；</h4></li><li><p><strong>梯度下降的公式：</strong><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/7.png' height=px ></p></li><li><p><strong>α是什么：</strong>在梯度下降中被称为学习率或者是步长，可以通过α来控制每一步走的距离,不能走太快或者走的太慢</p></li><li><p><strong>梯度要乘一个负数</strong>：意味着就是朝着梯度相反的方向前进，降的时候最快</p></li></ul><h2 id="3-2：梯度下降的概念：负梯度就是求导"><a href="#3-2：梯度下降的概念：负梯度就是求导" class="headerlink" title="3.2：梯度下降的概念：负梯度就是求导"></a>3.2：梯度下降的概念：负梯度就是求导</h2><h3 id="3-21：概念性质："><a href="#3-21：概念性质：" class="headerlink" title="3.21：概念性质："></a>3.21：概念性质：</h3><ul><li><h4 id="步长："><a href="#步长：" class="headerlink" title="步长："></a>步长：</h4><ul><li><strong>步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度</strong></li></ul></li><li><h4 id="特征："><a href="#特征：" class="headerlink" title="特征："></a>特征：</h4><ul><li><strong>指的是样本中输入部分</strong></li></ul></li><li><h4 id="假设函数："><a href="#假设函数：" class="headerlink" title="假设函数："></a>假设函数：</h4><ul><li><strong>在监督学习中，为了拟合输入样本，而使用的假设函数，记为hθ(x)： hθ(x)=θ0+θ1x</strong>，，，，，，θ0：权重值</li></ul></li><li><h4 id="损失函数-均方误差"><a href="#损失函数-均方误差" class="headerlink" title="损失函数(均方误差):"></a>损失函数(均方误差):</h4><ul><li>为了评估模型拟合的好坏，<strong>通常用损失函数来度量拟合的程度。</strong>损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。</li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/8.png' height=px ></li></ul></li></ul><h2 id="4：梯度下降的种类："><a href="#4：梯度下降的种类：" class="headerlink" title="4：梯度下降的种类："></a>4：梯度下降的种类：</h2><h3 id="4-1：全梯度下降（FGD）：代入全部样本在运行过程中不能添加新样本"><a href="#4-1：全梯度下降（FGD）：代入全部样本在运行过程中不能添加新样本" class="headerlink" title="4.1：全梯度下降（FGD）：代入全部样本在运行过程中不能添加新样本"></a>4.1：全梯度下降（FGD）：代入全部样本在运行过程中不能添加新样本</h3><h4 id="作法：在进行计算的时候-计算所有样本的误差平均值-作为我的目标函数"><a href="#作法：在进行计算的时候-计算所有样本的误差平均值-作为我的目标函数" class="headerlink" title="作法：在进行计算的时候,计算所有样本的误差平均值,作为我的目标函数"></a><strong>作法：</strong>在进行计算的时候,计算所有样本的误差平均值,作为我的目标函数</h4><p><strong>缺点</strong>：更新一次权重都需要计算所有样本误差，而实际问题中经常有上亿的训练样本，故效率偏低，且容易陷入局部最优解</p><ul><li><strong>批量梯度下降法</strong>，是梯度下降法最常用的形式，<strong>具体做法也就是在更新参数时使用所有的样本来进行更新。</strong> <strong>计算训练集所有样本误差</strong>，<strong>对其求和再取平均值作为目标函数</strong>。</li><li><strong>权重向量</strong>沿其梯度相反的方向移动，从而使当前目标函数减少得最多。</li><li><strong>公式θ为权重值</strong>：<img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/9.png' height=px ></li><li><strong>全梯度下降性质：</strong><ul><li>因为在<strong>执行每次更新时</strong>，我们需要在整个数据集上<strong>计算所有的梯度</strong>，所以批梯度下降法的<strong>速度会很慢</strong>，同时，批梯度下降法<strong>无法处理超出内存容量限制的数据集</strong>。</li><li><strong>批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本</strong></li></ul></li></ul><h3 id="4-2：-随机梯度下降算法-SGD-：只代入一个样本更新循环重复过程"><a href="#4-2：-随机梯度下降算法-SGD-：只代入一个样本更新循环重复过程" class="headerlink" title="4.2： 随机梯度下降算法(SGD)：只代入一个样本更新循环重复过程"></a>4.2： 随机梯度下降算法(SGD)：只代入一个样本更新循环重复过程</h3><h4 id="作法：每次只选择一个样本进行考核"><a href="#作法：每次只选择一个样本进行考核" class="headerlink" title="作法：每次只选择一个样本进行考核"></a>作法：每次只选择一个样本进行考核</h4><p><strong>缺点</strong>：SG每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解，而且每一轮的梯度更新都完全与上一轮的数据无关</p><table><thead><tr><th>函数</th><th>函数的性质</th></tr></thead><tbody><tr><td>from   sklearn.linear_model  import   <strong>SGDRegressor</strong>(loss=”squared_loss”, fit_intercept=True,max_iter=， learning_rate                                                            =’invscaling’, eta0=0.01)</td><td><strong>SGDRegressor</strong>类实现了随机梯度下降学习，<strong>loss函数和正则化惩罚项</strong>来拟合线性回归模型：                                                                             <strong>1：loss</strong>:损失类型  <strong>loss=”squared_loss”: 普通最小二乘法</strong>                 <strong>2：fit_intercept</strong>：是否计算偏置                                                             <strong>3：max_iter=数值</strong>：最大迭代次数，又称走了多少步                                                      <strong>4：learning_rate</strong> : string, optional：两个选择                                                              <strong>4.1：学习率填充</strong>                                                                                      **’constant’: eta = eta0 **：常数                                                                           <strong>‘optimal’: eta = 1.0 / (alpha（步长） * (t + t0)) [default]</strong>                                             随着迭代次数进行优化                                                                                   ‘invscaling’: eta = eta0 / pow(t, power_t)                                     <strong>power_t=0.25:存在父类当中</strong>                                                                      <strong>对于一个常数值的学习率来说，可以使用                                      learning_rate=’constant’                                                                              5：并使用eta0来指定学习率。</strong></td></tr><tr><td>SGDRegressor.coef_</td><td>回归系数</td></tr><tr><td>SGDRegressor.intercept_</td><td>偏置</td></tr></tbody></table><ul><li><h4 id="随机梯度下降算法：每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。"><a href="#随机梯度下降算法：每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。" class="headerlink" title="随机梯度下降算法：每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。"></a>随机梯度下降算法：<strong>每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。</strong></h4></li><li><h4 id="公式：-1"><a href="#公式：-1" class="headerlink" title="公式："></a>公式：<img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/10.png' height=px ></h4></li></ul><h3 id="4-3：小批量梯度下降算法：每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FGD迭代更新权重。"><a href="#4-3：小批量梯度下降算法：每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FGD迭代更新权重。" class="headerlink" title="4.3：小批量梯度下降算法：每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FGD迭代更新权重。"></a>4.3：小批量梯度下降算法：<strong>每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FGD迭代更新权重。</strong></h3><h4 id="作法：选择一部分样本进行考核"><a href="#作法：选择一部分样本进行考核" class="headerlink" title="作法：选择一部分样本进行考核"></a>作法：选择一部分样本进行考核</h4><ul><li><strong>公式：</strong><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/11.png' height=px ></li></ul><h3 id="4-4：-随机平均梯度下降算法-SAG-：在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。"><a href="#4-4：-随机平均梯度下降算法-SAG-：在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。" class="headerlink" title="4.4： 随机平均梯度下降算法(SAG)：在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。"></a>4.4： 随机平均梯度下降算法(SAG)：<strong>在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数</strong>。</h3><h4 id="作法：会给每个样本都维持一个平均值-后期计算的时候-参考这个平均值"><a href="#作法：会给每个样本都维持一个平均值-后期计算的时候-参考这个平均值" class="headerlink" title="作法：会给每个样本都维持一个平均值,后期计算的时候,参考这个平均值"></a>作法：会给每个样本都维持一个平均值,后期计算的时候,参考这个平均值</h4><ul><li><strong>公式为：</strong><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/12.png' height=px ></li><li><strong>随机平均梯度下降算法(SAG)解释</strong>：我们知道sgd是当前权重减去步长乘以梯度，得到新的权重。sag中的a，就是平均的意思，具体说，就是在第k步迭代的时候，我考虑的这一步和前面n-1个梯度的平均值，当前权重减去步长乘以最近n个梯度的平均值。</li></ul><h2 id="5：欠拟合和过拟合"><a href="#5：欠拟合和过拟合" class="headerlink" title="5：欠拟合和过拟合"></a>5：欠拟合和过拟合</h2><p><strong>如何优化训练数据</strong> —&gt; 主要用于<strong>解决欠拟合问题</strong>—》集成学习中boosting：逐步增强学习</p><p><strong>如何提升泛化性能</strong> —&gt; 主要用于<strong>解决过拟合问题</strong>—》集成学习中Bagging：采样学习集成</p><h4 id="5-1：欠拟合：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。-模型过于简单"><a href="#5-1：欠拟合：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。-模型过于简单" class="headerlink" title="5.1：欠拟合：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。(模型过于简单)"></a>5.1：欠拟合：一个假设<strong>在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据</strong>，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</h4><ul><li><h4 id="原因：学习到的数据的特征过少"><a href="#原因：学习到的数据的特征过少" class="headerlink" title="原因：学习到的数据的特征过少"></a>原因：学习到的数据的特征过少</h4></li><li><h4 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h4><ul><li>添加其他特征：“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段</li><li>添加多项式特征：将线性模型通过添加二次项或者三次项使模型泛化能力更强。</li></ul></li></ul><h4 id="5-2：过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合，-但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。-模型过于复杂"><a href="#5-2：过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合，-但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。-模型过于复杂" class="headerlink" title="5.2：过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)"></a>5.2：过拟合：一个假设<strong>在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据</strong>，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</h4><ul><li><h4 id="原因：原始特征过多，存在一些嘈杂特征，模型过于复杂，模型尝试去兼顾各个测试点"><a href="#原因：原始特征过多，存在一些嘈杂特征，模型过于复杂，模型尝试去兼顾各个测试点" class="headerlink" title="原因：原始特征过多，存在一些嘈杂特征，模型过于复杂，模型尝试去兼顾各个测试点"></a>原因：原始特征过多，存在一些嘈杂特征，模型过于复杂，模型尝试去兼顾各个测试点</h4></li><li><h4 id="解决办法：-1"><a href="#解决办法：-1" class="headerlink" title="解决办法："></a>解决办法：</h4><ul><li>重新清洗数据：可能数据不存导致</li><li>增大数据的训练量：训练数据占总比太小</li><li>正则化</li><li>减少特征维度，防止维灾难</li></ul></li></ul><h2 id="6：正则化：复杂度或者异常数据点太多减少这个特征的影响"><a href="#6：正则化：复杂度或者异常数据点太多减少这个特征的影响" class="headerlink" title="6：正则化：复杂度或者异常数据点太多减少这个特征的影响"></a>6：正则化：复杂度或者异常数据点太多减少这个特征的影响</h2><ul><li><strong>正则化越大权重值越小</strong></li><li><strong>比如训练集表现很好，测试集表现不好时让正则化惩罚系数变的大一些，限值模型的过拟合</strong></li><li><strong>如果训练和测试集表现的都不错，让正则化惩罚系数小一些</strong></li></ul><h4 id="正则化含义：在学习的时候，数据提供的特征有些影响模型的复杂度或者异常数据点太多在学习的时候应该减少这个特征的影响（甚至去删除某个特征的影响）"><a href="#正则化含义：在学习的时候，数据提供的特征有些影响模型的复杂度或者异常数据点太多在学习的时候应该减少这个特征的影响（甚至去删除某个特征的影响）" class="headerlink" title="正则化含义：在学习的时候，数据提供的特征有些影响模型的复杂度或者异常数据点太多在学习的时候应该减少这个特征的影响（甚至去删除某个特征的影响）"></a>正则化含义：在学习的时候，数据提供的特征有些影响模型的复杂度或者异常数据点太多在学习的时候应该减少这个特征的影响（甚至去删除某个特征的影响）</h4><p>复杂度：是</p><ul><li><strong>L1正则化：</strong><ul><li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li><li>LASSO回归</li></ul></li><li><strong>L2正则化：</strong><ul><li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li><li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li><li>Ridge回归</li></ul></li></ul><h4 id="Ridge-ElasticNet-Lasso的API"><a href="#Ridge-ElasticNet-Lasso的API" class="headerlink" title="Ridge, ElasticNet, Lasso的API"></a><strong>Ridge</strong>, <strong>ElasticNet</strong>, <strong>Lasso的API</strong></h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from sklearn.linear_model import <strong>Ridge</strong>, <strong>ElasticNet</strong>, <strong>Lasso</strong></td><td><strong>正则化线性模型</strong>：复杂度或者异常数据点太多减少这个特征的影响</td></tr><tr><td>Ridge</td><td><strong>岭回归模型</strong>：                                                                                               达到了在拟合数据的同时，使模型权重尽可能小的目的</td></tr><tr><td>ElasticNet</td><td><strong>弹性网络</strong>：                                                                                                        进行了折中，通过 <strong>混合比(mix ratio) r</strong> 进行控制</td></tr><tr><td>Lasso</td><td><strong>Lasso 回归</strong>：                                                                                                                  能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="6-0：RidgeCV（）：可以传入多个惩罚系数，还经过交叉验证了"><a href="#6-0：RidgeCV（）：可以传入多个惩罚系数，还经过交叉验证了" class="headerlink" title="6.0：RidgeCV（）：可以传入多个惩罚系数，还经过交叉验证了"></a><strong>6.0：RidgeCV（）：可以传入多个惩罚系数，还经过交叉验证了</strong></h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>**RidgeCV（(alphas=(0.1, 1.0, 10.0：可以传入不同的惩罚力度去最好的)) **</td><td>传入多个惩罚系数                                                                                             具有l2正则化的线性回归，可以进行交叉验证 coef_:回归系数，   这个函数还经过了交叉验证，会让数据集小，数据集小，验证集也会小，会带来一种问题，不一定会比Ridge（）的误差更小</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="6-1正则化线性模型：里面都是防止过拟合的："><a href="#6-1正则化线性模型：里面都是防止过拟合的：" class="headerlink" title="6.1正则化线性模型：里面都是防止过拟合的："></a>6.1正则化线性模型：里面都是防止过拟合的：</h3><h3 id="α-：为正则化惩罚力度"><a href="#α-：为正则化惩罚力度" class="headerlink" title="α ：为正则化惩罚力度"></a>α ：为正则化惩罚力度</h3><h3 id="6-11-Ridge-Regression-岭回归，又名-Tikhonov-regularization-："><a href="#6-11-Ridge-Regression-岭回归，又名-Tikhonov-regularization-：" class="headerlink" title="6.11 Ridge Regression (岭回归，又名 Tikhonov regularization)："></a>6.11 Ridge Regression (岭回归，又名 Tikhonov regularization)：</h3><h4 id="性质：达到了在拟合数据的同时，使模型权重尽可能小的目的"><a href="#性质：达到了在拟合数据的同时，使模型权重尽可能小的目的" class="headerlink" title="性质：达到了在拟合数据的同时，使模型权重尽可能小的目的"></a>性质：达到了在拟合数据的同时，使模型权重尽可能小的目的</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>sklearn.linear_model.<strong>Ridge</strong>(alpha=1.0, fit_intercept=True,solver=”auto”, normalize=False)</td><td><strong>岭回归模型：</strong>具有l2正则化的线性回归                                                                   <strong>alpha:正则化力度，也叫 λ</strong>                                                                             ① λ取值：0<del>1 1</del>10    ，值越小惩罚越小，相反值越大惩罚越大                                                                        <strong>solver:会根据数据自动选择优化方法</strong>                                                    ①sag:如果数据集、特征都比较大，选择该随机梯度下降优化 <strong>normalize:数据是否进行标准化，如果标准化过了就不用标准化了</strong>                                                                                  ①normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</td></tr><tr><td>Ridge.coef_</td><td>回归权重</td></tr><tr><td>Ridge.intercept_</td><td>回归偏置</td></tr></tbody></table><ul><li><h5 id="邻回归：在原来的均方误差中添加了正则项"><a href="#邻回归：在原来的均方误差中添加了正则项" class="headerlink" title="邻回归：在原来的均方误差中添加了正则项"></a>邻回归：在原来的均方误差中添加了正则项</h5></li><li><p>α ：为正则化惩罚力度</p></li><li><p>α 大惩罚越大，小惩罚越小</p></li><li><p>正则化力度越大，权重系数值越小</p></li><li><p>正则化力度越小，权重系数值越大</p></li><li><p>合并后的公式当前面的值都是一定时，<strong>后面的α 系数做惩罚系数和权重是此消彼长的形式，当α 大的时候权重小，相反如此</strong></p></li><li><p>L1直接把高次项的系数直接变成了0</p><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/13.png' height=px ></li></ul></li><li><h5 id="合成函数为"><a href="#合成函数为" class="headerlink" title="合成函数为"></a>合成函数为</h5><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/14.png' height=px ></li></ul></li></ul><h3 id="6-2：Lasso-Regression-Lasso-回归-很重要的性质是：倾向于完全消除不重要的权重。"><a href="#6-2：Lasso-Regression-Lasso-回归-很重要的性质是：倾向于完全消除不重要的权重。" class="headerlink" title="6.2：Lasso Regression(Lasso 回归):很重要的性质是：倾向于完全消除不重要的权重。"></a>6.2：Lasso Regression(Lasso 回归):很重要的性质是：倾向于完全消除不重要的权重。</h3><h4 id="性质：当α-取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0，也就是说，Lasso-Regression-能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。"><a href="#性质：当α-取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0，也就是说，Lasso-Regression-能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。" class="headerlink" title="性质：当α 取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0，也就是说，Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。"></a>性质：当α 取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0，也就是说，Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。</h4><ul><li><h4 id="Lasso-Regression-的代价函数在-θi-0处是不可导的"><a href="#Lasso-Regression-的代价函数在-θi-0处是不可导的" class="headerlink" title="Lasso Regression 的代价函数在 θi=0处是不可导的."></a>Lasso Regression 的代价函数在 θi=0处是不可导的.</h4></li><li><p>当前面的值都是一定时，<strong>后面的α 系数做惩罚系数和权重是此消彼长的形式，当α 大的时候权重小，相反如此</strong></p></li><li><h4 id="解决方法：在θi-0处用一个次梯度向量-subgradient-vector-代替梯度，运用权重对0做等价值小于0是-1-。。"><a href="#解决方法：在θi-0处用一个次梯度向量-subgradient-vector-代替梯度，运用权重对0做等价值小于0是-1-。。" class="headerlink" title="解决方法：在θi=0处用一个次梯度向量(subgradient vector)代替梯度，运用权重对0做等价值小于0是-1.。。"></a>解决方法：在θi=0处用一个次梯度向量(subgradient vector)代替梯度，运用权重对0做等价值小于0是-1.。。</h4><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/15.png' height=px ></li></ul></li></ul><h3 id="6-3：Elastic-Net-弹性网络-L1和L2进行了合并"><a href="#6-3：Elastic-Net-弹性网络-L1和L2进行了合并" class="headerlink" title="6.3：Elastic Net (弹性网络)L1和L2进行了合并"></a>6.3：Elastic Net (弹性网络)L1和L2进行了合并</h3><h4 id="性质：进行了折中，通过-混合比-mix-ratio-r-进行控制"><a href="#性质：进行了折中，通过-混合比-mix-ratio-r-进行控制" class="headerlink" title="性质：进行了折中，通过 混合比(mix ratio) r 进行控制"></a>性质：进行了折中，通过 <strong>混合比(mix ratio) r</strong> 进行控制</h4><p>前面的权重变大时，把r提高，后面的权重变大，把r减少</p><ul><li>r=0：弹性网络变为岭回归，对应后项式</li><li>r=1：弹性网络便为Lasso回归对应前项式子</li><li>公式：<ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/16.png' height=px ></li></ul></li></ul><p>常用：岭回归</p><p>假设只有少部分特征是有用的：</p><ul><li>弹性网络</li><li>Lasso </li><li>一般来说，弹性网络的使用更为广泛。因为在特征维度高于训练样本数，或者特征是强相关的情况下，Lasso回归的表现不太稳定。</li></ul><h2 id="7：模型的保存和加载"><a href="#7：模型的保存和加载" class="headerlink" title="7：模型的保存和加载;"></a>7：模型的保存和加载;</h2><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from sklearn.externals import <strong>joblib</strong></td><td>模型的保存和加载</td></tr><tr><td>joblib.<strong>dump</strong>(estimator, ‘test.pkl’)</td><td>保存</td></tr><tr><td>变量 = joblib.<strong>load</strong>(‘test.pkl’)</td><td>加载</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1：线性回归：属于预测模型&quot;&gt;&lt;a href=&quot;#1：线性回归：属于预测模型&quot; class=&quot;headerlink&quot; title=&quot;1：线性回归：属于预测模型&quot;&gt;&lt;/a&gt;1：线性回归：属于预测模型&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;评估线性损失的时&lt;/strong&gt;：</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xiaoyvlongoing.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://xiaoyvlongoing.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>集成学习lightGBM</title>
    <link href="https://xiaoyvlongoing.github.io/2020/11/04/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0lightGBM/"/>
    <id>https://xiaoyvlongoing.github.io/2020/11/04/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0lightGBM/</id>
    <published>2020-11-04T08:49:42.000Z</published>
    <updated>2021-11-05T06:35:05.205Z</updated>
    
    <content type="html"><![CDATA[<h1 id="lightGBM"><a href="#lightGBM" class="headerlink" title="lightGBM"></a>lightGBM</h1><h3 id="1：基于Histogram（直方图）的决策树算法"><a href="#1：基于Histogram（直方图）的决策树算法" class="headerlink" title="1：基于Histogram（直方图）的决策树算法"></a>1：基于Histogram（直方图）的决策树算法</h3><p>优点：<strong>最明显就是内存消耗的降低</strong>，<strong>在计算上的代价也大幅降低，因为普通的每遍历一个特征值就需要计算一次分裂的增益，而lightGBM把连续的浮点特征值离散化成k个整数，只计算k次</strong></p><ul><li>先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。</li><li>在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</li></ul><h3 id="2-Lightgbm-的Histogram（直方图）做差加速"><a href="#2-Lightgbm-的Histogram（直方图）做差加速" class="headerlink" title="2 Lightgbm 的Histogram（直方图）做差加速"></a>2 Lightgbm 的Histogram（直方图）做差加速</h3><p>一个叶子的直方图：父亲节点的直方图与它兄弟的直方图做差得到。</p><h3 id="3-带深度限制的Leaf-wise的叶子生长策略"><a href="#3-带深度限制的Leaf-wise的叶子生长策略" class="headerlink" title="3 带深度限制的Leaf-wise的叶子生长策略"></a>3 带深度限制的Leaf-wise的叶子生长策略</h3><p><strong>Level-wise</strong>便利一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。</p><p><strong>Leaf-wise</strong>则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环，只单独可以一个分到底，Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p><h3 id="4：-直接支持类别特征"><a href="#4：-直接支持类别特征" class="headerlink" title="4： 直接支持类别特征"></a>4： 直接支持类别特征</h3><p>LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。</p><h3 id="5-直接支持高效并行"><a href="#5-直接支持高效并行" class="headerlink" title="5 直接支持高效并行"></a>5 直接支持高效并行</h3><p>LightGBM原生支持并行学习，目前支持特征并行和数据并行的两种。</p><ul><li>特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。</li><li>数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。</li><li>**基于投票的数据并行(Voting Parallelization)**则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。</li></ul><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import <strong>lightgbm</strong> as lgb</td><td><strong>lightGBM</strong>模型方法</td></tr><tr><td>lgb.LGBMRegressor（objective,）</td><td>生成的模型的方法</td></tr><tr><td>objective [缺省值=reg:linear]</td><td><strong>reg:linear</strong>” – 线性回归                                                                                             “reg:logistic<strong>” – 逻辑回归                                                                           “</strong>binary:logistic<strong>” – 二分类逻辑回归，输出为概率                                                                                        “</strong>multi:softmax<strong>” – 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)                                                                        “</strong>multi:softprob**” – 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</td></tr><tr><td>learning_rate</td><td>学习率</td></tr><tr><td><strong>gbm.fit</strong>(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric=’L1’, early_stopping_rounds=值)</td><td><strong>eval_set</strong>：评价的数据集                                                                             <strong>eval_metric</strong>：依据目标函数选择评估指标这个用上了L1正则化，L1正则化值越小越好：                                                                      <strong>early_stopping_rounds</strong>：激活早停止，当验证的L1的L2中的错误率设置的数值轮数未下降，则停止训练。</td></tr><tr><td>gbm.dest_params_</td><td>c选择出最优的学习率和跌代次数</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><table><thead><tr><th>lightGBM：AIP里面的参数</th><th>含义</th><th>用法</th></tr></thead><tbody><tr><td>max_depth</td><td>树的最大深度</td><td>当模型过拟合时,可以考虑首先降低 max_depth</td></tr><tr><td>min_data_in_leaf</td><td>叶子可能具有的最小记录数</td><td>默认20，过拟合时用</td></tr><tr><td>feature_fraction</td><td>例如 为0.8时，意味着在每次迭代中随机选择80％的参数来建树</td><td>boosting 为 random forest 时用</td></tr><tr><td>bagging_fraction</td><td>每次迭代时用的数据比例</td><td>用于加快训练速度和减小过拟合</td></tr><tr><td>early_stopping_round</td><td>如果一次验证数据的一个度量在最近的early_stopping_round 回合中没有提高，模型将停止训练</td><td>加速分析，减少过多迭代</td></tr><tr><td>lambda</td><td>指定正则化</td><td>0～1</td></tr><tr><td>min_gain_to_split</td><td>描述分裂的最小 gain</td><td>控制树的有用的分裂</td></tr><tr><td>max_cat_group</td><td>在 group 边界上找到分割点</td><td>当类别数量很多时，找分割点很容易过拟合时</td></tr><tr><td>n_estimators</td><td>最大迭代次数</td><td>最大迭代数不必设置过大，可以在进行一次迭代后，根据最佳迭代数设置</td></tr></tbody></table><h4 id="核心参数，选择的算法"><a href="#核心参数，选择的算法" class="headerlink" title="核心参数，选择的算法"></a>核心参数，选择的算法</h4><table><thead><tr><th>Core Parameters</th><th>含义</th><th>用法</th></tr></thead><tbody><tr><td>Task</td><td>数据的用途</td><td>选择 train（训练） 或者 predict（预测）</td></tr><tr><td>application</td><td>模型的用途</td><td>选择 regression: 回归时， binary: 二分类时， multiclass: 多分类时</td></tr><tr><td>boosting</td><td>要用的算法</td><td>gbdt，                                                                                              rf: random forest，                                                                                                        dart: Dropouts meet Multiple Additive Regression Trees，                                                                                           goss: Gradient-based One-Side Sampling</td></tr><tr><td>num_boost_round</td><td>迭代次数</td><td>通常 100+</td></tr><tr><td>learning_rate</td><td>学习率</td><td>常用 0.1, 0.001, 0.003…</td></tr><tr><td>num_leaves</td><td>叶子数量</td><td>默认 31</td></tr><tr><td>device</td><td></td><td>cpu 或者 gpu</td></tr><tr><td>metric</td><td></td><td>mae: mean absolute error ，  mse: mean squared error ，  binary_logloss: loss for binary classification ， multi_logloss: loss for multi classification</td></tr></tbody></table><h3 id="保存的方式"><a href="#保存的方式" class="headerlink" title="保存的方式"></a>保存的方式</h3><table><thead><tr><th>IO parameter</th><th>含义</th></tr></thead><tbody><tr><td>max_bin</td><td>表示 feature 将存入的 bin 的最大数量</td></tr><tr><td>categorical_feature</td><td>如果 categorical_features = 0,1,2， 则列 0，1，2是 categorical 变量</td></tr><tr><td>ignore_column</td><td>与 categorical_features 类似，只不过不是将特定的列视为categorical，而是完全忽略</td></tr><tr><td>save_binary</td><td>这个参数为 true 时，则数据集被保存为二进制文件，下次读数据时速度会变快</td></tr></tbody></table><h2 id="3-调参建议"><a href="#3-调参建议" class="headerlink" title="3 调参建议"></a>3 调参建议</h2><table><thead><tr><th>IO parameter</th><th align="left">含义</th></tr></thead><tbody><tr><td><code>num_leaves</code></td><td align="left">取值应 &lt;= 2(max_depth)2^{(max_depth)}2(max_depth)， 超过此值会导致过拟合</td></tr><tr><td><code>min_data_in_leaf</code></td><td align="left">将它设置为较大的值可以避免生长太深的树，但可能会导致 underfitting，在大型数据集时就设置为数百或数千</td></tr><tr><td><code>max_depth</code></td><td align="left">这个也是可以限制树的深度（集成学习的时候设置的小些）</td></tr></tbody></table><p>下表对应了 Faster Speed ，better accuracy ，over-fitting 三种目的时，可以调的参数</p><table><thead><tr><th>Faster Speed（设置的快一些）</th><th align="left">better accuracy（更好的准确度）</th><th align="left">over-fitting（过拟合）</th></tr></thead><tbody><tr><td>将 <code>max_bin</code> 设置小一些</td><td align="left">用较大的 <code>max_bin</code></td><td align="left"><code>max_bin</code> 小一些</td></tr><tr><td></td><td align="left"><code>num_leaves</code> 大一些</td><td align="left"><code>num_leaves</code> 小一些</td></tr><tr><td>用 <code>feature_fraction</code>来做 <code>sub-sampling</code></td><td align="left"></td><td align="left">用 <code>feature_fraction</code></td></tr><tr><td>用 <code>bagging_fraction 和 bagging_freq</code>【bagging的次数，0表示禁用，bagging，非零表示执行k次bagging】</td><td align="left"></td><td align="left">设定 <code>bagging_fraction 和 bagging_freq</code></td></tr><tr><td></td><td align="left">training data 多一些</td><td align="left">training data 多一些</td></tr><tr><td>用 <code>save_binary</code>来加速数据加载（用二进制能加快一些）</td><td align="left">直接用 categorical feature（0，1,2,3）</td><td align="left">用 <code>gmin_data_in_leaf （叶节点的数据量限制）和 min_sum_hessian_in_leaf</code>（最叶节点的权重值来控制整体内容）【和xgboost中min_child_weight类似】</td></tr><tr><td>用 parallel learning（并行）</td><td align="left">用 dart【DART利用了深度学习中的dropout中的技巧，随机丢弃生成的决策树，然后再从剩下的决策树集中迭代优化提升树】</td><td align="left">用 <code>lambda_l1, lambda_l2 ，min_gain_to_split</code>（信息增益值来做限制） 做正则化</td></tr><tr><td></td><td align="left"><code>num_iterations</code> （迭代次数的大小）大一些，<code>learning_rate</code>小一些</td><td align="left">用 <code>max_depth</code> 控制树的深度</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;lightGBM&quot;&gt;&lt;a href=&quot;#lightGBM&quot; class=&quot;headerlink&quot; title=&quot;lightGBM&quot;&gt;&lt;/a&gt;lightGBM&lt;/h1&gt;&lt;h3 id=&quot;1：基于Histogram（直方图）的决策树算法&quot;&gt;&lt;a href=&quot;#1：基于H</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xiaoyvlongoing.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://xiaoyvlongoing.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="https://xiaoyvlongoing.github.io/2020/11/04/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://xiaoyvlongoing.github.io/2020/11/04/%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2020-11-04T07:01:11.000Z</published>
    <updated>2021-11-05T06:35:21.780Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1：决策树："><a href="#1：决策树：" class="headerlink" title="1：决策树："></a>1：决策树：</h2><h4 id="计算的步骤：先算出总体的信息熵，再算出部分属性的信息熵，再进行计算信息的增益，再进属性信息度量，再进行信息增益率："><a href="#计算的步骤：先算出总体的信息熵，再算出部分属性的信息熵，再进行计算信息的增益，再进属性信息度量，再进行信息增益率：" class="headerlink" title="计算的步骤：先算出总体的信息熵，再算出部分属性的信息熵，再进行计算信息的增益，再进属性信息度量，再进行信息增益率："></a>计算的步骤：先算出总体的信息熵，再算出部分属性的信息熵，再进行计算信息的增益，再进属性信息度量，再进行信息增益率：</h4><h3 id="共有5个名词："><a href="#共有5个名词：" class="headerlink" title="共有5个名词："></a>共有5个名词：</h3><ol><li>总体的信息熵：总体的与是否形成概率比进行计算</li><li>属性信息熵：    总属性与单属性进行的计算</li><li>信息增益：        总体信息熵  —  属性信息熵，ID3是用信息增益实现的：只能对离散属性数据集构成决策树。</li><li>属性信息度量：总体数与总属性数进行计算</li><li>信息增益率：    信息增益 / 属性信息度量 ，    C4.5是用信息增益率实现的：优化了ID3分支过程中总喜欢偏向选择值较多的属性</li><li>基尼值：       是为了不纯来计算的</li><li>基尼指数（实现2分类的问题）：     CART决策树：可以经行行分类和回归，可以处理离散属性，可以处理连续属性    </li></ol><p>遇到数字型时：要去两个值的中值经行分类</p><h4 id="总体信息熵-—-属性信息熵-信息增益"><a href="#总体信息熵-—-属性信息熵-信息增益" class="headerlink" title="总体信息熵  —    属性信息熵=信息增益"></a>总体信息熵  —    属性信息熵=信息增益</h4><h4 id="信息增益-属性信息度量-信息增益率"><a href="#信息增益-属性信息度量-信息增益率" class="headerlink" title="信息增益 / 属性信息度量 =  信息增益率"></a><strong>信息增益 / 属性信息度量 =  信息增益率</strong></h4><h4 id="性质：分支结构就是if-else结构，利用这类结构分割数据的一种分类学习方法"><a href="#性质：分支结构就是if-else结构，利用这类结构分割数据的一种分类学习方法" class="headerlink" title="性质：分支结构就是if-else结构，利用这类结构分割数据的一种分类学习方法"></a>性质：分支结构就是if-else结构，利用这类结构分割数据的一种分类学习方法</h4><h4 id="后剪枝：后剪枝防止过拟合"><a href="#后剪枝：后剪枝防止过拟合" class="headerlink" title="后剪枝：后剪枝防止过拟合"></a>后剪枝：后剪枝防止过拟合</h4><h3 id="决策树API："><a href="#决策树API：" class="headerlink" title="决策树API："></a>决策树API：</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>class sklearn.tree.<strong>DecisionTreeClassifier</strong>(criterion=’gini’, max_depth=None,random_state=None)</td><td><strong>criterion</strong>：  特征选择标准 “gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。一默认”gini”，即CART算法。 min_samples_split  内部节点再划分所需最小样本数 这个值限制了子树继续划分的条件，如果某节点的样本数少于                               <strong>min_samples_split</strong>：则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。                                                             <strong>min_samples_leaf</strong>  叶子节点最少样本数 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。                                                                           <strong>max_depth</strong>  ：决策树最大深度 决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间                                                                               <strong>random_state：</strong>  随机数种子</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="1-1：熵：系统越有序，熵值越低；系统越混乱或者分散，熵值越高。"><a href="#1-1：熵：系统越有序，熵值越低；系统越混乱或者分散，熵值越高。" class="headerlink" title="1.1：熵：系统越有序，熵值越低；系统越混乱或者分散，熵值越高。"></a>1.1：熵：<strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。</h3><h4 id="用于处理离散型数据"><a href="#用于处理离散型数据" class="headerlink" title="用于处理离散型数据"></a><strong>用于处理离散型数据</strong></h4><p>熵的描述：是描述信息的混乱程度</p><ul><li><strong>从信息的完整性上进行的描述:</strong><ul><li>当<strong>系统的有序状态一致时</strong>，数据越集中的地方熵值越小，数据越分散的地方熵值越大。</li></ul></li><li><strong>从信息的有序性上进行的描述</strong>:<ul><li>当<strong>数据量一致时</strong>，<strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。</li></ul></li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/1.png' height=px ></li></ul><h3 id="1-2：决策树的划分依据一—-信息增益：最大值划分节点"><a href="#1-2：决策树的划分依据一—-信息增益：最大值划分节点" class="headerlink" title="1.2：决策树的划分依据一—-信息增益：最大值划分节点"></a>1.2：决策树的划分依据一—-信息增益：最大值划分节点</h3><ul><li><p><strong>信息增益：</strong>以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以<strong>使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏</strong>。</p></li><li><p><strong>信息增益 =  entroy(前) - entroy(后)</strong></p></li><li><p><strong>信息增益的公式</strong>：信息增益越大优先取最大的进行分支，优先排序：把最大的优先排到前面</p></li><li><p><strong>概率比值</strong>：通过是否进行操作，是否共涉及总数比值是的多少和否的多少，进行比值计算成信息熵</p></li><li><p><strong>计算的步骤</strong>：先算出总体的信息熵，再算出部分属性的信息熵，再进行计算信息的增益率</p><ul><li>分配的概率需要注意一下、</li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/2.png' height=px ></li><li>接触的例子：</li></ul><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/3.png' height=px ></li></ul></li></ul><p>上式中求出的信息增益越大影响越大，相反小影响越小。</p><h3 id="1-3：决策树的划分依据2–信息增益率C4-5：解决过拟合，缺失值"><a href="#1-3：决策树的划分依据2–信息增益率C4-5：解决过拟合，缺失值" class="headerlink" title="1.3：决策树的划分依据2–信息增益率C4.5：解决过拟合，缺失值"></a>1.3：决策树的划分依据2–信息增益率C4.5：解决过拟合，缺失值</h3><ul><li><p><strong>后剪枝：后剪枝防止过拟合</strong></p></li><li><p><strong>对于缺失值的处理</strong></p></li><li><p>公式</p><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/4.png' height=px ></li><li>固有值：IV：以某个类别进行计算</li></ul><h3 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h3><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/5.png' height=px ></li></ul></li></ul><h3 id="2：基尼值基尼指数-，CART算法：评判分类的更进一步简化，如算法程度"><a href="#2：基尼值基尼指数-，CART算法：评判分类的更进一步简化，如算法程度" class="headerlink" title="2：基尼值基尼指数 ，CART算法：评判分类的更进一步简化，如算法程度"></a>2：基尼值基尼指数 ，CART算法：评判分类的更进一步简化，如算法程度</h3><h3 id="只能进行2分类"><a href="#只能进行2分类" class="headerlink" title="只能进行2分类"></a>只能进行2分类</h3><h4 id="2-1：基尼值：从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。"><a href="#2-1：基尼值：从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。" class="headerlink" title="2.1：基尼值：从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。"></a>2.1：基尼值：从数据集D中随机抽取两个样本，其类别标记不一致的概率。<strong>故，Gini（D）值越小，数据集D的纯度越高。</strong></h4><ul><li><strong>基尼的运算</strong>：用是否的是的数和是否的否的数经行计算，如下案例：</li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/6.png' height=px ></li><li><strong>基尼值案例</strong></li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/7.png' height=px ></li><li></li><li><strong>基尼指数案例每一个属性就是一种基尼指数，基尼指数越小就用它划分结果</strong></li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/8.png' height=px ></li></ul><h3 id="3：预剪枝，后剪枝：剪掉是因为属性太多，形成噪声，错误的属性误导"><a href="#3：预剪枝，后剪枝：剪掉是因为属性太多，形成噪声，错误的属性误导" class="headerlink" title="3：预剪枝，后剪枝：剪掉是因为属性太多，形成噪声，错误的属性误导"></a>3：预剪枝，后剪枝：剪掉是因为属性太多，形成噪声，错误的属性误导</h3><ul><li>**预剪枝(局部最优解)**：是为了不让数据形成过拟合，内存占比较小，通过信息增益率找到影响数据样本的属性，把不能有泛化性的提升的其他的不必要的属性经行剪掉，没有得到提升就没有什么用</li><li><strong>后剪枝（）</strong>：先把整个数据的树形成，从最后的面叶节点上开始经行对比，找到影响较小的属性经行剪掉<ul><li>这个要占空间较大，形成的欠拟合更小一些，</li></ul></li></ul><h3 id="4：特征的提取，特征工程，特征处理："><a href="#4：特征的提取，特征工程，特征处理：" class="headerlink" title="4：特征的提取，特征工程，特征处理："></a>4：特征的提取，特征工程，特征处理：</h3><ul><li>将文本转换成数值</li><li>将类别转换成数值</li></ul><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>sklearn.<strong>feature_extraction</strong></td><td>特征提取等于小数时有百分之几做选择</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="4-1：字典提取：将字符串的类型转换成数值形式，对于特征当中存在类别信息做了one-hot编码处理"><a href="#4-1：字典提取：将字符串的类型转换成数值形式，对于特征当中存在类别信息做了one-hot编码处理" class="headerlink" title="4.1：字典提取：将字符串的类型转换成数值形式，对于特征当中存在类别信息做了one-hot编码处理"></a>4.1：字典提取：将字符串的类型转换成数值形式，<strong>对于特征当中存在类别信息</strong>做了<strong>one-hot编码处理</strong></h4><ul><li>节省空间，读取效率</li></ul><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>sklearn.feature_extraction.<strong>DictVectorizer</strong>(sparse=True,…)</td><td><strong>字典特征提取</strong>：                                                        sparse：直接对应的参数，数值</td></tr><tr><td>DictVectorizer.<strong>fit_transform</strong>(X)</td><td>先拟合数据，在进行标准化，对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等                                                                           X:字典或者包含字典的迭代器返回值 返回sparse矩阵</td></tr><tr><td>DictVectorizer.<strong>get_feature_names</strong>()</td><td>返回类别特征的名称</td></tr><tr><td>x_train和x_test.<strong>to_dict</strong>(orient=”records”)</td><td>数据进行字典类型的转换</td></tr></tbody></table><h4 id="4-2：文本的特征提取，经行英文词的统计的次数"><a href="#4-2：文本的特征提取，经行英文词的统计的次数" class="headerlink" title="4.2：文本的特征提取，经行英文词的统计的次数"></a>4.2：文本的特征提取，经行英文词的统计的次数</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>sklearn.feature_extraction.text.CountVectorizer(stop_words=[])</strong></td><td><strong>文本的特征提取</strong>                            stop_words=[]：没有用到词加入到里面就不进行提取</td></tr><tr><td>CountVectorizer.<strong>fit_transform</strong>(X)</td><td>X:文本或者包含文本字符串的可迭代对象  返回值:返回sparse矩阵</td></tr><tr><td>CountVectorizer.<strong>get_feature_names</strong>()</td><td>返回值:单词列表</td></tr><tr><td>已经转换成标准的变量.<strong>toarray</strong>()</td><td>形成可查看的数据形式</td></tr></tbody></table><p>【】里面的是：给出的数据是词频，对应着下边的词：出现一次为：1，出现多次是多个：</p><h4 id="4-3：中文的文本特征提取，统计次数，中文提取是因为经过了空格提取的"><a href="#4-3：中文的文本特征提取，统计次数，中文提取是因为经过了空格提取的" class="headerlink" title="4.3：中文的文本特征提取，统计次数，中文提取是因为经过了空格提取的"></a>4.3：中文的文本特征提取，统计次数，中文提取是因为经过了空格提取的</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import  jieba</td><td></td></tr><tr><td>jieba.<strong>cut</strong>(变量,cut_all=False)</td><td>中文文本的提取</td></tr><tr><td>text = “ “.join(list(jieba.cut(text)))</td><td>经行从新的进行拼接          cut_all=False：不会对所有的值进行切割</td></tr></tbody></table><h4 id="4-4：Tf-idf文本特征提取个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现"><a href="#4-4：Tf-idf文本特征提取个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现" class="headerlink" title="4.4：Tf-idf文本特征提取个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现"></a>4.4：Tf-idf文本特征提取<strong>个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现</strong></h4><p><strong>词频概率的公式</strong>：某一个给定的词语在该文件中出现的频率                  出现的次数/总词数</p><p><strong>逆向文档频率</strong>：一个词语普遍重要性的度量                                              lg（总词数/出现的次数）× 词频概率</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from sklearn.feature_extraction.text import <strong>TfidfVectorizer</strong></td><td><strong>Tf-idf文本特征提取</strong>个词或短语出现的概率</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="4-5：决策树的可视化："><a href="#4-5：决策树的可视化：" class="headerlink" title="4.5：决策树的可视化："></a>4.5：决策树的可视化：</h4><p><strong>可以生成树状图</strong>：形成DOT文件进行打开，复制里面内容，传入网址<a href="http://webgraphviz.com/">http://webgraphviz.com/</a>里面就能生成树</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>sklearn.tree.<strong>export_graphviz</strong>()</td><td>该函数能够导出DOT（树图）格式</td></tr><tr><td>tree.<strong>export_graphviz</strong>(estimator,out_file=’tree.dot’,feature_names=[‘’,’’])</td><td><strong>estimator</strong>：那个训练模型，那个估计器                                         <strong>out_file=<strong>“地址.dot”：导入的那个文件夹</strong>feature_names</strong>：选择的特征有哪些名字</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="5：回归决策树：主要用于处理连续型数据。如果是分段处理的时候是用回归决策树，如果是不分段的是用线性回归，通过标签值来使用这两个哪一个"><a href="#5：回归决策树：主要用于处理连续型数据。如果是分段处理的时候是用回归决策树，如果是不分段的是用线性回归，通过标签值来使用这两个哪一个" class="headerlink" title="5：回归决策树：主要用于处理连续型数据。如果是分段处理的时候是用回归决策树，如果是不分段的是用线性回归，通过标签值来使用这两个哪一个"></a>5：回归决策树：<strong>主要用于处理连续型数据。</strong>如果是分段处理的时候是用回归决策树，如果是不分段的是用线性回归，通过标签值来使用这两个哪一个</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from sklearn.tree import <strong>DecisionTreeRegressor</strong></td><td><strong>max_depth</strong>: 树最大深度,可选，缺省None，                                   <strong>min_samples_split</strong> : 分割内部节点所需最少样本数,可选,缺省2 <strong>min_samples_leaf</strong> : 成为叶子节点所需最少样本, 可选, 缺省1 <strong>max_features</strong> : 寻找最佳分割时考虑的特征数目, 可选, 缺省None: float/比例, ‘sqrt’/sqrt(n_features), ‘log2’/log2(n_features)                   <strong>min_impurity_decrease</strong> : 如果节点分割导致不纯度减少超过此值，将进行分割，可选, 缺省0.0                                                                                            <strong>presort</strong> : 预排序，加速寻找最佳分割，可选，缺省False, 大数据集降低训练过程, 小训练集或受限深度，可加快训练                                             random_state: 缺省None; 若int, 随机数产生器seed, 若RandomStates实例, 随机数产生器, 若None, np.random</td></tr><tr><td><strong>函数调用</strong></td><td></td></tr><tr><td><strong>.fit(X,y)</strong></td><td>调用库函数决策树算法: 分类器y是整数或string;回归器y是浮点数</td></tr><tr><td><strong>.predict(X)</strong></td><td>预测样本类别或回归值，返回shape(n_samples)或(n_samples,n_outputs)</td></tr><tr><td><strong>.decision_path(X)</strong></td><td>返回决策路径，返回shape = [n_samples, n_nodes]</td></tr><tr><td><strong>.score(X_test, y_test)</strong></td><td>返回预测结果的R^2(1-u/v). u=((y_true - y_pred) ** 2).sum()                               v=((y_true - y_true.mean()) ** 2).sum()</td></tr><tr><td><strong>.apply(X</strong>)</td><td>返回每个样本预测为叶子的索引</td></tr><tr><td><strong>.n_features_</strong></td><td>执行’fit’时的特征数</td></tr><tr><td><strong>.n_outputs_</strong></td><td>执行’fit’时的输出数</td></tr><tr><td><strong>.tree_</strong></td><td>树对象</td></tr></tbody></table><h4 id="案例："><a href="#案例：" class="headerlink" title="案例："></a>案例：</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/9.png' height=px ><h4 id="5-1：在源于的数据上进行数据的处理：分成份，取平均"><a href="#5-1：在源于的数据上进行数据的处理：分成份，取平均" class="headerlink" title="5.1：在源于的数据上进行数据的处理：分成份，取平均"></a>5.1：在源于的数据上进行数据的处理：分成份，取平均</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/10.png' height=px ><h4 id="5-2：取得计算取最小的值进行进行分组，下面6-5是最小的数，"><a href="#5-2：取得计算取最小的值进行进行分组，下面6-5是最小的数，" class="headerlink" title="5.2：取得计算取最小的值进行进行分组，下面6.5是最小的数，"></a>5.2：取得计算取最小的值进行进行分组，下面6.5是最小的数，</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/11.png' height=px ><h4 id="5-3：循环进行划分"><a href="#5-3：循环进行划分" class="headerlink" title="5.3：循环进行划分"></a>5.3：循环进行划分</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/12.png' height=px ><h4 id="5-4：形成回归树"><a href="#5-4：形成回归树" class="headerlink" title="5.4：形成回归树"></a>5.4：形成回归树</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E5%86%B3%E7%AD%96%E6%A0%91/13.png' height=px >]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1：决策树：&quot;&gt;&lt;a href=&quot;#1：决策树：&quot; class=&quot;headerlink&quot; title=&quot;1：决策树：&quot;&gt;&lt;/a&gt;1：决策树：&lt;/h2&gt;&lt;h4 id=&quot;计算的步骤：先算出总体的信息熵，再算出部分属性的信息熵，再进行计算信息的增益，再进属性信息度量，再</summary>
      
    
    
    
    <category term="算法，树" scheme="https://xiaoyvlongoing.github.io/categories/%E7%AE%97%E6%B3%95%EF%BC%8C%E6%A0%91/"/>
    
    
    <category term="算法，树" scheme="https://xiaoyvlongoing.github.io/tags/%E7%AE%97%E6%B3%95%EF%BC%8C%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>聚类算法,降维</title>
    <link href="https://xiaoyvlongoing.github.io/2020/10/04/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E9%99%8D%E7%BB%B4/"/>
    <id>https://xiaoyvlongoing.github.io/2020/10/04/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E9%99%8D%E7%BB%B4/</id>
    <published>2020-10-04T06:20:03.000Z</published>
    <updated>2021-11-04T07:34:06.019Z</updated>
    
    <content type="html"><![CDATA[<h2 id="聚类算法：使用不同的聚类准则，产生的聚类结果不同"><a href="#聚类算法：使用不同的聚类准则，产生的聚类结果不同" class="headerlink" title="聚类算法：使用不同的聚类准则，产生的聚类结果不同"></a>聚类算法：使用不同的聚类准则，产生的聚类结果不同</h2><h3 id="概念：一种典型的无监督算法，主要用于将相似的样本自动归到一个类别中。"><a href="#概念：一种典型的无监督算法，主要用于将相似的样本自动归到一个类别中。" class="headerlink" title="概念：一种典型的无监督算法，主要用于将相似的样本自动归到一个类别中。"></a>概念：一种典型的无监督算法，主要用于将相似的样本自动归到一个类别中。</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from sklearn.datasets.samples_generator import <strong>make_blobs</strong></td><td>**生成的聚类的数据集 **                                                                         n_samples:表示数据样本点个数,默认值100  n_features:表示数据的维度，默认值是2  centers:产生数据的中心点，默认值3  cluster_std：数据集的标准差，浮点数或者浮点数序列，默认值1.0  center_box：中心确定之后的数据边界，默认值(-10.0, 10.0)  shuffle ：洗乱，默认值是True  random_state:官网解释是随机生成器的种子</td></tr><tr><td>from sklearn.metrics import <strong>calinski_harabaz_score</strong></td><td><strong>评价聚类效果</strong></td></tr><tr><td>sklearn.cluster.KMeans(n_clusters=8)</td><td><strong>聚类算法模型</strong>：参数:                                                                                                             n_clusters:开始的聚类中心数量  整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。</td></tr><tr><td>estimator.fit_predict(x)</td><td>计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)</td></tr><tr><td>estimator.predict(x)</td><td>输出的预测值</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="1-2：k-means："><a href="#1-2：k-means：" class="headerlink" title="1.2：k-means："></a><strong>1.2：k-means：</strong></h3><ul><li><h4 id="K-初始中心点个数（计划聚类数）"><a href="#K-初始中心点个数（计划聚类数）" class="headerlink" title="K : 初始中心点个数（计划聚类数）"></a>K : 初始中心点个数（计划聚类数）</h4></li><li><h4 id="means：求中心点到其他数据点距离的平均值"><a href="#means：求中心点到其他数据点距离的平均值" class="headerlink" title="means：求中心点到其他数据点距离的平均值"></a>means：求中心点到其他数据点距离的平均值</h4></li></ul><h3 id="1-3：模型评估："><a href="#1-3：模型评估：" class="headerlink" title="1.3：模型评估："></a>1.3：模型评估：</h3><h4 id="误差平方和：真实值和预测值的差，再进行平方，一起相加"><a href="#误差平方和：真实值和预测值的差，再进行平方，一起相加" class="headerlink" title="误差平方和：真实值和预测值的差，再进行平方，一起相加"></a>误差平方和：真实值和预测值的差，再进行平方，一起相加</h4><ul><li><h4 id="SSE：误差平方和的值越小越好："><a href="#SSE：误差平方和的值越小越好：" class="headerlink" title="SSE：误差平方和的值越小越好："></a>SSE：误差平方和的值越小越好：</h4></li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E8%81%9A%E7%B1%BB/1.png' height=px ></li><li><p>ci：不同的族，p真实值：所有值的均值，</p></li><li><p>如果质心的初始值选择不好，sse只会达到局部最优解</p></li></ul><h3 id="2-1肘”方法-Elbow-method-—-K值确定：下降率突然变缓时即认为是最佳的k值。"><a href="#2-1肘”方法-Elbow-method-—-K值确定：下降率突然变缓时即认为是最佳的k值。" class="headerlink" title="2.1肘”方法 (Elbow method) — K值确定：下降率突然变缓时即认为是最佳的k值。"></a><strong>2.1肘”方法 (Elbow method)</strong> — K值确定：<strong>下降率突然变缓时即认为是最佳的k值</strong>。</h3><h4 id="在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在增加分类无法带来更多回报时，我们停止增加类别。"><a href="#在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在增加分类无法带来更多回报时，我们停止增加类别。" class="headerlink" title="在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在增加分类无法带来更多回报时，我们停止增加类别。"></a>在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在<strong>增加分类无法带来更多回报时，我们停止增加类别</strong>。</h4><h2 id="3-1：轮廓系数法：-作用：内部距离最小化，外部距离最大化"><a href="#3-1：轮廓系数法：-作用：内部距离最小化，外部距离最大化" class="headerlink" title="3.1：轮廓系数法：   作用：内部距离最小化，外部距离最大化"></a>3.1：轮廓系数法：   作用：内部距离最小化，外部距离最大化</h2><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E8%81%9A%E7%B1%BB/2.png' height=px ><p>平均轮廓系数的取值范围为[-1,1]，系数越大，聚类效果越好，但是合理的系数也好，</p><p>簇内样本的距离越近，簇间样本距离越远</p><h2 id="4-1：CH系数：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好"><a href="#4-1：CH系数：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好" class="headerlink" title="4.1：CH系数：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好"></a>4.1：CH系数：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好</h2><p><strong>CH：用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。</strong></p><hr><ul><li><h3 id="CH系数公式："><a href="#CH系数公式：" class="headerlink" title="CH系数公式："></a>CH系数公式：</h3><ul><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E8%81%9A%E7%B1%BB/3.png' height=px ></li></ul></li><li><p>矩阵的迹：矩阵的对角线可以表示一个物体的相似性，获取矩阵的迹，就可以表示这一块数据的最重要的特征了，这样就可以把很多无关紧要的数据删除掉，达到简化数据，提高处理速度。</p></li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E8%81%9A%E7%B1%BB/4.png' height=px ></li></ul><h2 id="5：聚类的算法的优化"><a href="#5：聚类的算法的优化" class="headerlink" title="5：聚类的算法的优化"></a>5：聚类的算法的优化</h2><h3 id="5-1：Canopy算法配合初始聚类：算法中-T1、T2的确定问题-，依旧可能落入局部最优解-，"><a href="#5-1：Canopy算法配合初始聚类：算法中-T1、T2的确定问题-，依旧可能落入局部最优解-，" class="headerlink" title="5.1：Canopy算法配合初始聚类：算法中 T1、T2的确定问题 ，依旧可能落入局部最优解 ，"></a>5.1：Canopy算法配合初始聚类：算法中 T1、T2的确定问题 ，依旧可能落入局部最优解 ，</h3><p>通过圆进行找到质心</p><ul><li><h3 id="Canopy粗聚类配合kmeans"><a href="#Canopy粗聚类配合kmeans" class="headerlink" title="Canopy粗聚类配合kmeans"></a>Canopy粗聚类配合kmeans</h3></li><li><p><strong>随机选取1个质心</strong>，以这个为质心进行画圆T1圆，T2圆，分别标注里面圆的颜色。</p></li><li><p>再选择质心点是<strong>尽可能的选择远一点的，在T1,T2圆外</strong>，<strong>半径</strong>是随机指定的，在这些之外再进行画标注颜色，重复进行，所有点包含进去才进行终止</p></li></ul><h3 id="5-2：K-means-：让选择的质心尽可能的分散"><a href="#5-2：K-means-：让选择的质心尽可能的分散" class="headerlink" title="5.2：K-means++：让选择的质心尽可能的分散"></a>5.2：K-means++：让选择的质心尽可能的分散</h3><p>通过距离找到质心</p><h3 id="5-3：二分k-means，所有点作为一个簇，选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。"><a href="#5-3：二分k-means，所有点作为一个簇，选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。" class="headerlink" title="5.3：二分k-means，所有点作为一个簇，选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。"></a>5.3：二分k-means，所有点作为一个簇，选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。</h3><p>想树的过程</p><ul><li>拆除SSE最大的簇</li></ul><h3 id="5-4：k-medoids：对异常值的处理很好，小样本起作用"><a href="#5-4：k-medoids：对异常值的处理很好，小样本起作用" class="headerlink" title="5.4：k-medoids：对异常值的处理很好，小样本起作用"></a>5.4：k-medoids：对异常值的处理很好，小样本起作用</h3><ul><li><strong>K-medoids中</strong>，将从当前cluster 中选取到其他所有（当前cluster中的）点的距离之和最小的点作为中心点。</li><li><strong>k-medoids</strong>只能对小样本起作用，样本大，速度就太慢了，当样本多的时候，少数几个噪音对k-means的质心影响也没有想象中的那么重，所以k-means的应用明显比k-medoids多。</li></ul><h2 id="进行了解"><a href="#进行了解" class="headerlink" title="进行了解"></a>进行了解</h2><h3 id="5-5：Kernel-k-means：映射到高维空间"><a href="#5-5：Kernel-k-means：映射到高维空间" class="headerlink" title="5.5：Kernel k-means：映射到高维空间"></a>5.5：Kernel k-means：映射到高维空间</h3><h3 id="就是将每个样本进行一个投射到高维空间的处理，然后再将处理后的数据使用普通的k-means算法思想进行聚类。"><a href="#就是将每个样本进行一个投射到高维空间的处理，然后再将处理后的数据使用普通的k-means算法思想进行聚类。" class="headerlink" title="就是将每个样本进行一个投射到高维空间的处理，然后再将处理后的数据使用普通的k-means算法思想进行聚类。"></a>就是将每个样本进行一个投射到高维空间的处理，然后再将处理后的数据使用普通的k-means算法思想进行聚类。</h3><h3 id="5-6：ISODATA：动态聚类，可以更改K值大小"><a href="#5-6：ISODATA：动态聚类，可以更改K值大小" class="headerlink" title="5.6：ISODATA：动态聚类，可以更改K值大小"></a>5.6：ISODATA：动态聚类，可以更改K值大小</h3><ul><li><p>对类别数会进行合并，分裂，</p><p>“合并”：（当聚类结果某一类中样本数太少，或两个类间的距离太近时）</p><p>“分裂”：（当聚类结果中某一类的类内方差太大，将该类进行分裂）</p></li></ul><h3 id="5-7：Mini-Batch-K-Means：大数据集分批聚类"><a href="#5-7：Mini-Batch-K-Means：大数据集分批聚类" class="headerlink" title="5.7：Mini Batch K-Means：大数据集分批聚类"></a>5.7：Mini Batch K-Means：大数据集分批聚类</h3><p>也是可以落到局部最优解的，但是效率是最快的</p><ul><li><p>(1)从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心</p><p>(2)更新质心</p><p>​        与Kmeans相比，数据的更新在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。</p></li></ul><h2 id="6：特征降维，特征处理"><a href="#6：特征降维，特征处理" class="headerlink" title="6：特征降维，特征处理"></a>6：特征降维，特征处理</h2><h3 id="6-1：降维：是指在某些限定条件下，降低随机变量-特征-个数，得到一组“不相关”主变量的过程"><a href="#6-1：降维：是指在某些限定条件下，降低随机变量-特征-个数，得到一组“不相关”主变量的过程" class="headerlink" title="6.1：降维：是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程"></a>6.1：降维：是指在某些限定条件下，<strong>降低随机变量(特征)个数</strong>，得到<strong>一组“不相关”主变量</strong>的过程</h3><ul><li>降低随机变量个数    </li><li>相关特征(correlated feature)<ul><li>相对湿度与降雨量之间的相关</li></ul></li></ul><h3 id="6-2：降维的两种方式"><a href="#6-2：降维的两种方式" class="headerlink" title="6.2：降维的两种方式"></a>6.2：降维的两种方式</h3><ul><li><h4 id="特征选择：数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从"><a href="#特征选择：数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从" class="headerlink" title="特征选择：数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从"></a>特征选择：数据中包含<strong>冗余或无关变量（或称特征、属性、指标等）</strong>，旨在从</h4><p><strong>原有特征中找出主要特征</strong>。</p><h3 id="低方差过滤的API："><a href="#低方差过滤的API：" class="headerlink" title="低方差过滤的API："></a>低方差过滤的API：</h3></li><li><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>sklearn.feature_selection.<strong>VarianceThreshold</strong>(threshold = 0.0)</td><td>**删除所有低方差特征，列的里面的数据删除掉了 **                                                                              <strong>threshold</strong> = 0.0：方差设置的大小</td></tr><tr><td><strong>Variance.fit_transform(X)</strong></td><td><strong>X</strong>:numpy array格式的数据[n_samples,n_features] 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。</td></tr><tr><td></td><td></td></tr></tbody></table><ul><li><h3 id="ilter-过滤式-主要探究特征本身特点-特征与特征和目标值之间关联"><a href="#ilter-过滤式-主要探究特征本身特点-特征与特征和目标值之间关联" class="headerlink" title="ilter(过滤式):主要探究特征本身特点,特征与特征和目标值之间关联"></a>ilter(过滤式):主要探究特征本身特点,特征与特征和目标值之间关联</h3><ul><li><h4 id="方差选择法：低方差特征过滤"><a href="#方差选择法：低方差特征过滤" class="headerlink" title="方差选择法：低方差特征过滤"></a><strong>方差选择法：低方差特征过滤</strong></h4></li><li><p><strong>低方差过滤</strong>：删除低方差的一些特征</p><ul><li><strong>删除特征方差小</strong>：某个特征大多样本的值比较相近</li><li><strong>保留特征方差大</strong>：某个特征很多样本的值都有差别</li></ul></li><li><h3 id="相关系数：让低方差可控制"><a href="#相关系数：让低方差可控制" class="headerlink" title="相关系数：让低方差可控制"></a><strong>相关系数</strong>：让低方差可控制</h3><ul><li><h4 id="皮尔逊相关系数：反映变量之间相关关系密切程度的统计指标"><a href="#皮尔逊相关系数：反映变量之间相关关系密切程度的统计指标" class="headerlink" title="皮尔逊相关系数：反映变量之间相关关系密切程度的统计指标,"></a><strong>皮尔逊相关系数：</strong>反映变量之间相关关系密切程度的统计指标,</h4></li><li><h4 id="通过具体值的大小进行计算"><a href="#通过具体值的大小进行计算" class="headerlink" title="通过具体值的大小进行计算"></a>通过具体值的大小进行计算</h4></li><li><h5 id="特点："><a href="#特点：" class="headerlink" title="特点："></a><strong>特点：</strong></h5><p><strong>相关系数的值介于–1与+1之间，即–1≤ r ≤+1</strong>。其性质如下：</p><ul><li><p><strong>当r&gt;0时，表示两变量正相关，r&lt;0时，两变量为负相关</strong></p></li><li><p>当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系</p></li><li><p><strong>当0&lt;|r|&lt;1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近于0，表示两变量的线性相关越弱</strong></p></li><li><p><strong>一般可按三级划分：|r|&lt;0.4为低度相关；0.4≤|r|&lt;0.7为显著性相关；0.7≤|r|&lt;1为高度线性相关</strong></p></li><li><p><strong>API</strong>：</p><ul><li><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from scipy.stats import **pearsonr **</td><td><strong>皮尔逊相关系数</strong>：变量之间相关关系密切程度的统计指标                                                                                            x : (N,) array_like                                                                                         y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value)</td></tr><tr><td></td><td></td></tr></tbody></table></li></ul></li><li><p>列如：</p></li><li><img src='https://boes.oss-cn-beijing.aliyuncs.com/%E8%81%9A%E7%B1%BB/5.png' height=px ></li></ul></li><li><h4 id="斯皮尔曼相关系数：反映变量之间相关关系密切程度的统计指标"><a href="#斯皮尔曼相关系数：反映变量之间相关关系密切程度的统计指标" class="headerlink" title="斯皮尔曼相关系数：反映变量之间相关关系密切程度的统计指标"></a><strong>斯皮尔曼相关系数：</strong>反映变量之间相关关系密切程度的统计指标</h4></li><li><p>通过等级差进行计算</p><ul><li><p>特点：相关系数的值介于–1与+1之间，即–1≤ r ≤+1，和上面的一样</p></li><li><p><strong>API</strong></p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from scipy.stats import <strong>spearmanr</strong></td><td><strong>斯皮尔曼相关系数</strong>：反映变量之间相关关系密切程度的统计指标</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></li></ul></li></ul></li></ul></li><li><h3 id="Embedded-嵌入式-：算法自动选择特征（特征与目标值之间的关联）"><a href="#Embedded-嵌入式-：算法自动选择特征（特征与目标值之间的关联）" class="headerlink" title="Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）"></a>Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）</h3></li><li><p>以下这些都可以进行降维</p><ul><li><strong>决策树:信息熵、信息增益</strong></li><li><strong>正则化：L1、L2</strong></li><li><strong>深度学习：卷积等</strong></li></ul></li></ul></li><li><h4 id="主成分分析（可以理解一种特征提取的方式）"><a href="#主成分分析（可以理解一种特征提取的方式）" class="headerlink" title="主成分分析（可以理解一种特征提取的方式）"></a><strong>主成分分析（可以理解一种特征提取的方式）</strong></h4></li></ul><h2 id="5：主成分分析：高维数据转换为低维数据，然后产生了新的变量"><a href="#5：主成分分析：高维数据转换为低维数据，然后产生了新的变量" class="headerlink" title="5：主成分分析：高维数据转换为低维数据，然后产生了新的变量"></a>5：主成分分析：高维数据转换为低维数据，然后产生了新的变量</h2><ul><li>定义：<strong>高维数据转化为低维数据的过程</strong>，在此过程中<strong>可能会舍弃原有数据、创造新的变量</strong></li><li>作用：<strong>是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。</strong></li><li>应用：回归分析或者聚类分析当中</li></ul><table><thead><tr><th><strong>函数</strong></th><th>函数性质</th></tr></thead><tbody><tr><td>sklearn.decomposition.<strong>PCA</strong>(n_components=None)</td><td><strong>将数据分解为较低维数空间</strong>，                                <strong>n_components:</strong>                                                         <strong>小数：表示保留百分之多少的信息</strong>                                        <strong>整数：降低到几维</strong></td></tr><tr><td>PCA.fit_transform(X) X:numpy array格式的数据[n_samples,n_features]</td><td>返回值：转换后指定维度的array</td></tr><tr><td>pca.explained_variance_ratio_</td><td>每一维元素的表示信息，数据占比的可解释情况</td></tr></tbody></table><h2 id="维度爆炸：当维度太多比特征数还要多的时候就会出现维度爆炸"><a href="#维度爆炸：当维度太多比特征数还要多的时候就会出现维度爆炸" class="headerlink" title="维度爆炸：当维度太多比特征数还要多的时候就会出现维度爆炸"></a>维度爆炸：当维度太多比特征数还要多的时候就会出现维度爆炸</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;聚类算法：使用不同的聚类准则，产生的聚类结果不同&quot;&gt;&lt;a href=&quot;#聚类算法：使用不同的聚类准则，产生的聚类结果不同&quot; class=&quot;headerlink&quot; title=&quot;聚类算法：使用不同的聚类准则，产生的聚类结果不同&quot;&gt;&lt;/a&gt;聚类算法：使用不同的聚类准则</summary>
      
    
    
    
    <category term="机器学习" scheme="https://xiaoyvlongoing.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://xiaoyvlongoing.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>NLP文本处理</title>
    <link href="https://xiaoyvlongoing.github.io/2020/06/06/NLP%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"/>
    <id>https://xiaoyvlongoing.github.io/2020/06/06/NLP%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/</id>
    <published>2020-06-06T07:22:53.000Z</published>
    <updated>2021-11-05T07:27:15.647Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NLP文本的预处理"><a href="#NLP文本的预处理" class="headerlink" title="NLP文本的预处理"></a>NLP文本的预处理</h2><h3 id="字符数据库"><a href="#字符数据库" class="headerlink" title="字符数据库"></a>字符数据库</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import   unicodedata</td><td>字符数据库</td></tr><tr><td>unicodedata.normalize()</td><td>第一个参数指定字符串标准化的方式。 NFC表示字符应该是整体组成(比如可能的话就使用单一编码)，而NFD表示字符应该分解为多个组合字符表示。</td></tr><tr><td></td><td></td></tr><tr><td>import string</td><td>定义了一些常用的属性（包含所有数字，字母，可打印的所有ascii码等）</td></tr><tr><td>string.ascii_letters</td><td>是生成所有字母，从a-z和A-Z,</td></tr><tr><td>string.digits</td><td>是生成所有数字0-9.</td></tr><tr><td></td><td></td></tr><tr><td>glob.glob()</td><td>返回所有匹配的文件路径列表。它只有一个参数pathname，定义了文件路径匹配规则，这里可以是绝对路径，也可以是相对路径</td></tr><tr><td>glob.glob(./路径*.txt还可以是：./路径 *.png)</td><td>也可以是文件，和照片，可以把所有的文件进行提取，配合for循环可以把文件名打印出来</td></tr></tbody></table><h3 id="文件分离后缀名"><a href="#文件分离后缀名" class="headerlink" title="文件分离后缀名"></a>文件分离后缀名</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>os.path.splitext(“文件路径”)</td><td>分离文件名与扩展名（后缀名）；默认返回(fname,fextension)元组，可做分片操作</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="对文本张量最大的进行提取及下标"><a href="#对文本张量最大的进行提取及下标" class="headerlink" title="对文本张量最大的进行提取及下标"></a>对文本张量最大的进行提取及下标</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>变量2，变量3=变量1张量.topk（input（可以输入指定的几行几列）, dim=None, descending=False, out=None）</td><td>对输入张量<code>input</code>沿着指定维按升序排序。如果不给定<code>dim</code>，则默认为输入的最后一维。如果指定参数<code>descending</code>为<code>True</code>，则按降序排序，                                           返回：函数1返回的是升序还是降序的张量 ，                                                       函数2：返回的是对用排序好的下标对应数</td></tr><tr><td></td><td>参数:                                                                                                                                              <strong>input</strong> (Tensor) – 要对比的张量                                                                                                                        <strong>dim</strong> (int, optional) – 沿着此维排序                              <strong>descending</strong> (bool, optional) – 布尔值，控制升降排序                                                                                                                                              <strong>out</strong> (tuple, optional) – 输出张量。必须为<code>ByteTensor</code>或者与第一个参数<code>tensor</code>相同类型</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="jieba分词"><a href="#jieba分词" class="headerlink" title="jieba分词"></a>jieba分词</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import  jieba</td><td>结巴分词</td></tr><tr><td>jieba.cut(变量, cut_all=False)</td><td>返回一个生成器的对象</td></tr><tr><td>jieba.lcut(content, cut_all=False)</td><td>精确模式</td></tr><tr><td>jieba.lcut(content, cut_all=True)</td><td>全模式</td></tr><tr><td>jieba.load_lcut(“.自定义文本的地址.txt”)</td><td>可以设置自定义的词典</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="hanlp（不怎么用搞坏环境）"><a href="#hanlp（不怎么用搞坏环境）" class="headerlink" title="hanlp（不怎么用搞坏环境）"></a>hanlp（不怎么用搞坏环境）</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>tokenizer = hanlp.load(‘CTB6_CONVSEG’)</td><td>中文分词</td></tr><tr><td>tokenizer = hanlp.utils.rules.tokenize_english</td><td>英文分词</td></tr><tr><td>recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)</td><td>加载中文命名实体识别的预训练模型MSRA_NER_BERT_BASE_ZH</td></tr><tr><td>recognizer = hanlp.load(hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN))</td><td>加载英文命名实体识别的预训练模型CONLL03_NER_BERT_BASE_UNCASED_EN</td></tr><tr><td>tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)</td><td>加载中文命名实体识别的预训练模型CTB5_POS_RNN_FASTTEXT_ZH</td></tr><tr><td>tagger = hanlp.load(hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN)</td><td>加载英文命名实体识别的预训练模型PTB_POS_RNN_FASTTEXT_EN</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="词性的标注"><a href="#词性的标注" class="headerlink" title="词性的标注"></a>词性的标注</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import   jieba.posseg  as pseg</td><td>结巴进行词性标注</td></tr><tr><td>pseg.lcut()</td><td>词性的标注</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="文本张量表示方法"><a href="#文本张量表示方法" class="headerlink" title="文本张量表示方法"></a>文本张量表示方法</h4><p>one-hot编码：又称独热编码，将每个词表示成具有n个元素的向量，这个词向量中只有一个元素是1，其他元素都是0，不同词汇元素为0的位置不同，其中n的大小是整个语料中不同词汇的总数</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import joblib</td><td>导入用于对象保存与加载的joblib</td></tr><tr><td>from keras.preprocessing.text import Tokenizer</td><td>导入keras中的词汇映射器Tokenizer</td></tr><tr><td>t = Tokenizer(num_words=None, char_level=False)</td><td>实例化一个词汇映射器对象</td></tr><tr><td>t.fit_on_texts(vocab)</td><td># 使用映射器拟合现有文本数据</td></tr><tr><td>joblib.dump（t，地址）</td><td>保存映射器</td></tr><tr><td></td><td></td></tr><tr><td>text</td><td>文本数值映射后的结果</td></tr><tr><td>该类允许使用两种方法向量化一个文本语料库：<br/>将每个文本转化为一个整数序列（每个整数都是词典中标记的索引）；<br/>或者将其转化为一个向量，其中每个标记的系数可以是二进制值、词频、TF-IDF权重等。</td><td><strong>Tokenizer</strong></td></tr><tr><td>keras.preprocessing.text.<strong>Tokenizer</strong>(num_words=None,                                                                                                                      filters=’!”#$%&amp;()*+,-./:;&lt;=&gt;?@[]^_`{|}~ ‘,                                     lower=True,                                                                                             split=’ ‘,                                                                                                char_level=False,                                     oov_token=None,                                     document_count=0)</td><td><strong>参数</strong>：<strong>num_words</strong>: 需要保留的最大词数，基于词频。只有最常出现的 <code>num_words</code> 词会被保留。 <strong>filters</strong>: 一个字符串，其中每个元素是一个将从文本中过滤掉的字符。默认值是所有标点符号，加上制表符和换行符，减去 <code>&#39;</code> 字符。 <strong>lower</strong>: 布尔值。是否将文本转换为小写。 <strong>split</strong>: 字符串。按该字符串切割文本。 <strong>char_level</strong>: 如果为 True，则每个字符都将被视为标记。 <strong>oov_token</strong>: 如果给出，它将被添加到 word_index 中，并用于在 <code>text_to_sequence</code> 调用期间替换词汇表外的单词。</td></tr></tbody></table><h3 id="word2id-就是把人类能看懂的字变成数字化"><a href="#word2id-就是把人类能看懂的字变成数字化" class="headerlink" title="word2id:就是把人类能看懂的字变成数字化"></a>word2id:就是把人类能看懂的字变成数字化</h3><p>字–》数字——》词向量</p><h3 id="word2vec文本表示张量的"><a href="#word2vec文本表示张量的" class="headerlink" title="word2vec文本表示张量的"></a>word2vec文本表示张量的</h3><p>一种流行的将词汇表示成向量的无监督训练方法, 该过程将构建神经网络模型, 将网络参数作为词汇的向量表示, 它包含CBOW和skipgram两种训练模式.</p><h5 id="CBOW-Continuous-bag-of-words-模式-（周围的词预测中间的词）"><a href="#CBOW-Continuous-bag-of-words-模式-（周围的词预测中间的词）" class="headerlink" title="CBOW(Continuous bag of words)模式:（周围的词预测中间的词）"></a>CBOW(Continuous bag of words)模式:（周围的词预测中间的词）</h5><h5 id="窗口一般为为奇数：3，5或者7"><a href="#窗口一般为为奇数：3，5或者7" class="headerlink" title="窗口一般为为奇数：3，5或者7"></a>窗口一般为为奇数：3，5或者7</h5><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/7.png' height=px ><h4 id="skipgram模式-中间的词预测周边的词"><a href="#skipgram模式-中间的词预测周边的词" class="headerlink" title="skipgram模式:中间的词预测周边的词"></a>skipgram模式:中间的词预测周边的词</h4><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/8.png' height=px ><h3 id="使用fasttext工具实现word2vec的训练和使用"><a href="#使用fasttext工具实现word2vec的训练和使用" class="headerlink" title="使用fasttext工具实现word2vec的训练和使用"></a>使用fasttext工具实现word2vec的训练和使用</h3><ul><li><strong>文本分类</strong>： 有监督学习</li><li><strong>词向量表征</strong>：无监督学习</li><li>第一步: 获取训练数据</li><li>fasttext.<strong>train_unsupervised</strong>：无监督学习</li><li>第二步: 训练词向量</li><li>第三步: 模型超参数设定</li><li>第四步: 模型效果检验</li><li>第五步: 模型的保存与重加载</li></ul><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import fasttext</td><td>训练词向量</td></tr><tr><td>.<strong>get_word_vector</strong>（“对应的词汇”）</td><td>通过get_word_vector方法来获得指定词汇的词向量</td></tr><tr><td>fasttext.<strong>train_unsupervised</strong>（“文件”，无监督训练模式（’skipgram’ 或者 ‘cbow’），词嵌入维度dim，数据循环次数epoch，学习率lr，使用的线程数thread），                   <strong>无监督学习</strong></td><td><strong>文件名</strong>：对应的导入文件                                                                                                                   <strong>无监督训练模式</strong>: ‘skipgram’ 或者 ‘cbow’, 默认为’skipgram’, 在实践中，skipgram模式在利用子词方面比cbow更好.                                                               <strong>词嵌入维度dim</strong>: 默认为100, 但随着语料库的增大, 词嵌入的维度往往也要更大.                                                                                                                       <strong>数据循环次数epoch</strong>: 默认为5, 但当你的数据集足够大, 可能不需要那么多次. # 学习率lr: 默认为0.05, 根据经验, 建议选择[0.01，1]范围内.                                  <strong>使用的线程数thread</strong>: 默认为12个线程, 一般建议和你的cpu核数相同.</td></tr><tr><td>model.<strong>get_nearest_neighbors(“词”)</strong></td><td>查找的邻近单词</td></tr><tr><td></td><td></td></tr><tr><td>model.<strong>save_model</strong>(“fil9.bin”)</td><td>保存模型</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="word-embedding-词嵌入-：就是把将词汇映射到指定维度-一般是更高维度-的空间"><a href="#word-embedding-词嵌入-：就是把将词汇映射到指定维度-一般是更高维度-的空间" class="headerlink" title="word embedding(词嵌入)：就是把将词汇映射到指定维度(一般是更高维度)的空间."></a>word embedding(词嵌入)：就是把将词汇映射到指定维度(一般是更高维度)的空间.</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from torch.utils.tensorboard import SummaryWriter</td><td></td></tr><tr><td>torch.randn(100, 50)</td><td>随机初始化一个100x50的矩阵, 认为它是我们已经得到的词嵌入矩阵 # 代表100个词汇, 每个词汇被表示成50维的向量</td></tr><tr><td></td><td></td></tr><tr><td><strong>词嵌入层</strong></td><td></td></tr><tr><td><code>torch.nn.</code><strong>Embedding</strong></td><td>一个简单的查找表，用于存储固定字典和大小的嵌入。该模块通常用于存储单词嵌入s并使用索引检索它们。模块的输入是索引列表，而输出是相应的词嵌入。</td></tr><tr><td>torch.nn.<strong>Embedding</strong>(<em>num_embeddings</em>, <em>embedding_dim</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em>, <em>_weight=None</em>)</td><td><strong>num_ embedding <strong>（<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>）–嵌入s的字典的大小        <strong>embedding _dim</strong>（<a href="https://docs.python.org/3/library/functions.html#int"> <em>int</em></a>）–每个嵌入向量的大小 <strong>padding_idx</strong>（<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>，**optional</em>）–如果指定，则at的条目<code>padding_idx</code>对梯度不起作用；因此，嵌入向量at<code>padding_idx</code>在训练期间不会更新，即，它仍然是固定的“填充”。对于新构建的Embedding，嵌入向量at<code>padding_idx</code>将默认为全零，但可以更新为另一个值以用作填充向量。 <strong>max_norm</strong>（<a href="https://docs.python.org/3/library/functions.html#float"><em>float</em></a>*，</strong>optional*）–如果给定，则将范数大于的每个嵌入向量<code>max_norm</code> 重新归一</strong>化为<em>*norm <code>max_norm</code>。 <strong>norm_type</strong>（<a href="https://docs.python.org/3/library/functions.html#float"><em>float</em></a><em>，**optional</em>）–为该<code>max_norm</code>选项计算的p范数的p 。默认值<code>2</code>。                               <strong>scale_grad_by_freq</strong>（*boolean</em> <em>，**可选</em>）–如果给定，将按小批量中单词频率的倒数来缩放梯度。默认值<code>False</code>。 <strong>sparse</strong>（<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>，**可选</em>）–如果为<code>True</code>，则梯度wrt<code>weight</code>矩阵将为稀疏张量。有关稀疏渐变的更多详细信息，请参见注释。</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="文本数据分析"><a href="#文本数据分析" class="headerlink" title="文本数据分析"></a>文本数据分析</h2><p>文本数据分析的作用:</p><ul><li>文本数据分析能够有效帮助我们理解数据语料, 快速检查出语料可能存在的问题, 并指导之后模型训练过程中一些超参数的选择.</li></ul><p>常用的几种文本数据分析方法: </p><ul><li>标签数量分布</li><li>句子长度分布</li><li>词频统计与关键词词云</li></ul><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import <strong>fileinput</strong></td><td>可以对一个或多个文件中的内容进行迭代、遍历等操作。</td></tr><tr><td>from wordcloud import <strong>WordCloud</strong></td><td><strong>词云展示</strong></td></tr><tr><td>import jieba.posseg as pseg</td><td>词性标注方法切分文本,获得具有词性属性flag和词汇属性word的对象,</td></tr><tr><td>from itertools import chain</td><td>导入chain方法用于扁平化列表</td></tr><tr><td></td><td></td></tr><tr><td>fileinput.FileInput(“文件地址及名字”)</td><td>导入数据文件</td></tr><tr><td>wordcloud.<strong>WordCloud</strong>(width，height，min_font_size，max_font_size，font_step，font_path，max_words，stop_words，mask，background_color)</td><td><strong>代表一个文本对应的词云</strong>：                                                                         <strong>width</strong>：指定词云对象生成图片的宽度，默认400像素，                               <strong>height：</strong>指定词云对象生成图片的高度，默认200像素，<strong>min_font_size</strong>：指定词云中字体的最小字号，默认4号，<strong>max_font_size</strong>：指定词云中字体的最大字号，                                                    <strong>font_step</strong>：指定词云中字体字号的步进间隔，默认为1，                                                           <strong>font_path：</strong>指定字体文件的路径，默认None，                                                                         <strong>max_words：</strong>指定词云显示的最大单词数量，默认200，                                                             <strong>stop_words</strong>：指定词云的排除词列表，即不显示的单词列表，                <strong>mask</strong>：指定词云形状，默认为长方形，需要引用imread()函数，<strong>background_color：</strong>指定词云图片的背景颜色，默认为黑色，”white”：白色</td></tr><tr><td>wordcloud（对应的上边的变量）.to_file(.图片的后缀名)</td><td>保存词云图片</td></tr><tr><td>str.join(sequence)：用于将序列中的元素以指定的字符连接生成一个新的字符串。</td><td>sequence – 要连接的元素序列。</td></tr><tr><td>.generate</td><td>生成词云</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="文本特征处理"><a href="#文本特征处理" class="headerlink" title="文本特征处理"></a>文本特征处理</h2><p>n-gram特征, 以及对加入特征之后的文本语料进行必要的处理, 如: 长度规范. 这些特征处理工作能够有效的将重要的文本特征加入模型训练中, 增强模型评估指标.</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>zip（）</td><td>两个列表中的进行合并成组</td></tr><tr><td>text</td><td>文本映射后的结果</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="文本长度规范及其作用：般模型的输入需要等尺寸大小的矩阵-对超长文本进行截断-对不足文本进行补齐"><a href="#文本长度规范及其作用：般模型的输入需要等尺寸大小的矩阵-对超长文本进行截断-对不足文本进行补齐" class="headerlink" title="文本长度规范及其作用：般模型的输入需要等尺寸大小的矩阵,对超长文本进行截断, 对不足文本进行补齐"></a>文本长度规范及其作用：般模型的输入需要等尺寸大小的矩阵,对超长文本进行截断, 对不足文本进行补齐</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from keras.preprocessing import sequence</td><td>样本不均衡</td></tr><tr><td>sequence.pad_sequences（输入的数据，长度）</td><td>样本不均衡进行0填充</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="文本数据增强：回译数据增强法"><a href="#文本数据增强：回译数据增强法" class="headerlink" title="文本数据增强：回译数据增强法"></a>文本数据增强：回译数据增强法</h3><p> 一般基于google翻译接口, 将文本数据翻译成另外一种语言(一般选择小语种),之后再翻译回原语言</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>from google_trans_new import google_translator</td><td>谷歌翻译接口</td></tr><tr><td>translator.translate(文本变量,’ko：韩语’，’zh-cn：国语’)</td><td>进行翻译：ko：韩语’，’zh-cn：国语’</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="数据的初始换-初始化时可以设置为负数，一般设置的数会小些，各层的权重参数都是初始化为均匀分布"><a href="#数据的初始换-初始化时可以设置为负数，一般设置的数会小些，各层的权重参数都是初始化为均匀分布" class="headerlink" title="数据的初始换:初始化时可以设置为负数，一般设置的数会小些，各层的权重参数都是初始化为均匀分布"></a>数据的初始换:初始化时可以设置为负数，一般设置的数会小些，各层的权重参数都是初始化为均匀分布</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>uniform_(x,y)</strong></td><td>方法将随机生成下一个实数，它在 [x, y] 范围内。</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="数据加载器"><a href="#数据加载器" class="headerlink" title="数据加载器"></a>数据加载器</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><code>torch.utils.data.``DataLoader</code>（）</td><td>数据加载器。组合数据集和采样器，并在给定的数据集上提供可迭代的。</td></tr><tr><td><code>torch.utils.data.``DataLoader</code>(<em>dataset</em>, <em>batch_size=1</em>, <em>shuffle=False</em>, <em>sampler=None</em>, <em>batch_sampler=None</em>, <em>num_workers=0</em>, <em>collate_fn=None</em>, <em>pin_memory=False</em>, <em>drop_last=False</em>, <em>timeout=0</em>, <em>worker_init_fn=None</em>, <em>multiprocessing_context=None</em>, <em>generator=None</em>, ***, <em>prefetch_factor=2</em>, <em>persistent_workers=False</em>)</td><td><strong>dataset</strong>                                                                                      （<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset"><em>Dataset</em></a>）–从中加载数据的数据集。                                                            <strong>batch_size</strong>（<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>，**optional</em>）–每个批次要加载多少个样本（默认值<strong>：）</strong><code>1</code>。                                                                                         <strong>shuffle</strong>（<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>，**可选</em>）–设置为<code>True</code>在每个时期重新<strong>随机播放</strong>数据（默认值：）<code>False</code>。                                                                            <strong>sampler</strong>（<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler"><em>Sampler</em></a><em>或**Iterable</em> <em>，**可选</em>）–定义从数据集中抽取样本的策略。可以是任何<code>Iterable</code>与<code>__len__</code> 实施。如果指定，则<code>shuffle</code>不得指定。 <strong>batch_sampler</strong>（<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler"><em>Sampler</em></a><em>或**Iterable</em> <em>，**可选</em>）–类似<code>sampler</code>，但是一次返回一批索引。互斥有 <code>batch_size</code>，<code>shuffle</code>，<code>sampler</code>，和<code>drop_last</code>。 <strong>num_workers</strong>（<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>，**optional</em>）–要用于数据加载的子<strong>进程数</strong>。<code>0</code>表示将在主进程中加载数据。（默认值：<code>0</code>） <strong>collate_fn</strong>（<em>callable</em> <em>，**optional</em>）–合并样本列表，以形成张量的小批量。在从地图样式数据集中使用批量加载时使用。                                                                                                               <strong>pin_memory</strong>（<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>，**可选</em>）–如果为<code>True</code>，则数据加载器在将张量返回之前将其复制到CUDA固定的内存中。如果您的数据元素是自定义类型，或者您<code>collate_fn</code>返回的是自定义类型的批次，请参见下面的示例。 <strong>drop_last</strong>（<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>，**可选</em>）–设置为<code>True</code>删除最后一个不完整的批次，如果该数据集大小不能被该批次大小整除。如果<code>False</code>并且数据集的大小不能被批次大小整除，那么最后一个批次将更小。（默认值：<code>False</code>） <strong>超时</strong>（<em>数字<strong>，</strong>可选</em>）–如果为正，则为从工作人员收集批次的超时值。应始终为非负数。（默认值：<code>0</code>）                                                                    <strong>worker_init_fn</strong>（<em>callable</em> <em>，**optional</em>）–如果不是<code>None</code>，则在种子工作之后和数据加载之前，将在每个工作程序子<strong>进程</strong>上以工作程序ID（int中的int ）作为输入来调用它。（默认值：）<code>[0, num_workers - 1]``None</code> <strong>prefetch_factor</strong>（<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>，<strong>可选</strong>，**仅关键字arg</em>）–每个工作人员预先加载的样本数。<code>2</code>意味着将在所有工作人员中预先提取2 * num_workers个样本。（默认值：<code>2</code>） <strong>sistence_workers</strong>（<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>，**可选</em>）–如果为<code>True</code>，则数据加载器使用完一次后，数据加载器将不会关闭工作进程。这样可以使Worker Dataset实例保持活动状态。（默认值：<code>False</code>）</td></tr><tr><td></td><td>表示从键到数据样本的映射的所有数据集都应该子类化<br/>它。  所有子类都应该覆盖 <code>__getitem__()</code>, 支持取一个<br/>给定键的数据样本。  子类也可以选择性地覆盖<br/><code>__len__()</code>, 预计将返回数据集的大小<br/><a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler"><code>Sampler</code></a>实现和默认选项<br/>的 <a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader"><code>DataLoader</code></a>.</td></tr></tbody></table><img src='https://boes.oss-cn-beijing.aliyuncs.com/Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20/9.png' height=px ><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><code>torch.optim.lr_scheduler.``StepLR</code>(<em>optimizer</em>, <em>step_size</em>, <em>gamma=0.1</em>, <em>last_epoch=-1</em>, <em>verbose=False</em>)</td><td><strong>optimizer</strong> ([<em>Optimizer*](<a href="https://pytorch.org/docs/stable/optim.html?highlight=torch">https://pytorch.org/docs/stable/optim.html?highlight=torch</a> optim lr_scheduler steplr#torch.optim.Optimizer))：-包裹的Optim IZER。，*<em>step_size</em></em> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)：学习率衰减的时间段。，            <strong>gamma</strong> (<a href="https://docs.python.org/3/library/functions.html#float"><em>float</em></a>)：学习率衰减的乘数。默认值：0.1，与学习率相乘，                                                                <strong>last_epoch</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)：最后一个纪元的索引。默认值：-1。，                                                                          <strong>verbose</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a>)：如果<code>True</code>为，则每次更新都会向stdout打印一条消息。默认值：<code>False</code>。</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;NLP文本的预处理&quot;&gt;&lt;a href=&quot;#NLP文本的预处理&quot; class=&quot;headerlink&quot; title=&quot;NLP文本的预处理&quot;&gt;&lt;/a&gt;NLP文本的预处理&lt;/h2&gt;&lt;h3 id=&quot;字符数据库&quot;&gt;&lt;a href=&quot;#字符数据库&quot; class=&quot;header</summary>
      
    
    
    
    <category term="NLP" scheme="https://xiaoyvlongoing.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://xiaoyvlongoing.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib基础</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/Matplotlib%E5%9F%BA%E7%A1%80/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/Matplotlib%E5%9F%BA%E7%A1%80/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-11-05T07:21:24.331Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Matplotlib基本操作："><a href="#Matplotlib基本操作：" class="headerlink" title="Matplotlib基本操作："></a>Matplotlib基本操作：</h2><table><thead><tr><th>函数</th><th>函数的用途</th></tr></thead><tbody><tr><td>import matplotlib.pyplot as plt</td><td>导入包</td></tr><tr><td>from pylab import mpl</td><td>导入中文字体包</td></tr><tr><td>plt.figure(figsize=(10,10),dpi=100)</td><td>创建画布，画布的范围和画布的大小</td></tr><tr><td>plt.plot()</td><td>添加图形的数据</td></tr><tr><td>plt.plot(x, y_beijing, color=’r’, linestyle=’–’, label=”北京”)</td><td>可以填加多个数据，“r”为红色，“–”虚线表示，右上角显示</td></tr><tr><td>plt.xticks(x[::5],x_tick[::5])</td><td>添加x轴的名字</td></tr><tr><td>plt.yticks(y_ticks[::5])</td><td>添加y轴的名字</td></tr><tr><td>plt.grid(True,linestyle=”–”,alpha=20)</td><td>添加网格线</td></tr><tr><td>plt.xlabel(“时间”)</td><td>添加描述信息</td></tr><tr><td>plt.ylabel(“温度”)</td><td>添加描述信息</td></tr><tr><td>plt.title(“”)</td><td>添加标题描述信息</td></tr><tr><td>plt.savefig(“./test.png”)</td><td>图像的保存</td></tr><tr><td>plt.legend(loc=0)</td><td>添加图例，右上角会显示：北京或者上海</td></tr><tr><td>plt.show()</td><td>图像的显示</td></tr><tr><td>mpl.rcParams[“font.sans-serif”] = [“SimHei”]</td><td>设置显示中文字体</td></tr><tr><td>mpl.rcParams[“axes.unicode_minus”] = False</td><td>设置正常显示符号</td></tr><tr><td></td><td></td></tr><tr><td>plt.imshow(x,<strong>cmap</strong>)</td><td>图片显示:                                                                                                  <strong>x</strong>：图片                                                                                               <strong>cmap</strong>：默认：“绿色”</td></tr><tr><td></td><td></td></tr><tr><td>fig,axes=plt.<strong>subplots</strong>(nrows=1,ncols=2,figsize=(10,8),dpi=100)</td><td>其中参数1和2分别代表子图的行数和列数,</td></tr><tr><td>.set_title</td><td>设置的图像的标题</td></tr><tr><td></td><td></td></tr><tr><td>plt.<strong>subplot</strong>（nrows, ncols, index, **kwargs）</td><td><strong>在循环中使用创建多少行多少列图</strong>                                                              nrows：生成多少行图，                                                          ncols：生成多少个列图，                                                         index：在第几个图生成对应的图</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><p>**plt.函数名()<strong>相当于面向过程的画图方法，</strong>axes.set_方法名()**相当于面向对象的画图方法。</p><h2 id="创建多个图像"><a href="#创建多个图像" class="headerlink" title="创建多个图像"></a>创建多个图像</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 0.准备数据</span>x <span class="token operator">=</span> range<span class="token punctuation">(</span><span class="token number">60</span><span class="token punctuation">)</span>y_shanghai <span class="token operator">=</span> <span class="token punctuation">[</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> x<span class="token punctuation">]</span>y_beijing <span class="token operator">=</span> <span class="token punctuation">[</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> x<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 1.创建画布</span><span class="token comment" spellcheck="true"># plt.figure(figsize=(20, 8), dpi=100)</span>fig<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>nrows<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ncols<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dpi<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#plt.subplots：拆分页面，多图展示</span><span class="token comment" spellcheck="true"># 2.绘制图像</span><span class="token comment" spellcheck="true"># plt.plot(x, y_shanghai, label="上海")</span><span class="token comment" spellcheck="true"># plt.plot(x, y_beijing, color="r", linestyle="--", label="北京")</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y_shanghai<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"上海"</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y_beijing<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"r"</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">"--"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"北京"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 2.1 添加x,y轴刻度</span><span class="token comment" spellcheck="true"># 构造x,y轴刻度标签</span>x_ticks_label <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"11点&amp;#123;&amp;#125;分"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> x<span class="token punctuation">]</span>y_ticks <span class="token operator">=</span> range<span class="token punctuation">(</span><span class="token number">40</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 刻度显示</span><span class="token comment" spellcheck="true"># plt.xticks(x[::5], x_ticks_label[::5])</span><span class="token comment" spellcheck="true"># plt.yticks(y_ticks[::5])</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xticks<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_yticks<span class="token punctuation">(</span>y_ticks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xticklabels<span class="token punctuation">(</span>x_ticks_label<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xticks<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_yticks<span class="token punctuation">(</span>y_ticks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xticklabels<span class="token punctuation">(</span>x_ticks_label<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 2.2 添加网格显示</span><span class="token comment" spellcheck="true"># plt.grid(True, linestyle="--", alpha=0.5)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">"--"</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">"--"</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 2.3 添加描述信息</span><span class="token comment" spellcheck="true"># plt.xlabel("时间")</span><span class="token comment" spellcheck="true"># plt.ylabel("温度")</span><span class="token comment" spellcheck="true"># plt.title("中午11点--12点某城市温度变化图", fontsize=20)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"时间"</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"温度"</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"中午11点--12点某城市温度变化图"</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"时间"</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"温度"</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"中午11点--12点某城市温度变化图"</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># # 2.4 图像保存</span>plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">"./test.png"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># # 2.5 添加图例</span><span class="token comment" spellcheck="true"># plt.legend(loc=0)</span>axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 3.图像显示</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="创建其他图像的api"><a href="#创建其他图像的api" class="headerlink" title="创建其他图像的api"></a>创建其他图像的api</h2><table><thead><tr><th>函数</th><th>函数的定义</th></tr></thead><tbody><tr><td>plt.plot(x, y)</td><td>创建折线图</td></tr><tr><td>plt.scatter(x, y)</td><td><strong>散点图</strong></td></tr><tr><td>plt.bar(x, width, align=’center’, **kwargs)</td><td><strong>柱状图</strong>：x : 需要传递的数据，width : 柱状图的宽度，align : 每个柱状图的位置对齐方式，**kwargs : color=:选择柱状图的颜色</td></tr><tr><td>matplotlib.pyplot.hist(x, bins=None)</td><td>直方图 ：    x : 需要传递的数据 ，bins : 组距</td></tr><tr><td>api：plt.pie(x, labels=,autopct=,colors)</td><td>饼图：  x:数量，自动算百分比， labels:每部分名称可以写成名字或者数据 的变量，autopct:占比显示指定%1.2f%% ，colors:每部分颜色</td></tr><tr><td></td><td></td></tr><tr><td><strong>形成柱状图</strong></td><td></td></tr><tr><td>plt.text(x, y, s, fontdict=None, withdash=False, **kwargs）</td><td><strong>x</strong>，<strong>y</strong>：显示内容的坐标位置                                                                                                              <strong>s</strong> ：显示内容                                                                                                                                            <strong>fontdict</strong>：一个定义s格式的dict                                                                                                                   <strong>fontsize</strong>：字体大小                                                                                                                                <strong>color</strong>：str or tuple, 设置字体颜色 ,单个字符候选项{‘b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’}，也可以’black’,’red’等，tuple时用[0,1]之间的浮点型数据，RGB或者RGBA, 如: (0.1, 0.2, 0.5)、(0.1, 0.2, 0.5, 0.3)等                                                    <strong>backgroundcolor</strong>：字体背景颜色                                                                         <strong>horizontalalignment</strong>(ha)：设置垂直对齐方式，可选参数：left,right,center                                                                                         <strong>verticalalignment</strong>(va)：设置水平对齐方式 ，可选参数 ： ‘center’ , ‘top’ , ‘bottom’ ,‘baseline’                                                                                                             <strong>rotation</strong>(旋转角度)：可选参数为:vertical,horizontal 也可以为数字                            <strong>alpha</strong>：透明度，参数值0至1之间</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Matplotlib基本操作：&quot;&gt;&lt;a href=&quot;#Matplotlib基本操作：&quot; class=&quot;headerlink&quot; title=&quot;Matplotlib基本操作：&quot;&gt;&lt;/a&gt;Matplotlib基本操作：&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
</summary>
      
    
    
    
    <category term="python高级" scheme="https://xiaoyvlongoing.github.io/categories/python%E9%AB%98%E7%BA%A7/"/>
    
    
    <category term="python高级" scheme="https://xiaoyvlongoing.github.io/tags/python%E9%AB%98%E7%BA%A7/"/>
    
  </entry>
  
  <entry>
    <title>Numpy基础</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/Numpy%E5%9F%BA%E7%A1%80/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/Numpy%E5%9F%BA%E7%A1%80/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-06-27T16:54:29.626Z</updated>
    
    <content type="html"><![CDATA[<h2 id="numpy数组"><a href="#numpy数组" class="headerlink" title="numpy数组"></a>numpy数组</h2><h4 id="numpy夹角运算"><a href="#numpy夹角运算" class="headerlink" title="numpy夹角运算"></a>numpy夹角运算</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>umpy.arctan2(<em>x1</em>, <em>x2</em>)</td><td>对应的可以算数夹角</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>numpy.clip( <em>a</em> , <em>a_min</em> , <em>a_max</em> , <em>out=None</em> , **<em>kwargs</em> )</td><td>给定一个区间，区间之外的值被 裁剪 为 <br/>区间边。 例如，如果间隔为 <code>[0, 1]</code>指定，小于 0 的值变为 0，而大于 0 的值<br/>比 1 变成 1。</td></tr><tr><td>例如</td><td>np.clip(a, 3, 6, out=a)                                                                                      array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])</td></tr><tr><td><a href="https://numpy.org/doc/stable/reference/generated/numpy.clip.html?highlight=clip#numpy.clip">https://numpy.org/doc/stable/reference/generated/numpy.clip.html?highlight=clip#numpy.clip</a></td><td></td></tr></tbody></table><h4 id="查看数组基本操作"><a href="#查看数组基本操作" class="headerlink" title="查看数组基本操作"></a>查看数组基本操作</h4><table><thead><tr><th>函数</th><th>函数定义</th></tr></thead><tbody><tr><td>np.array（【】，dtype=np.数组类型）</td><td>进行数据的存储的容器</td></tr><tr><td>存储数据容器的变量.shape</td><td>产看数组维度的元组，几行几列用（）写出</td></tr><tr><td>存储数据容器的变量.ndim</td><td>查看是几维的数组</td></tr><tr><td>存储数据容器的变量.size</td><td>查看数组里面有多少元素</td></tr><tr><td>存储数据容器的变量.itemsize</td><td>一个数组元素的长度（字节大小）</td></tr><tr><td>存储数据容器的变量.dtype</td><td>数组元素的类型</td></tr><tr><td>np.<strong>sum</strong>()</td><td>求和</td></tr><tr><td>np.<strong>polyfit</strong>（）</td><td>多项式拟合</td></tr></tbody></table><h4 id="生成创建数组的方法"><a href="#生成创建数组的方法" class="headerlink" title="生成创建数组的方法"></a>生成创建数组的方法</h4><table><thead><tr><th>函数</th><th>函数使用方法</th></tr></thead><tbody><tr><td>np.ones([])</td><td>创建里面都是1的二维数组，【】写上几行几列用逗号隔开</td></tr><tr><td>np.ones_like(存储数据容器的变量)</td><td>创建里面都是1的二维数组</td></tr><tr><td>np.zeros([])</td><td>创建里面都是0的二维数组，【】写上几行几列用逗号隔开</td></tr><tr><td>np.zeros_like(存储数据容器的变量)</td><td>创建里面都是0的二维数组</td></tr><tr><td></td><td></td></tr><tr><td>np.array(存储数据容器的变量)</td><td>对存储数据容器的变量的深拷贝</td></tr><tr><td>np.asarray(存储数据容器的变量)</td><td>对存储数据容器的变量的浅拷贝</td></tr><tr><td><strong>创建等比等差数列的数组</strong></td><td></td></tr><tr><td>np.linspace(start,stop,num,endpoint)</td><td>创建等差数组 — 指定数量 参数:                                                     start:序列的起始值                                                                                    stop:序列的终止值                                                                                               num:要生成的等间隔样例数量，默认为50                                             endpoint:序列中是否包含stop值，默认为ture</td></tr><tr><td>np.arange(start,stop,step,dtype)</td><td>创建等差数组 — 指定步长                                                                                   参数start:为开头，                                                                                                                   stop：为结尾                                                                                                       step:步长,默认值为1</td></tr><tr><td>np.logspace(start,stop,num)</td><td>创建等比数列 参数:                                                                                          start:表示用几位数开头，                                                                                stop：表示用几位数结束                                                                               num:要生成的等比数列数量的元素，默认为50</td></tr><tr><td><strong>正态分布</strong></td><td></td></tr><tr><td>np.random.normal（<strong>loc=0.0, scale=1.0, size=None</strong>）</td><td>生成正态分布的随机数，                                                                                loc：float 此概率分布的<strong>均值</strong>（对应着整个分布的中心centre）scale：float：此概率分布的<strong>标准差</strong>（对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高）                                                              size：可以创建一个维度的多少数，也几行几列二维的随机数</td></tr><tr><td><strong>np.random.uniform(low=0.0, high=1.0, size=None)</strong></td><td>low: 采样下界，float类型，默认值为0；                                                              high: 采样上界，float类型，默认值为1；                                                                  size: 输出样本数目，为int或元组(tuple)类型，例如，size=(m,n,k), 则输出m<em>n</em>k个样本，缺省时输出1个值。</td></tr></tbody></table><h4 id="数组的索引，切片，形状改变"><a href="#数组的索引，切片，形状改变" class="headerlink" title="数组的索引，切片，形状改变"></a>数组的索引，切片，形状改变</h4><table><thead><tr><th>函数</th><th>函数定义</th></tr></thead><tbody><tr><td>存储数据容器的变量【】</td><td>要分清维度，想要几维的用逗号隔开，在想要的维度再用《：》提取</td></tr><tr><td>存储数据容器的变量.reshape([])</td><td>改变想要的几行几列，但是要与原来的总元素量相匹配，是用成法得到总元素相同</td></tr><tr><td>存储数据容器的变量.reshape(-1,1)</td><td>把每一个元素都形成一个维</td></tr><tr><td>存储数据容器的变量.resize([])</td><td></td></tr><tr><td>存储数据容器的变量.T</td><td>进行行与列进行互换</td></tr><tr><td></td><td></td></tr><tr><td>折叠成一维的数组</td><td></td></tr><tr><td>变量.<strong>flatten</strong>()</td><td>返回一个折叠成一维的数组。但是该函数只能适用于numpy对象，即array或者mat，普通的list列表是不行的。</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="数组的修改"><a href="#数组的修改" class="headerlink" title="数组的修改"></a>数组的修改</h4><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>stock_change.astype(np.int32)</td><td>数组的修类型</td></tr><tr><td>ndarray.tostring([order])</td><td></td></tr><tr><td></td><td></td></tr><tr><td>np.unique(存储数据容器的变量)</td><td>数组的去重</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="ndarray运算（逻辑）"><a href="#ndarray运算（逻辑）" class="headerlink" title="ndarray运算（逻辑）"></a>ndarray运算（逻辑）</h3><table><thead><tr><th>函数</th><th>函数定义</th></tr></thead><tbody><tr><td>np.random.seed()</td><td>确定随机数生成种子，用它后每次生成的数都是一样的</td></tr><tr><td>np.random.randn(几行，几列)</td><td>创建0到1之间的随机数</td></tr><tr><td>np.random.randit(开头,结尾,(几行,几列))</td><td>随机创建数组</td></tr><tr><td>np.all(存储数据容器的变量[取值范围]&gt;条件判断)</td><td>符合条件为True，不符合为：False,判断给定轴向上的所有元素是否都为True</td></tr><tr><td>np.any()</td><td>判断给定轴向上是否有一个元素为True</td></tr><tr><td>np.where(存储数据容器的变量[取值范围]&gt;条件判断,1,0)</td><td>符合条件为1，不符合为0</td></tr><tr><td>np.where(np.logical_and(temp &gt; 60,temp&lt;90),1,0)</td><td>np.logical_and:符合条件为1，不符合为0，</td></tr><tr><td>np.where(np.logical_or(temp &gt; 90, temp &lt; 60), 1, 0)</td><td>np.logical_or：大于90或小于60的换为1，否则为0</td></tr><tr><td></td><td></td></tr><tr><td><strong>统计函数</strong></td><td></td></tr><tr><td>np.max(存储数据容器的变量,axis=0或者1)</td><td>按照列（0）或者行（1）进行提取，</td></tr><tr><td>np.min(存储数据容器的变量,axis=0或者1)</td><td>按照列（0）或者行（1）进行提取，</td></tr><tr><td>np.median()</td><td>先排序在找中位数</td></tr><tr><td>np.mean()</td><td>均差</td></tr><tr><td>np.std()</td><td>标准差</td></tr><tr><td>np.var()</td><td>求解方差</td></tr><tr><td>np.argmax(axis=)</td><td>最大元素对应的下标</td></tr><tr><td>np.argmin(axis=)</td><td>最小元素对应的下标</td></tr><tr><td>axis(“off”)</td><td>不按行或列</td></tr><tr><td>np.square</td><td>每个元素的经行平方</td></tr><tr><td><strong>sqrt()</strong></td><td>方法返回数字x的平方根</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>np.maximum（【】，【】）</td><td>能选取最大的列表（向量）</td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="广播机制：一个数组与另一个数组是否能不能运算"><a href="#广播机制：一个数组与另一个数组是否能不能运算" class="headerlink" title="广播机制：一个数组与另一个数组是否能不能运算"></a>广播机制：一个数组与另一个数组是否能不能运算</h2><p>横着为行。竖着为列，</p><p><strong>是否能相加</strong>：要从两个数组的后缘维度（从末尾开始算起的维度）的轴长度相符</p><p><strong>怎么查看维度</strong>：第一个是最外面的中括号有几个元素，后面进行递减查看</p><p><strong>矩阵的乘法遵循法则</strong>：第一横行乘于第一竖列再进行相加得出第一个相乘的数</p><h3 id="单位矩阵，逆，转置"><a href="#单位矩阵，逆，转置" class="headerlink" title="单位矩阵，逆，转置"></a><strong>单位矩阵</strong>，<strong>逆</strong>，<strong>转置</strong></h3><h6 id="单位矩阵：从-左上角到右下角的对角线（称为主对角线）上的元素均为-1-以外全都为-0"><a href="#单位矩阵：从-左上角到右下角的对角线（称为主对角线）上的元素均为-1-以外全都为-0" class="headerlink" title="单位矩阵：从 左上角到右下角的对角线（称为主对角线）上的元素均为 1 以外全都为 0"></a><strong>单位矩阵</strong>：<strong>从 左上角到右下角的对角线（称为主对角线）上的元素均为 1 以外全都为 0</strong></h6><p><strong>逆</strong>：两个数组相乘，称为了单位矩阵就算是一个数组是另一个数组的逆</p><p><strong>转置</strong>：行与列的互换</p><h2 id="NP的矩阵乘法要遵循下面的公式，也可以进过转至进行运算"><a href="#NP的矩阵乘法要遵循下面的公式，也可以进过转至进行运算" class="headerlink" title="NP的矩阵乘法要遵循下面的公式，也可以进过转至进行运算"></a>NP的矩阵乘法要遵循下面的公式，也可以进过转至进行运算</h2><table><thead><tr><th>函数</th><th>函数定义</th></tr></thead><tbody><tr><td>np.dot</td><td>矩阵进行相乘</td></tr><tr><td>np.matmul</td><td>矩阵进行相乘，单是不能矩阵与标量（一维）的相乘</td></tr><tr><td>.set_printoptions(<em>precision=None</em>, <em>threshold=None</em>, <em>edgeitems=None</em>, <em>linewidth=None</em>, <em>suppress=None</em>, <em>nanstr=None</em>, <em>infstr=None</em>, <em>formatter=None</em>, <em>sign=None</em>, <em>floatmode=None</em>, ***, <em>legacy=None</em>)</td><td><strong>precision</strong>：浮点精度可以设置，<strong>threshold</strong>：长数组可以总结为，<strong>suppress=True</strong>小结果可以被抑制，<strong>formatter={}<strong>自定义格式化程序可用于根据需要显示数组元素，</strong>要放回默认选项</strong>，可以使用(edgeitems=3, infstr=’inf’, linewidth=75, nanstr=’nan’, precision=8, suppress=False, threshold=1000, formatter=None)</td></tr></tbody></table><h3 id="按元素减去参数。"><a href="#按元素减去参数。" class="headerlink" title="按元素减去参数。"></a>按元素减去参数。</h3><h4 id="numpy-subtract"><a href="#numpy-subtract" class="headerlink" title="numpy.subtract"></a>numpy.subtract</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><code>numpy.subtract</code>(<em>x1</em>, <em>x2</em>, <em>/</em>, <em>out=None</em>, ***, <em>where=True</em>, <em>casting=’same_kind’</em>, <em>order=’K’</em>, <em>dtype=None</em>, <em>subok=True</em>[, <em>signature</em>, <em>extobj</em>])</td><td>参数 ：                                                                                                                                  <strong>x1, x2</strong>：要彼此相减的数组。如果 <code>x1.shape != x2.shape</code>，它们必须可以广播到一个普通的形状（将成为输出的形状，                                                                                          <strong>out</strong>：结果存储的位置。  如果提供，它必须具有输入广播到的形状。  如果未提供或没有，返回一个新分配的数组。  元组（只能作为关键字参数）的长度必须等于输出的数量</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><pre class=" language-python"><code class="language-python">np<span class="token punctuation">.</span>subtract<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">3.0</span>x1 <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">9.0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>x2 <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>subtract<span class="token punctuation">(</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这 -运算符可以用作 np.subtract上 数组。 </span>x1 <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">9.0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>x2 <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">)</span>x1 <span class="token operator">-</span> x2array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;numpy数组&quot;&gt;&lt;a href=&quot;#numpy数组&quot; class=&quot;headerlink&quot; title=&quot;numpy数组&quot;&gt;&lt;/a&gt;numpy数组&lt;/h2&gt;&lt;h4 id=&quot;numpy夹角运算&quot;&gt;&lt;a href=&quot;#numpy夹角运算&quot; class=&quot;header</summary>
      
    
    
    
    <category term="Numpy" scheme="https://xiaoyvlongoing.github.io/categories/Numpy/"/>
    
    
    <category term="Numpy" scheme="https://xiaoyvlongoing.github.io/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>SQL数据类型</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/SQL%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/SQL%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-11-04T07:21:05.951Z</updated>
    
    <content type="html"><![CDATA[<img src='https://suncos-01-1254144885.cos.ap-shanghai.myqcloud.com/Hexo/S.png' height=400px ><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><p>数据库：就是以一定格式进行组织的数据集合</p><p>关系是用表格存储的，二维表格存储</p><p>非关系是用字典类型存储的</p><p>数据库管理系统：DBMS</p><ul><li>数据库文件集合：主要一系列的数据文件，作用存储数据</li></ul><p>SQL语句：结构化查询语言</p><p>数据库文件一般都是.db后缀名</p><ol><li>取数据与数据库交流</li><li></li></ol><table><thead><tr><th>SQL语句</th><th>语句的性质</th></tr></thead><tbody><tr><td>sudo apt-get install mysql-server</td><td>安装服务端</td></tr><tr><td>sudo apt-get install mysql-client</td><td>安装客户端</td></tr><tr><td>sudo service mysql start</td><td>启动服务端</td></tr><tr><td>ps -ajx|grep mysql：就像是任务管理器</td><td>查看服务器是否开启，就像是任务管理器</td></tr><tr><td>mysql -uroot -pmysql</td><td>mysql -u用户名-pmysql登录：可以增删改查，创建表格</td></tr><tr><td>sudo service mysql stop</td><td>停止服务端</td></tr><tr><td>exit /quit/ctrl+d</td><td>退出</td></tr><tr><td>/etc/mysql/mysql.conf.d/mysqld.cnf</td><td>配置文件路径为</td></tr><tr><td></td><td></td></tr><tr><td>select now();</td><td>查看时间</td></tr><tr><td>select version();</td><td>查看版本信息</td></tr><tr><td></td><td></td></tr><tr><td>sudo service mysql restart</td><td><strong>重启MySQL服务</strong></td></tr></tbody></table><p>客户端Navicat使用</p><h2 id="SQL数据类型"><a href="#SQL数据类型" class="headerlink" title="SQL数据类型"></a>SQL数据类型</h2><table><thead><tr><th><strong>类型</strong></th><th><strong>功能</strong></th></tr></thead><tbody><tr><td><strong>字符串：</strong></td><td></td></tr><tr><td>char：定长字符串：最多255</td><td>名字不管多次都占10个字节（知道固定长度的字节）</td></tr><tr><td>varchar：变字符串最多65535</td><td>节省空间（适合经常变化的）（存多少占多数）</td></tr><tr><td>text</td><td>字符串 text 表示存储大文本，当字符大于 4000 时推荐使用</td></tr><tr><td>字段就是最上面的属性名.enum：枚举类型</td><td>列举出一些类型，只能在这些列举的范围选择</td></tr><tr><td></td><td></td></tr><tr><td><strong>年月日类型：</strong></td><td></td></tr><tr><td>date</td><td>年，月，日</td></tr><tr><td>datetime</td><td>年，月，日，时分秒</td></tr><tr><td>decimal：定点数</td><td>（5,2）共代表5个</td></tr><tr><td></td><td></td></tr><tr><td><strong>整型</strong></td><td></td></tr><tr><td>TINYINT：</td><td></td></tr><tr><td>SMALLINT</td><td></td></tr><tr><td>singned:有符号</td><td></td></tr><tr><td>unsingned：无符号</td><td></td></tr><tr><td></td><td></td></tr><tr><td><strong>小数类型</strong></td><td></td></tr><tr><td>float：小数类型</td><td></td></tr><tr><td>decimal表示浮点数</td><td>如 decimal(5, 2) 表示共存5位数，小数占 2 位.</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="数据完整性和约束"><a href="#数据完整性和约束" class="headerlink" title="数据完整性和约束"></a>数据完整性和约束</h3><h6 id="参照完整性，就是增删改查中：要保证数据的正确性，"><a href="#参照完整性，就是增删改查中：要保证数据的正确性，" class="headerlink" title="参照完整性，就是增删改查中：要保证数据的正确性，"></a>参照完整性，就是增删改查中：要保证数据的正确性，</h6><h5 id="约束：删除的时候不让删"><a href="#约束：删除的时候不让删" class="headerlink" title="约束：删除的时候不让删"></a>约束：删除的时候不让删</h5><p>保证数据的独一性</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>NOT NULL</td><td>非空约束</td></tr><tr><td>PRMARY KEY</td><td>主键约束</td></tr><tr><td>UNIQUE KE</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><p>数据库基本操作</p><table><thead><tr><th>数据表库的命令</th><th>使用的功能</th></tr></thead><tbody><tr><td>show databases;</td><td>查看所有数据库</td></tr><tr><td>select database();</td><td>查看当前使用的数据库</td></tr><tr><td>create database (数据库名字)  charset=utf8;</td><td>创建数据库</td></tr><tr><td>show  create database （数据库的名字）；</td><td>创建数据库查看数据库的字符集</td></tr><tr><td>drop database （数据库名称）;</td><td>删除数据表</td></tr><tr><td>数据表名.drop(数据表名[数据表名[对应的异常值这些异常值写出了T] == T].index,inplace = T)</td><td>对表里面的一些异常数据进行删除</td></tr><tr><td>use (数据库名称);   返回上一层服务器 use mysql</td><td>使用数据表进入指定的数据库</td></tr><tr><td></td><td></td></tr><tr><td>create database 数据库的名字</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="查看表结构命令："><a href="#查看表结构命令：" class="headerlink" title="查看表结构命令："></a>查看表结构命令：</h2><table><thead><tr><th>命令</th><th>命令介绍</th></tr></thead><tbody><tr><td>show tables;</td><td>产看数据库的所有表</td></tr><tr><td>desc 表名;</td><td>查看表里面的内容</td></tr><tr><td>show create table 表名;</td><td>查看数据表</td></tr><tr><td>select * from students;</td><td>查看添加的文档</td></tr></tbody></table><h2 id="创建数据表"><a href="#创建数据表" class="headerlink" title="创建数据表"></a>创建数据表</h2><pre><code>crea table 表名 （字段   类型    约束    【字段   类型     约束】）；crea table xxx(id int unsigned primary key auto_increment not null,name varchar(20));create table创建students 表（id ，name ，age， high， （decimal），gender（enum），cls_id）</code></pre><h6 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h6><pre class=" language-python"><code class="language-python">create table 列表名<span class="token punctuation">(</span>id int unsigned（无符号<span class="token number">0</span><span class="token operator">-</span><span class="token number">255</span>）  primary key（主键） auto_increment（自动增加） <span class="token operator">not</span> null（不能为空）<span class="token punctuation">,</span>name varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>（上限为<span class="token number">20</span>个字符） <span class="token operator">not</span> null<span class="token punctuation">,</span>age tinyint unsigned default  <span class="token number">0</span>（缺省值自动填充为<span class="token number">0</span>）<span class="token punctuation">,</span>height decimal<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>（一共有<span class="token number">5</span>位数，有小数点是有两位）gender enum<span class="token punctuation">(</span><span class="token string">"男"</span><span class="token punctuation">,</span>“女”<span class="token punctuation">,</span><span class="token string">"人妖"</span><span class="token punctuation">,</span><span class="token string">"保密"</span><span class="token punctuation">)</span>（枚举），time date default <span class="token string">"2020-01-01"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><pre><code>create table classes(id int unsigned  primary key auto_increment not null,name varchar(20） not null,id_age tinyint unsigned default  0);</code></pre><pre class=" language-python"><code class="language-python">create table students<span class="token punctuation">(</span>id int unsigned primary key  auto_increment <span class="token operator">not</span> null<span class="token punctuation">,</span>name varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token operator">not</span> null<span class="token punctuation">,</span>age tinyint unsigned default  <span class="token number">0</span><span class="token punctuation">,</span>height decimal<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>gender enum<span class="token punctuation">(</span><span class="token string">"男"</span><span class="token punctuation">,</span><span class="token string">"女"</span><span class="token punctuation">,</span><span class="token string">"人妖"</span><span class="token punctuation">,</span><span class="token string">"保密"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>time date default  <span class="token string">"2020-01-01"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><img src='https://boes.oss-cn-beijing.aliyuncs.com/1611042404103.png' height=px ><ul><li>Field：创建的字段</li><li>Type：属性的性质，和约束，枚举</li><li>Null：表示字段为空，yes表示可以为空</li><li>Key：唯一标识</li><li>Default：，默认值，如果字段被设计时，如果允许默认条件下，用户会进行数据的插入，那么就可以用事先准备好的数据来填充通常填充的是NULL</li><li>Extre：表示id可以顺序下去，</li></ul><table><thead><tr><th>alter table 表名 drop 列名;</th><th>删除字段</th></tr></thead><tbody><tr><td>drop table 表名;</td><td>删除表</td></tr><tr><td>delete from  表名 where id =数字几;</td><td>物理删除行</td></tr><tr><td>alter table   表名 add is_delete bit default 0;</td><td>逻辑增添</td></tr><tr><td>update 表名 set is_delete =1 where name = “行名”</td><td>逻辑删除</td></tr></tbody></table><h2 id="创建数据库和增删改查流程"><a href="#创建数据库和增删改查流程" class="headerlink" title="创建数据库和增删改查流程"></a>创建数据库和增删改查流程</h2><table><thead><tr><th>创建数据库命令</th><th>命令的作用</th></tr></thead><tbody><tr><td>mysql -uroot -pmysql;</td><td>登录数据库</td></tr><tr><td>quit、exit，</td><td>退出数据库</td></tr><tr><td>show databases;</td><td>查看所有的数据库</td></tr><tr><td>create database  数据库名称    charset=utf8;</td><td>创建数据库</td></tr><tr><td>select database();</td><td>查看当前使用数据库</td></tr><tr><td>use 数据库名称;</td><td>使用数据库</td></tr><tr><td>show tables；</td><td>查看当前数据库中所有的表</td></tr><tr><td>create table students(id  int  数据表名   primary  key  auto_increment  not mull,name varchar(20)  not  null, age tinyint unsigned default0,height decimal(5,2),gender enum(“男”，“女”));</td><td>创建表</td></tr><tr><td>alter table 表名 add 列名 类型   约束;</td><td>添加字段</td></tr><tr><td>alter table 表名 modify 列名 类型 约束;         例:alter table students modify birthday date not null;</td><td>修改字段类型及约束</td></tr><tr><td>alter table 表名 drop 列名</td><td>修改字段</td></tr><tr><td>show create table  表名;</td><td>查看数据表的类型，约束</td></tr><tr><td>show create database  数据库名;</td><td>查看数据库的类型</td></tr><tr><td>drop table 表名;</td><td>删除表</td></tr><tr><td><strong>增添数据</strong></td><td></td></tr><tr><td>insert into 表名  values(…..);：添加数据</td><td>添加数据值，对应着字段名字</td></tr><tr><td>insert into 表名 （列1.。。）values(值1，。。。);       insert into students (name,age,height,gender,time) values(“姓名”,18,180.22,”男”,”2020-07-08”);</td><td>部分的插入，对应的字段和插入的内容</td></tr><tr><td>insert into 表名 values（，，，），（。。。）。。;</td><td>全列多行插入</td></tr><tr><td>insert into 表名 （列1，。。。） values（值1，。。。），（值1.。）;</td><td>部分列多行插入</td></tr><tr><td>select * from 表名;</td><td>查看表的内容</td></tr><tr><td>select 列1 ，列2 。。。from 表名;</td><td>查看指定字段的内容</td></tr><tr><td>update 表名 set 列1 = 值1，列2=值2.。。where  条件;    列如:update students set age = 18, gender = ‘女’ where id = 6;</td><td>修改的数据</td></tr><tr><td>delete from 表名 where 条件;    列如:delete from students where id = 3;</td><td>删除数据（物理删除）</td></tr><tr><td>alter table   表名 add is_delete bit default 0;</td><td>逻辑的添加</td></tr><tr><td>update 表名 set is_delete =1 where name = “行名”</td><td>逻辑的删除，要先增添，才能删除</td></tr><tr><td>drop database 数据库名称;</td><td>删除数据库</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&#39;https://suncos-01-1254144885.cos.ap-shanghai.myqcloud.com/Hexo/S.png&#39; height=400px &gt;



&lt;h2 id=&quot;数据库&quot;&gt;&lt;a href=&quot;#数据库&quot; class=&quot;headerl</summary>
      
    
    
    
    <category term="SQL" scheme="https://xiaoyvlongoing.github.io/categories/SQL/"/>
    
    
    <category term="SQL" scheme="https://xiaoyvlongoing.github.io/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>fasttext工具</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/fasttext%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/fasttext%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-11-04T09:03:44.312Z</updated>
    
    <content type="html"><![CDATA[<img src='https://github.com/xiaoyvlongoing/xiaoyvlongoing.github.io/blob/main/favicon.png' height=400px ><h1 id="fasttext工具的使用"><a href="#fasttext工具的使用" class="headerlink" title="fasttext工具的使用"></a>fasttext工具的使用</h1><p>作为NLP工程领域常用的工具包, fasttext有两大作用:</p><ul><li>进行文本分类</li><li>训练词向量</li></ul><h3 id="使用fasttext工具进行文本分类的过程"><a href="#使用fasttext工具进行文本分类的过程" class="headerlink" title="使用fasttext工具进行文本分类的过程"></a>使用fasttext工具进行文本分类的过程</h3><ul><li>第一步: 获取数据</li><li>第二步: 训练集与验证集的划分</li><li>第三步: 训练模型</li><li>第四步: 使用模型进行预测并评估</li><li>第五步: 模型调优</li><li>第六步: 模型保存与重加载 </li></ul><hr><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>import fasttext</td><td>构建fasttext工具包</td></tr><tr><td></td><td></td></tr><tr><td>model=fasttext.<strong>train_supervised</strong>（(input=文件名，epoch=数值轮数表示，wordNgrams=关联，loss=’hs’，“ova”）</td><td>使用fasttext的train_supervised方法<strong>进行文本分类模型</strong>的训练，                                                                                <strong>wordNgrams</strong>：设置为2意味着添加2-gram特征, 这些特征帮助模型捕捉前后词汇之间的关联, 更好的提取分类规则用于模型分类, 当然这也会增加模型训时练占用的资源和时间，<strong>loss</strong>：来修改损失计算方式(等效于输出层的结构), 默认是softmax层结构，设置为’hs’, 代表层次softmax结构, 意味着输出层的结构(计算方式)发生了变化, 将以一种更低复杂度的方式来计算损失，“ova”：标签多分类问题的损失计算方式，</td></tr><tr><td></td><td></td></tr><tr><td>model.predict(“一句话”)</td><td>模型进行预测并评估</td></tr><tr><td>model.test（“需要评估的数据集”）</td><td>为了评估模型到底表现如何,元组中的每项分别代表, 验证集样本数量, 精度以及召回率</td></tr><tr><td></td><td></td></tr><tr><td>model.predict（“一句话”，k=-1, threshold=0.5）</td><td>模型进行预测并评估，k=-1：尽可能输出多个标签，threshold：阈值</td></tr><tr><td></td><td></td></tr><tr><td>model.save_model(“./模型名字.bin”)</td><td>保存模型</td></tr><tr><td></td><td></td></tr><tr><td>model = fasttext.load_model(“./模型名字.bin”)</td><td>模型重加载</td></tr></tbody></table><h2 id="词向量迁移"><a href="#词向量迁移" class="headerlink" title="词向量迁移"></a>词向量迁移</h2><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>model = fasttext.load_model(“模型名字.bin”)</td><td>加载模型</td></tr><tr><td>model.words[：100]</td><td>查看前100个词汇(这里的词汇是广义的, 可以是中文符号或汉字))</td></tr><tr><td>model.get_word_vector（“词名”）</td><td>使用模型获得’词名’这个名词的词向量</td></tr><tr><td>model.get_nearest_neighbors（“词名”）</td><td>以’音乐’为例, 返回的邻近词基本上与音乐都有关系, 如乐曲, 音乐会, 声乐等.</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&#39;https://github.com/xiaoyvlongoing/xiaoyvlongoing.github.io/blob/main/favicon.png&#39; height=400px &gt;









&lt;h1 id=&quot;fasttext工具的使用&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="NLP" scheme="https://xiaoyvlongoing.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://xiaoyvlongoing.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>二分查找</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-11-04T05:53:34.093Z</updated>
    
    <content type="html"><![CDATA[<h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><ol><li><h5 id="二分查找要求是有序，是为了索引快速查找到中间值"><a href="#二分查找要求是有序，是为了索引快速查找到中间值" class="headerlink" title="二分查找要求是有序，是为了索引快速查找到中间值"></a>二分查找要求是有序，是为了索引快速查找到中间值</h5></li><li><h5 id="二分查找要求是顺序存储的"><a href="#二分查找要求是顺序存储的" class="headerlink" title="二分查找要求是顺序存储的"></a>二分查找要求是顺序存储的</h5></li><li><h5 id="二分查找是从中间开始查找的"><a href="#二分查找是从中间开始查找的" class="headerlink" title="二分查找是从中间开始查找的"></a>二分查找是从中间开始查找的</h5></li><li><h5 id="时间复杂度最好O（1），最坏O（logn）"><a href="#时间复杂度最好O（1），最坏O（logn）" class="headerlink" title="时间复杂度最好O（1），最坏O（logn）"></a>时间复杂度最好O（1），最坏O（logn）</h5></li></ol><h2 id="递归版本"><a href="#递归版本" class="headerlink" title="递归版本"></a>递归版本</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">binary_search</span><span class="token punctuation">(</span>alist<span class="token punctuation">,</span>item<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> len<span class="token punctuation">(</span>alist<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">False</span>    n <span class="token operator">=</span> len<span class="token punctuation">(</span>alist<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>    <span class="token keyword">if</span> item <span class="token operator">==</span> alist<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token boolean">True</span>    <span class="token keyword">elif</span> item <span class="token operator">&lt;</span> alist<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> binary_search<span class="token punctuation">(</span>alist<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">,</span>item<span class="token punctuation">)</span>    <span class="token keyword">elif</span> item <span class="token operator">></span> alist<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">return</span>  binary_search<span class="token punctuation">(</span>alist<span class="token punctuation">[</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>item<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    alist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>binary_search<span class="token punctuation">(</span>alist<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>binary_search<span class="token punctuation">(</span>alist<span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="非递归版本"><a href="#非递归版本" class="headerlink" title="非递归版本"></a>非递归版本</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">binary_search</span><span class="token punctuation">(</span>alist<span class="token punctuation">,</span>item<span class="token punctuation">)</span><span class="token punctuation">:</span>    start <span class="token operator">=</span> <span class="token number">0</span>    end <span class="token operator">=</span> len<span class="token punctuation">(</span>alist<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span>    <span class="token keyword">while</span> start <span class="token operator">&lt;=</span> end<span class="token punctuation">:</span>        mid <span class="token operator">=</span><span class="token punctuation">(</span>start<span class="token operator">+</span>end<span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">2</span>        <span class="token keyword">if</span> item <span class="token operator">==</span> alist<span class="token punctuation">[</span>mid<span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">True</span><span class="token comment" spellcheck="true">#要是找到数值的下标的话返回mid</span>        <span class="token keyword">elif</span> item <span class="token operator">&lt;</span> alist<span class="token punctuation">[</span>mid<span class="token punctuation">]</span><span class="token punctuation">:</span>            end <span class="token operator">=</span> mid <span class="token operator">-</span><span class="token number">1</span>        <span class="token keyword">elif</span> item <span class="token operator">></span> alist<span class="token punctuation">[</span>mid<span class="token punctuation">]</span><span class="token punctuation">:</span>            start <span class="token operator">=</span> mid <span class="token operator">+</span><span class="token number">1</span>    <span class="token keyword">return</span> <span class="token boolean">False</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    alist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>binary_search<span class="token punctuation">(</span>alist<span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="查找3的第一个数"><a href="#查找3的第一个数" class="headerlink" title="查找3的第一个数"></a>查找3的第一个数</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">binary_search</span><span class="token punctuation">(</span>alist<span class="token punctuation">,</span>item<span class="token punctuation">)</span><span class="token punctuation">:</span>    start <span class="token operator">=</span> <span class="token number">0</span>    end <span class="token operator">=</span> len<span class="token punctuation">(</span>alist<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span>    <span class="token keyword">while</span> start <span class="token operator">&lt;=</span> end<span class="token punctuation">:</span>        mid <span class="token operator">=</span><span class="token punctuation">(</span>start<span class="token operator">+</span>end<span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">2</span>        <span class="token keyword">if</span> item <span class="token operator">==</span> alist<span class="token punctuation">[</span>mid<span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> mid <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">or</span> alist<span class="token punctuation">[</span>mid<span class="token number">-1</span><span class="token punctuation">]</span> <span class="token operator">!=</span>item<span class="token punctuation">:</span>                <span class="token keyword">return</span> mid            <span class="token keyword">else</span><span class="token punctuation">:</span>                end <span class="token operator">=</span> mid <span class="token operator">-</span><span class="token number">1</span>        <span class="token keyword">elif</span> item <span class="token operator">&lt;</span> alist<span class="token punctuation">[</span>mid<span class="token punctuation">]</span><span class="token punctuation">:</span>            end <span class="token operator">=</span> mid <span class="token operator">-</span><span class="token number">1</span>        <span class="token keyword">elif</span> item <span class="token operator">></span> alist<span class="token punctuation">[</span>mid<span class="token punctuation">]</span><span class="token punctuation">:</span>            start <span class="token operator">=</span> mid <span class="token operator">+</span><span class="token number">1</span>    <span class="token keyword">return</span> <span class="token boolean">False</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    alist <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>binary_search<span class="token punctuation">(</span>alist<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="线性结构：就是一对一的关系"><a href="#线性结构：就是一对一的关系" class="headerlink" title="线性结构：就是一对一的关系"></a>线性结构：就是一对一的关系</h3><h3 id="非线性结构：一个节点可能对应着前驱后继的多个节点"><a href="#非线性结构：一个节点可能对应着前驱后继的多个节点" class="headerlink" title="非线性结构：一个节点可能对应着前驱后继的多个节点"></a>非线性结构：一个节点可能对应着前驱后继的多个节点</h3><h2 id="树：非线性"><a href="#树：非线性" class="headerlink" title="树：非线性"></a>树：非线性</h2><ol><li><h5 id="有序树，二叉树：每个节点最多含有二个子树为二叉树"><a href="#有序树，二叉树：每个节点最多含有二个子树为二叉树" class="headerlink" title="有序树，二叉树：每个节点最多含有二个子树为二叉树"></a>有序树，二叉树：每个节点最多含有二个子树为二叉树</h5></li><li><h5 id="完全二叉树，除了最后一层节点除外，其他的节点都满了为完全二叉树，最后一层还是紧密排序的"><a href="#完全二叉树，除了最后一层节点除外，其他的节点都满了为完全二叉树，最后一层还是紧密排序的" class="headerlink" title="完全二叉树，除了最后一层节点除外，其他的节点都满了为完全二叉树，最后一层还是紧密排序的"></a>完全二叉树，除了最后一层节点除外，其他的节点都满了为完全二叉树，最后一层还是紧密排序的</h5></li><li><h5 id="满二叉树：所有节点都满了没有一个空的为满二叉树"><a href="#满二叉树：所有节点都满了没有一个空的为满二叉树" class="headerlink" title="满二叉树：所有节点都满了没有一个空的为满二叉树"></a>满二叉树：所有节点都满了没有一个空的为满二叉树</h5></li><li><h5 id="平衡二叉树：当且仅当任何节点的两棵子树的高度差不大于1的二叉树"><a href="#平衡二叉树：当且仅当任何节点的两棵子树的高度差不大于1的二叉树" class="headerlink" title="平衡二叉树：当且仅当任何节点的两棵子树的高度差不大于1的二叉树"></a>平衡二叉树：当且仅当任何节点的两棵子树的高度差不大于1的二叉树</h5></li><li><h5 id="排序二叉树："><a href="#排序二叉树：" class="headerlink" title="排序二叉树："></a>排序二叉树：</h5><h6 id="1：若左子树不为空，则左子树所有节点的值均小于它的根节点"><a href="#1：若左子树不为空，则左子树所有节点的值均小于它的根节点" class="headerlink" title="1：若左子树不为空，则左子树所有节点的值均小于它的根节点"></a>1：若左子树不为空，则左子树所有节点的值均小于它的根节点</h6><h6 id="2：若右子树不为空，则右子树所有节点的值均大于它的根节点"><a href="#2：若右子树不为空，则右子树所有节点的值均大于它的根节点" class="headerlink" title="2：若右子树不为空，则右子树所有节点的值均大于它的根节点"></a>2：若右子树不为空，则右子树所有节点的值均大于它的根节点</h6><h6 id="3：子节点的树左右也为排序"><a href="#3：子节点的树左右也为排序" class="headerlink" title="3：子节点的树左右也为排序"></a>3：子节点的树左右也为排序</h6></li></ol><p>​    </p><h2 id="链表二叉树"><a href="#链表二叉树" class="headerlink" title="链表二叉树"></a>链表二叉树</h2><ol><li><h5 id="二叉树的第i层至多有2-i-1节点"><a href="#二叉树的第i层至多有2-i-1节点" class="headerlink" title="二叉树的第i层至多有2^i-1节点"></a>二叉树的第i层至多有2^i-1节点</h5></li><li><h5 id="深度为k的二叉树至多与2-k-1个节点"><a href="#深度为k的二叉树至多与2-k-1个节点" class="headerlink" title="深度为k的二叉树至多与2^k-1个节点"></a>深度为k的二叉树至多与2^k-1个节点</h5></li></ol><h2 id="二叉树的广度优先遍历"><a href="#二叉树的广度优先遍历" class="headerlink" title="二叉树的广度优先遍历"></a>二叉树的广度优先遍历</h2><h5 id="深度优先：深度优先，选择一个路直达找到终点"><a href="#深度优先：深度优先，选择一个路直达找到终点" class="headerlink" title="深度优先：深度优先，选择一个路直达找到终点"></a>深度优先：深度优先，选择一个路直达找到终点</h5><h5 id="广度优先：可以找到最短的路径，先选择一条路找一层，没有再选择一条路径进行找，这一层没有就要从新选路走两层，再换直达找到。"><a href="#广度优先：可以找到最短的路径，先选择一条路找一层，没有再选择一条路径进行找，这一层没有就要从新选路走两层，再换直达找到。" class="headerlink" title="广度优先：可以找到最短的路径，先选择一条路找一层，没有再选择一条路径进行找，这一层没有就要从新选路走两层，再换直达找到。"></a>广度优先：可以找到最短的路径，先选择一条路找一层，没有再选择一条路径进行找，这一层没有就要从新选路走两层，再换直达找到。</h5><h2 id="深度优先遍历"><a href="#深度优先遍历" class="headerlink" title="深度优先遍历"></a>深度优先遍历</h2><h5 id="先序遍历：根左右，先写根，然后再靠左边的数值都写出来，写到底，然后在写最后叶节点的右边"><a href="#先序遍历：根左右，先写根，然后再靠左边的数值都写出来，写到底，然后在写最后叶节点的右边" class="headerlink" title="先序遍历：根左右，先写根，然后再靠左边的数值都写出来，写到底，然后在写最后叶节点的右边"></a>先序遍历：根左右，先写根，然后再靠左边的数值都写出来，写到底，然后在写最后叶节点的右边</h5><h5 id="中序遍历：左根右，左边开始，要从最左边的叶节点开始写"><a href="#中序遍历：左根右，左边开始，要从最左边的叶节点开始写" class="headerlink" title="中序遍历：左根右，左边开始，要从最左边的叶节点开始写"></a>中序遍历：左根右，左边开始，要从最左边的叶节点开始写</h5><h5 id="后序遍历：左右根，从叶节点开始先左右再跟"><a href="#后序遍历：左右根，从叶节点开始先左右再跟" class="headerlink" title="后序遍历：左右根，从叶节点开始先左右再跟"></a>后序遍历：左右根，从叶节点开始先左右再跟</h5><h2 id="遍历的结果反推处理二叉树的是什么结构"><a href="#遍历的结果反推处理二叉树的是什么结构" class="headerlink" title="遍历的结果反推处理二叉树的是什么结构"></a>遍历的结果反推处理二叉树的是什么结构</h2><h5 id="知道中序遍历，和先序遍历就可以找到规律"><a href="#知道中序遍历，和先序遍历就可以找到规律" class="headerlink" title="知道中序遍历，和先序遍历就可以找到规律"></a>知道中序遍历，和先序遍历就可以找到规律</h5>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;二分查找&quot;&gt;&lt;a href=&quot;#二分查找&quot; class=&quot;headerlink&quot; title=&quot;二分查找&quot;&gt;&lt;/a&gt;二分查找&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;h5 id=&quot;二分查找要求是有序，是为了索引快速查找到中间值&quot;&gt;&lt;a href=&quot;#二分查找要求是有序，是为了</summary>
      
    
    
    
    <category term="算法，树" scheme="https://xiaoyvlongoing.github.io/categories/%E7%AE%97%E6%B3%95%EF%BC%8C%E6%A0%91/"/>
    
    
    <category term="算法" scheme="https://xiaoyvlongoing.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>线程任务</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/%E5%A4%9A%E7%BA%BF%E7%A8%8B/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-06-24T17:12:15.352Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多线程任务"><a href="#多线程任务" class="headerlink" title="多线程任务"></a>多线程任务</h2><h3 id="一：多线程"><a href="#一：多线程" class="headerlink" title="一：多线程"></a>一：多线程</h3><table><thead><tr><th>函数名</th><th>使用的方法</th></tr></thead><tbody><tr><td>导入的函数包import multiprocessing</td><td>创建的进程导入的包名</td></tr><tr><td>multiprocessing.Process([group [, target [, name [, args：传入指定形参 [, kwargs：以字典的类型进行传参]]]]])</td><td>导入进程函数说明</td></tr><tr><td>target=方法名</td><td>执行的目标任务名（函数名，线程名）</td></tr><tr><td>name=进程的名字</td><td>进程名字</td></tr><tr><td>对象名=multiprocessing.Process(target=方法名)</td><td>启动进程的方法</td></tr><tr><td>子进程对象.start()</td><td>对象.start（）：启动子进程实例（创建子进程）</td></tr><tr><td>os.getpid</td><td>print(os.getpid)查看子进程编号</td></tr><tr><td>os.getppid</td><td>print(os.getppid)查看父进程编号</td></tr><tr><td>multiprocessing.current_process().name进程名字</td><td>查看当前进程编号</td></tr><tr><td>子进程对象.join()</td><td>加入jojn后主线程会等待子线程执行后再结束主线程，让线程可以形成串行，形成有顺序的，线性和进程的也可进行串行，结束前一个，进行下一个执行</td></tr><tr><td>子进程对象.terminate()</td><td>不管任务是否完成，立即终止子进程</td></tr><tr><td>子进程对象.daemom = True</td><td>不管任务是否完成，立即终止子进程</td></tr><tr><td>进程在ubuntu中 ps -aux|grep 要查找的线程的名字</td><td>过滤需要的进程来查看ID</td></tr><tr><td>os.kill(os.getpid(),9)  进程的id</td><td>强制杀死进程</td></tr><tr><td></td><td></td></tr></tbody></table><p>1：进程是操作系统资源分配的最小单位</p><p>2：进程之间执行也是无序的，它是由操作系统调度决定的，操作系统调度哪个进程，哪个进程就先执行，没有调度的进程不能执行</p><p>3：进程不共享全局变量</p><h3 id="二：线程和进程有什么区别："><a href="#二：线程和进程有什么区别：" class="headerlink" title="二：线程和进程有什么区别："></a>二：线程和进程有什么区别：</h3><p>1：进程是操作系统资源分配的最小单位，线程是cpu调度的最小单位</p><p>2：线程依附于进程，没有进程就没有线程，一个进程默认创造一个主线程，一个进程可以创造多个线程</p><p>3：进程不共享全局变量，而同一个进程的线程可以共享进程的全局变量</p><p>4：进程开销比较大，可以利用多核（并行），线程开销比较小不能利用多核（并发）</p><h3 id="三：什么是主进程："><a href="#三：什么是主进程：" class="headerlink" title="三：什么是主进程："></a>三：什么是主进程：</h3><p>主进程是：程序执行的入口，一个进程里默认带一个线程，主进程可以创建多个子线程，</p><h3 id="四：写出创造进程的步骤："><a href="#四：写出创造进程的步骤：" class="headerlink" title="四：写出创造进程的步骤："></a>四：写出创造进程的步骤：</h3><pre class=" language-python"><code class="language-python"><span class="token number">1</span>：导入进程包：​        <span class="token keyword">import</span>  multiprocessing<span class="token number">2</span>：创建进程函数（方法）​        <span class="token keyword">def</span> num（）：​                <span class="token keyword">pass</span><span class="token number">3</span>：创建进程，调用进程函数（方法）​        变量 <span class="token operator">=</span> multiprocessing<span class="token punctuation">.</span>Process<span class="token punctuation">(</span>target <span class="token operator">=</span> num<span class="token punctuation">)</span><span class="token number">4</span><span class="token punctuation">:</span>启动进程        变量<span class="token punctuation">.</span>start（）</code></pre><h3 id="五：怎么获得进程的pid"><a href="#五：怎么获得进程的pid" class="headerlink" title="五：怎么获得进程的pid"></a>五：怎么获得进程的pid</h3><pre class=" language-python"><code class="language-python"><span class="token number">1</span>：improre  os <span class="token number">2</span>：os<span class="token punctuation">.</span>getppid <span class="token punctuation">:</span>   <span class="token keyword">print</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>getppid<span class="token punctuation">)</span>查看父进程id编号<span class="token number">3</span>：os<span class="token punctuation">.</span>getpid  ：   <span class="token keyword">print</span>（os<span class="token punctuation">.</span>getpid）查看当前进程id编号<span class="token number">4</span>：multiprocessing<span class="token punctuation">.</span>current_process<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>name进程（线程）名字   ：查看当前进程（线程）的id编号    </code></pre><h3 id="六：怎么给进程函数传递参数："><a href="#六：怎么给进程函数传递参数：" class="headerlink" title="六：怎么给进程函数传递参数："></a>六：怎么给进程函数传递参数：</h3><p>1：通过元组进程传递参数</p><pre><code>multiprocessing.Process(target = coding , args = （参数，）)传入的是一个元组需要加上，传入的参数要保持一致</code></pre><h3 id="2：通过字典进程传参："><a href="#2：通过字典进程传参：" class="headerlink" title="2：通过字典进程传参："></a>2：通过字典进程传参：</h3><pre><code>multiprocessing.Process(target = coding , kwargs = &#123;传递的进程变量名：传入的参数&#125;)因为用的是字典的要与进程参数对住</code></pre><h3 id="七：在主进程中，怎么堵塞等待子进程完成后，才能继续运行"><a href="#七：在主进程中，怎么堵塞等待子进程完成后，才能继续运行" class="headerlink" title="七：在主进程中，怎么堵塞等待子进程完成后，才能继续运行"></a>七：在主进程中，怎么堵塞等待子进程完成后，才能继续运行</h3><p>1：把join加入到子进程中，子进程变量.join，形成串联运行完子进程在运行主线线程</p><p>八：Process创建的实例对象的常用的方法有什么？</p><p>1：start（）：启动子进程实例（创建子进程）</p><p>2：join（）：等待子进程执行结束，形成串联</p><p>3：terminate（）：不管任务是否完成，立即终止子进程</p><p>八《一》：    Process常用的属性：</p><p>1：name：当前进程的别名，默认为Process - n ，n为从1开始递增的整数</p><p>2：pid：当前进程的id</p><p>九：编写一个程序，实现创建一个子进程，子进程每一秒打印一次：这是子进程，循环的第n次，打印 的次数是由主进程传递给子进程。</p><pre><code>1：#利用args和kwargs传参import multiprocessingimport timedef ness(num):    for i in range(num):        time.sleep(1)        print(&quot;这个是一个线程&quot;)if __name__ == &#39;__main__&#39;:    # mues = multiprocessing.Process(target= ness,args=(6,))    mues = multiprocessing.Process(target=ness, kwargs=&#123;&quot;num&quot;:6&#125;)    mues.start()2：#利用类方法实现线程import multiprocessingimport timeclass nees():    def __init__(self,name):        super(nees, self).__init__()        self.name = name    def num(self):        for i in range(self.name):            time.sleep(1)            print(&quot;这个是一个子进程&quot;)if __name__ == &#39;__main__&#39;:    num_process = multiprocessing.Process(target=nees)    A=nees(3)    A.num()</code></pre><p>：线程资源竞争问题：</p><p>1：100万次加法，线程1和线程2谁先执行完</p><p>2：线程1计算的结果一定比线程2小吗？</p><p>3：某个线程计算的结果有没有可能小于100万？为什么：</p><p>​            1：不一定谁先执行完，因为线程是由cpu进行调度的，没有执行顺序那个先执行完都是不一定的</p><p>​            2：不一定，因为没办法知道那个线程能先执行完，所有没有办法知道执行的时间，没有办法知道执行的大小</p><p>​            3：有的，因为多线程共享全局变量，再赋予全局变量的时候，线程一和线程二计算的先后没有办法确定下来，当线程一计算的多时，线程二计算的少时，线程一又把线程二中的值共享过来这时就不安之前的顺序的，会小于100万</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;多线程任务&quot;&gt;&lt;a href=&quot;#多线程任务&quot; class=&quot;headerlink&quot; title=&quot;多线程任务&quot;&gt;&lt;/a&gt;多线程任务&lt;/h2&gt;&lt;h3 id=&quot;一：多线程&quot;&gt;&lt;a href=&quot;#一：多线程&quot; class=&quot;headerlink&quot; title=&quot;一：多</summary>
      
    
    
    
    <category term="python高级" scheme="https://xiaoyvlongoing.github.io/categories/python%E9%AB%98%E7%BA%A7/"/>
    
    
    <category term="python高级" scheme="https://xiaoyvlongoing.github.io/tags/python%E9%AB%98%E7%BA%A7/"/>
    
  </entry>
  
  <entry>
    <title>神经网络--猫狗分类</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%8C%AB%E7%8B%97%E5%88%86%E7%B1%BB/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%8C%AB%E7%8B%97%E5%88%86%E7%B1%BB/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-06-24T16:57:20.464Z</updated>
    
    <content type="html"><![CDATA[<h2 id="读取猫狗的图片矩阵数据，自实现神经元神经网络进行二分类"><a href="#读取猫狗的图片矩阵数据，自实现神经元神经网络进行二分类" class="headerlink" title="读取猫狗的图片矩阵数据，自实现神经元神经网络进行二分类"></a>读取猫狗的图片矩阵数据，自实现神经元神经网络进行二分类</h2><p>1：读取数据集<br>2：实现前向传播与反向传播<br>3：模型预测函数实现</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> h5py<span class="token comment" spellcheck="true">#这个读取</span><span class="token keyword">def</span> <span class="token function">load_dataset</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    train_dataset <span class="token operator">=</span> h5py<span class="token punctuation">.</span>File<span class="token punctuation">(</span><span class="token string">'datasets/train_catvnoncat.h5'</span><span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#从datasets中读取，从训练和测试两个文件中</span>    train_set_x_orig <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>train_dataset<span class="token punctuation">[</span><span class="token string">"train_set_x"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># your train set features</span>    train_set_y_orig <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>train_dataset<span class="token punctuation">[</span><span class="token string">"train_set_y"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># your train set labels</span>    test_dataset <span class="token operator">=</span> h5py<span class="token punctuation">.</span>File<span class="token punctuation">(</span><span class="token string">'datasets/test_catvnoncat.h5'</span><span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span>    test_set_x_orig <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>test_dataset<span class="token punctuation">[</span><span class="token string">"test_set_x"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># your test set features</span>    test_set_y_orig <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>test_dataset<span class="token punctuation">[</span><span class="token string">"test_set_y"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># your test set labels</span>    classes <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>test_dataset<span class="token punctuation">[</span><span class="token string">"list_classes"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># the list of classes</span>    <span class="token comment" spellcheck="true">#对应的类别分别通过下边返回了</span>    train_set_y_orig <span class="token operator">=</span> train_set_y_orig<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> train_set_y_orig<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    test_set_y_orig <span class="token operator">=</span> test_set_y_orig<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> test_set_y_orig<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> train_set_x_orig<span class="token punctuation">,</span> train_set_y_orig<span class="token punctuation">,</span> test_set_x_orig<span class="token punctuation">,</span> test_set_y_orig<span class="token punctuation">,</span> classes    主体函数导入相关包<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> data <span class="token keyword">import</span> load_dataset<span class="token comment" spellcheck="true">#数据结构的包</span>主体逻辑<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 1、读取样本数据</span>    train_x<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_x<span class="token punctuation">,</span> test_y<span class="token punctuation">,</span> classes <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#读出训练集，测试集和训练的类别</span><span class="token comment" spellcheck="true">#打印出形状</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"训练集的样本数: "</span><span class="token punctuation">,</span> train_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"测试集的样本数: "</span><span class="token punctuation">,</span> test_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"train_x形状: "</span><span class="token punctuation">,</span> train_x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"train_y形状: "</span><span class="token punctuation">,</span> train_y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"test_x形状: "</span><span class="token punctuation">,</span> test_x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"test_x形状: "</span><span class="token punctuation">,</span> test_y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 输入数据的形状修改以及归一化</span>    train_x <span class="token operator">=</span> train_x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>train_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T    test_x <span class="token operator">=</span> test_x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>test_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T    train_x <span class="token operator">=</span> train_x <span class="token operator">/</span> <span class="token number">255</span><span class="token punctuation">.</span>    test_x <span class="token operator">=</span> test_x <span class="token operator">/</span> <span class="token number">255</span><span class="token punctuation">.</span>    <span class="token comment" spellcheck="true"># 2、模型训练以及预测，封装到model的中</span>    d <span class="token operator">=</span> model<span class="token punctuation">(</span>train_x<span class="token punctuation">,</span> train_y<span class="token punctuation">,</span> test_x<span class="token punctuation">,</span> test_y<span class="token punctuation">,</span> num_iterations<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.005</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#训练和测试的数据集还有迭代的步骤步数还有学习率</span><span class="token number">2</span>、模型预测函数实现模型函数<span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> Y_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> Y_test<span class="token punctuation">,</span> num_iterations<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    """</span><span class="token comment" spellcheck="true">#先初始模型的参数，初始化单个的神经元一个逻辑回归,wb是某个神经元的参数</span>    <span class="token comment" spellcheck="true"># 初始化参数</span>    w<span class="token punctuation">,</span> b <span class="token operator">=</span> initialize_with_zeros<span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#下面进行优化通过optimize去实现模型的整个梯度下降的过程，每一次更新都要通过optimize函数去做</span><span class="token comment" spellcheck="true">#模型训练得出来的都是参数，损失的列表，每一次更新损失降了多少个，驯良的结果返回出来</span>    <span class="token comment" spellcheck="true"># 梯度下降</span>    <span class="token comment" spellcheck="true"># params:更新后的网络参数</span>    <span class="token comment" spellcheck="true"># grads:最后一次梯度</span>    <span class="token comment" spellcheck="true"># costs:每次更新的损失列表</span>    params<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> costs <span class="token operator">=</span> optimize<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X_train<span class="token punctuation">,</span> Y_train<span class="token punctuation">,</span> num_iterations<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#参数，每一次的梯度，损失</span>    <span class="token comment" spellcheck="true"># 获取训练的参数</span>    <span class="token comment" spellcheck="true"># 预测结果</span>    w <span class="token operator">=</span> params<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span>    b <span class="token operator">=</span> params<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">#用你输出的进行预测</span>    <span class="token comment" spellcheck="true">#用你测试的进行预测得到测试集的效果</span>    Y_prediction_train <span class="token operator">=</span> predict<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X_train<span class="token punctuation">)</span>    Y_prediction_test <span class="token operator">=</span> predict<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X_test<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#预测出来的都是0,1的结果</span>    <span class="token comment" spellcheck="true"># 打印准确率</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"训练集准确率: &amp;#123;&amp;#125; "</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">100</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>abs<span class="token punctuation">(</span>Y_prediction_train <span class="token operator">-</span> Y_train<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>#预测结果与真正的结果进行相减，abs：绝对值，mean：再求平均值，平均结果就是错误率，乘<span class="token number">100</span>，得到了多少，再用<span class="token number">100</span>减去就是准确率    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"测试集准确率: &amp;#123;&amp;#125; "</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">100</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>abs<span class="token punctuation">(</span>Y_prediction_test <span class="token operator">-</span> Y_test<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    d <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"costs": costs,</span>         <span class="token string">"Y_prediction_test"</span><span class="token punctuation">:</span> Y_prediction_test<span class="token punctuation">,</span>         <span class="token string">"Y_prediction_train"</span><span class="token punctuation">:</span> Y_prediction_train<span class="token punctuation">,</span>         <span class="token string">"w"</span><span class="token punctuation">:</span> w<span class="token punctuation">,</span>         <span class="token string">"b"</span><span class="token punctuation">:</span> b<span class="token punctuation">,</span>         <span class="token string">"learning_rate"</span><span class="token punctuation">:</span> learning_rate<span class="token punctuation">,</span>         <span class="token string">"num_iterations"</span><span class="token punctuation">:</span> num_iterations<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span> <span class="token comment" spellcheck="true">#把预测的结果损失，还有权重参数，学习率，迭代的次数，返回出去</span>    <span class="token keyword">return</span> d其中涉及两个函数一个是sigmoid另一个是初始化函数：<span class="token keyword">def</span> <span class="token function">basic_sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    计算sigmoid函数    """</span>    s <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> s<span class="token keyword">def</span> <span class="token function">initialize_with_zeros</span><span class="token punctuation">(</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    创建一个形状为 (shape, 1) 的w参数和b=0.    return:w, b    """</span>    w <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>shape<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    b <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">return</span> w<span class="token punctuation">,</span> b<span class="token number">3</span>、前向传播与反向传播实现<span class="token keyword">def</span> <span class="token function">optimize</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">,</span> num_iterations<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    参数：    w:权重,b:偏置,X特征,Y目标值,num_iterations总迭代次数,learning_rate学习率    Returns:    params:更新后的参数字典    grads:梯度    costs:损失结果    """</span>    costs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#每一次循环多少次，步数的迭代，计算出每一次更新的我w，b</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 梯度更新计算函数</span>        grads<span class="token punctuation">,</span> cost <span class="token operator">=</span> propagate<span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 取出两个部分参数的梯度</span>        dw <span class="token operator">=</span> grads<span class="token punctuation">[</span><span class="token string">'dw'</span><span class="token punctuation">]</span>        db <span class="token operator">=</span> grads<span class="token punctuation">[</span><span class="token string">'db'</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># 按照梯度下降公式去计算</span>        w <span class="token operator">=</span> w <span class="token operator">-</span> learning_rate <span class="token operator">*</span> dw        b <span class="token operator">=</span> b <span class="token operator">-</span> learning_rate <span class="token operator">*</span> db        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            costs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cost<span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"损失结果 %i: %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>i<span class="token punctuation">,</span> cost<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span>    params <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"w": w,</span>              <span class="token string">"b"</span><span class="token punctuation">:</span> b<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span>    grads <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"dw": dw,</span>             <span class="token string">"db"</span><span class="token punctuation">:</span> db<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span><span class="token comment" spellcheck="true">#更新出来的参数结果</span>    <span class="token keyword">return</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">,</span> costs<span class="token keyword">def</span> <span class="token function">propagate</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#反向传播的过程</span>    <span class="token triple-quoted-string string">"""    参数：w,b,X,Y：网络参数和数据    Return:    损失cost、参数W的梯度dw、参数b的梯度db    """</span>    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 前向传播，计算</span>    <span class="token comment" spellcheck="true"># w (n,1), x (n, m)</span>    A <span class="token operator">=</span> basic_sigmoid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">.</span>T<span class="token punctuation">,</span> X<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 计算损失</span>    cost <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>Y <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>A<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> Y<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> A<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 反向传播</span>    dz <span class="token operator">=</span> A <span class="token operator">-</span> Y    dw <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dz<span class="token punctuation">.</span>T<span class="token punctuation">)</span>    db <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> m <span class="token operator">*</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dz<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这一次更新的梯度</span>    grads <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"dw": dw,</span>             <span class="token string">"db"</span><span class="token punctuation">:</span> db<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span>    <span class="token keyword">return</span> grads<span class="token punctuation">,</span> cost预测函数为：<span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> b<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    利用训练好的参数预测    return：预测结果    '''</span>    m <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    Y_prediction <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">)</span>    w <span class="token operator">=</span> w<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 计算结果</span>    A <span class="token operator">=</span> basic_sigmoid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">.</span>T<span class="token punctuation">,</span> X<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> A<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> <span class="token number">0.5</span><span class="token punctuation">:</span>            Y_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            Y_prediction<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>    <span class="token keyword">assert</span> <span class="token punctuation">(</span>Y_prediction<span class="token punctuation">.</span>shape <span class="token operator">==</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> Y_prediction运行结果显示：训练集的样本数<span class="token punctuation">:</span>  <span class="token number">209</span>测试集的样本数<span class="token punctuation">:</span>  <span class="token number">50</span>train_x形状<span class="token punctuation">:</span>  <span class="token punctuation">(</span><span class="token number">209</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>train_y形状<span class="token punctuation">:</span>  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">209</span><span class="token punctuation">)</span>test_x形状<span class="token punctuation">:</span>  <span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>test_x形状<span class="token punctuation">:</span>  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span>损失结果 <span class="token number">0</span><span class="token punctuation">:</span> <span class="token number">0.693147</span><span class="token operator">-</span><span class="token number">0.000777511961722488</span>损失结果 <span class="token number">100</span><span class="token punctuation">:</span> <span class="token number">0.584508</span><span class="token operator">-</span><span class="token number">0.004382762341768198</span>损失结果 <span class="token number">200</span><span class="token punctuation">:</span> <span class="token number">0.466949</span><span class="token operator">-</span><span class="token number">0.006796745374030192</span>损失结果 <span class="token number">300</span><span class="token punctuation">:</span> <span class="token number">0.376007</span><span class="token operator">-</span><span class="token number">0.008966216045043067</span>损失结果 <span class="token number">400</span><span class="token punctuation">:</span> <span class="token number">0.331463</span><span class="token operator">-</span><span class="token number">0.010796335272035083</span>损失结果 <span class="token number">500</span><span class="token punctuation">:</span> <span class="token number">0.303273</span><span class="token operator">-</span><span class="token number">0.012282447313396519</span>损失结果 <span class="token number">600</span><span class="token punctuation">:</span> <span class="token number">0.279880</span><span class="token operator">-</span><span class="token number">0.013402386273819053</span>损失结果 <span class="token number">700</span><span class="token punctuation">:</span> <span class="token number">0.260042</span><span class="token operator">-</span><span class="token number">0.014245091216970799</span>损失结果 <span class="token number">800</span><span class="token punctuation">:</span> <span class="token number">0.242941</span><span class="token operator">-</span><span class="token number">0.014875420165524832</span>损失结果 <span class="token number">900</span><span class="token punctuation">:</span> <span class="token number">0.228004</span><span class="token operator">-</span><span class="token number">0.015341288386626626</span>损失结果 <span class="token number">1000</span><span class="token punctuation">:</span> <span class="token number">0.214820</span><span class="token operator">-</span><span class="token number">0.015678788375442378</span>损失结果 <span class="token number">1100</span><span class="token punctuation">:</span> <span class="token number">0.203078</span><span class="token operator">-</span><span class="token number">0.015915536343924556</span>损失结果 <span class="token number">1200</span><span class="token punctuation">:</span> <span class="token number">0.192544</span><span class="token operator">-</span><span class="token number">0.01607292624287493</span>损失结果 <span class="token number">1300</span><span class="token punctuation">:</span> <span class="token number">0.183033</span><span class="token operator">-</span><span class="token number">0.016167692508505707</span>损失结果 <span class="token number">1400</span><span class="token punctuation">:</span> <span class="token number">0.174399</span><span class="token operator">-</span><span class="token number">0.016213022073676534</span>损失结果 <span class="token number">1500</span><span class="token punctuation">:</span> <span class="token number">0.166521</span><span class="token operator">-</span><span class="token number">0.016219364232163875</span>损失结果 <span class="token number">1600</span><span class="token punctuation">:</span> <span class="token number">0.159305</span><span class="token operator">-</span><span class="token number">0.01619503271238927</span>损失结果 <span class="token number">1700</span><span class="token punctuation">:</span> <span class="token number">0.152667</span><span class="token operator">-</span><span class="token number">0.016146661324349904</span>损失结果 <span class="token number">1800</span><span class="token punctuation">:</span> <span class="token number">0.146542</span><span class="token operator">-</span><span class="token number">0.01607955397736277</span>损失结果 <span class="token number">1900</span><span class="token punctuation">:</span> <span class="token number">0.140872</span><span class="token operator">-</span><span class="token number">0.015997956805040348</span>训练集准确率<span class="token punctuation">:</span> <span class="token number">99.04306220095694</span> 测试集准确率<span class="token punctuation">:</span> <span class="token number">70.0</span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;读取猫狗的图片矩阵数据，自实现神经元神经网络进行二分类&quot;&gt;&lt;a href=&quot;#读取猫狗的图片矩阵数据，自实现神经元神经网络进行二分类&quot; class=&quot;headerlink&quot; title=&quot;读取猫狗的图片矩阵数据，自实现神经元神经网络进行二分类&quot;&gt;&lt;/a&gt;读取猫狗的</summary>
      
    
    
    
    <category term="神经网络案例" scheme="https://xiaoyvlongoing.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A1%88%E4%BE%8B/"/>
    
    
    <category term="猫狗分类" scheme="https://xiaoyvlongoing.github.io/tags/%E7%8C%AB%E7%8B%97%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>pytorch架构基础</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/pytorch%E5%9F%BA%E7%A1%80/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/pytorch%E5%9F%BA%E7%A1%80/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-06-27T16:48:57.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pytorch架构基础"><a href="#pytorch架构基础" class="headerlink" title="pytorch架构基础"></a>pytorch架构基础</h2><h4 id="创建固定，"><a href="#创建固定，" class="headerlink" title="创建固定，"></a>创建固定，</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>torch.tensor([],[])</td><td>创建固定张量      类型为torch.float32类型</td></tr><tr><td>toch.tensor(np.array([]))</td><td>用np创建张量</td></tr><tr><td></td><td></td></tr><tr><td><strong>张量进行拼接</strong></td><td></td></tr><tr><td>torch.cat((变量张量1,变量张量2, 变量张量3), 0)</td><td>张量进行拼接：                                                                                             张量拼接，                                                                                                                    0：代表竖列拼接，1代表横向拼接</td></tr><tr><td></td><td></td></tr><tr><td>numpy.mgrid（）</td><td>回一个密集的多维“ meshgrid”，以便每个返回的参数具有相同的形状。  输出数组的尺寸和数量等于索引尺寸的数量。  如果步长不是复数，则停止不包括在内</td></tr><tr><td></td><td></td></tr><tr><td>np.<strong>dstack</strong>()</td><td>沿深度方向（沿第三轴）按顺序堆叠数组。                                                                              阵列的序列: 除第三个轴外，所有阵列的形状都必须相同。  一维或二维阵列必须具有相同的形状。</td></tr><tr><td><strong>创建64的类型</strong></td><td></td></tr><tr><td>torch.set_default_tensor_type(torch.DoubleTensor)</td><td>创建64的类型</td></tr><tr><td></td><td></td></tr><tr><td><strong>恢复32的类型</strong></td><td></td></tr><tr><td>torch.set_default_tensor_type(torch.FloatTensor)</td><td>创建32的类型</td></tr><tr><td></td><td></td></tr></tbody></table><pre class=" language-python"><code class="language-python">np<span class="token punctuation">.</span>mgrid<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>mgrid<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">5j</span><span class="token punctuation">]</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span> <span class="token punctuation">,</span>  <span class="token number">0.5</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>dstack<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>a <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>dstack<span class="token punctuation">(</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h4 id="np-vstack-和np-hstack"><a href="#np-vstack-和np-hstack" class="headerlink" title="np.vstack()和np.hstack()"></a>np.vstack()和np.hstack()</h4><p>在这里我们介绍两个拼接数组的方法：</p><p>np.vstack():在竖直方向上堆叠</p><p>np.hstack():在水平方向上平铺</p><pre><code>import numpy as nparr1=np.array([1,2,3])arr2=np.array([4,5,6])print np.vstack((arr1,arr2)) print np.hstack((arr1,arr2)) a1=np.array([[1,2],[3,4],[5,6]])a2=np.array([[7,8],[9,10],[11,12]])print a1print a2print np.hstack((a1,a2))</code></pre><p>结果如下：</p><pre><code>[[1 2 3] [4 5 6]][1 2 3 4 5 6][[1 2] [3 4] [5 6]][[ 7  8] [ 9 10] [11 12]][[ 1  2  7  8] [ 3  4  9 10] [ 5  6 11 12]]</code></pre><h4 id="数组创建"><a href="#数组创建" class="headerlink" title="数组创建"></a>数组创建</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>numpy.<strong>eye</strong>(<em>N</em>, <em>M=None</em>, <em>k=0</em>, <em>dtype=&lt;class ‘float’&gt;</em>, <em>order=’C’</em>, ***, <em>like=None</em>)</td><td><strong>参数                                                                                                                                                                             N：</strong>int：输出中的行数                                                                                                                                 <strong>M</strong>：int：输出中的列数。 如果没有，默认为 <em>N</em> 。                                                                                                   <strong>k</strong>：int ：对角线的索引：0（默认）是指主对角线，正值表示上对角线，负值表示到较低的对角线。</td></tr><tr><td><strong>返回</strong>：</td><td>返回一个二维数组，对角线上为 1，其他地方为 0。</td></tr></tbody></table><pre class=" language-python"><code class="language-python">np<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>int<span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>int<span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>torch.LongTensor（）</td><td>构建一个多少*多少的 <code>Long</code>类型的张量，</td></tr><tr><td>Variable(torch.LongTensor([2, 1, 0]))</td><td>加上Variable后可以表示标签</td></tr><tr><td></td><td></td></tr><tr><td>torch.Variable（）</td><td>int变量，required_grad=True是会更新参数，默认为False它是一种可以变化的变量，这正好就符合了反向传播，参数更新的属性。<code>Variable API</code> 几乎和 <code>Tensor API</code>一致 (除了一些<code>in-place</code>方法，这些<code>in-place</code>方法会修改 <code>required_grad=True</code>的 <code>input</code> 的值)。多数情况下，将<code>Tensor</code>替换为<code>Variable</code>，代码一样会正常的工作。由于这个原因，我们不会列出<code>Variable</code>的所有方法，你可以通过<code>torch.Tensor</code>的文档来获取相关知识。</td></tr><tr><td></td><td></td></tr><tr><td>.long()</td><td>形成torch.int64</td></tr><tr><td>.int()</td><td>形成torch.32</td></tr><tr><td>.float()</td><td>形成float32</td></tr><tr><td></td><td></td></tr><tr><td><strong>获取默认的数据类型</strong></td><td></td></tr><tr><td>torch.get_default_dtype()</td><td>默认的数据类型</td></tr><tr><td></td><td></td></tr><tr><td><strong>扩展数组</strong></td><td></td></tr><tr><td>np.expand_dims(<em>a</em>, <em>axis</em>)</td><td><strong>参数</strong>                                                                                                                          a：输入数组。                                                                                                       axis ：在第几维度增加一维，axis=0，就在第1维增加，axis=-1就在最后一维增加，可以为（第几维，第几维）一次增加2个</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h4><table><thead><tr><th>函数</th><th>函数性质.</th></tr></thead><tbody><tr><td>.shape（）</td><td>张量的维度</td></tr><tr><td>.size()</td><td>得到张量的尺寸，几行几列</td></tr><tr><td>.dim()</td><td>得到是张量的阶数</td></tr><tr><td>.view(行，列)</td><td>不改变内存里面的顺序的，改变形状，如2行,3列，可以转换为3行，2列</td></tr><tr><td>torch.rand（行，列）</td><td>创建一个<strong>有初始化</strong>的矩阵，</td></tr><tr><td>torch.empty(行，列)</td><td>创建一个<strong>没有初始化</strong>的矩阵，<strong>float32</strong></td></tr><tr><td>torch.tensor（[数据1，数据2]）</td><td>直接通过数据创建张量</td></tr><tr><td>变量.new_ones（行，列，dtype=torch.double）</td><td>通过已有的一个张量创建相同尺寸的新张量，<strong>float64</strong></td></tr><tr><td>torch.randn_like(变量,dtype = torch.float)</td><td>利用randn_like方法得到相同张量尺寸的一个新张量, 并且采用随机初始化来对其赋值，<strong>float32</strong></td></tr><tr><td>torch.ones()</td><td>创建全为1的张量</td></tr><tr><td>torch.zeros（行，列，dtype=torch.long）</td><td>创建一个全0矩阵并可指定数据元素的类型为long，<strong>int64</strong></td></tr><tr><td>torch.long</td><td>将此存储空间转换为 长 型</td></tr><tr><td></td><td></td></tr><tr><td><strong>创建一定范围的序列张量</strong></td><td></td></tr><tr><td>torch.<strong>ones_like</strong>(tensor)</td><td></td></tr><tr><td>torch.arange（起始，末尾，步长）</td><td>创建以步长等差，为开始结束的张量</td></tr><tr><td>torch.logspace()</td><td>等比数列</td></tr><tr><td>torch.rand(行，列)</td><td>创建以范围（0到1）的区间的随机张量</td></tr><tr><td>torch.randint（范围1，范围2，（行，列））</td><td>在这范围的进行随机生成几行几列</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="pytorch中tensor的属性，"><a href="#pytorch中tensor的属性，" class="headerlink" title="pytorch中tensor的属性，"></a>pytorch中tensor的属性，</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>torch.tensor(np.array(1))</td><td>创建为tensor(1, dtype=torch.int32)</td></tr><tr><td>.item()</td><td>把上面的tensor(1, dtype=torch.int32)创建的数值提取出来</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="类型转换-1"><a href="#类型转换-1" class="headerlink" title="类型转换"></a>类型转换</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>numpy.ascontiguousarray( <em>a</em> , <em>dtype=None</em> , *** , <em>like=None</em> )</td><td>参数</td></tr><tr><td><strong>返回：</strong>内存中的连续数组（ndim &gt;= 1）（C 顺序）。</td><td><strong>一个</strong> array_like 输入数组。   <strong>dtype</strong> str 或 dtype 对象，可选 返回数组的数据类型。   <strong>像</strong> array_like 允许创建非数组的引用对象  NumPy 数组。  如果一个类似数组的传入作为 <code>like</code>支持  这 <code>__array_function__</code>协议，结果将被定义  通过它。  在这种情况下，它确保创建一个数组对象  与通过此参数传入的兼容。</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="pytorch基本的运算操作"><a href="#pytorch基本的运算操作" class="headerlink" title="pytorch基本的运算操作"></a>pytorch基本的运算操作</h3><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>torch.add（变量1，变量2，alpha=10）先用alpha数值去乘于变量2在变量1相加</td><td>加法操作</td></tr><tr><td>变量2.add_(变量1)</td><td>加法方式:把和的总数给了变量2</td></tr><tr><td>torch.add(变量1, 变量2, out=空张量)</td><td>将空的张量作为加法的结果存储张量</td></tr><tr><td>torch.randn（【行，列】）</td><td>创建均值为0方差为1的随机数列（高斯分布）</td></tr><tr><td>变量2.sub(变量1)</td><td>进行相减</td></tr><tr><td>变量1.mm(变量2)</td><td>进行矩阵相乘，矩阵相乘法则，</td></tr><tr><td>变量1.max()</td><td>找到最大的值</td></tr><tr><td>变量.max(dim=可以写0,1,2)</td><td>在那个维度上上找到最大的值</td></tr><tr><td></td><td></td></tr><tr><td>特殊张量运算</td><td></td></tr><tr><td>torch.bmm（变量1张量3维，变量2张量3维）</td><td>进后两维进行运算</td></tr><tr><td># 如果参数1形状是(b × n × m), 参数2形状是(b × m × p), 则输出为(b × n × p) &gt;&gt;&gt; input = torch.randn(10, 3, 4) &gt;&gt;&gt; mat2 = torch.randn(10, 4, 5) &gt;&gt;&gt; res = torch.bmm(input, mat2) &gt;&gt;&gt; res.size() torch.Size([10, 3, 5])</td><td></td></tr></tbody></table><h4 id="pytorch改变性状-转置，正加（填充）维度，减少（压缩）维度"><a href="#pytorch改变性状-转置，正加（填充）维度，减少（压缩）维度" class="headerlink" title="pytorch改变性状,转置，正加（填充）维度，减少（压缩）维度"></a>pytorch改变性状,转置，正加（填充）维度，减少（压缩）维度</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>.view(行，列)</td><td>不改变内存里面的顺序的，改变形状，如2行,3列，可以转换为3行，2列</td></tr><tr><td>变量1.view(-1, 一行多少个)</td><td>-1代表自动匹配个数，改变行列</td></tr><tr><td>.t()</td><td>进制转置</td></tr><tr><td>.transpose(第几维度,第几维度)</td><td>维度颠倒，内存也进行的乱序，第几维度与另一个设置的第几维度可以进行转置</td></tr><tr><td>.transpose</td><td>经过这个来进行转置的时候还原原来的维度需要进行.contiguous()函数的操作</td></tr><tr><td>.permute(维度几,维度几,维度几)</td><td>可以让多个维度（不只是3维是多维）进行互换转置</td></tr><tr><td>.squeeze(数值)</td><td>压缩维度，可以添加数值，压缩第数值上的维度，</td></tr><tr><td>.unsqueeze(数值)</td><td>增加维度，增添数值的维度</td></tr><tr><td>.reshape(几行,几列)</td><td>改变形状</td></tr><tr><td>.sum()</td><td>求和</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="方向翻转"><a href="#方向翻转" class="headerlink" title="方向翻转"></a>方向翻转</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>numpy.<strong>fliplr</strong>( <em>m</em> )</td><td>参数：<strong>m</strong> 数组类似 输入数组，必须至少是二维的。</td></tr><tr><td>返回：例子</td><td><strong>原来数组</strong>：A = [[1.,  0.,  0.],        [0.,  2.,  0.],        [0.,  0.,  3.]                                                                                         np.<strong>fliplr</strong>(A)                                                                                                                                                <strong>打印</strong>：array([[0.,  0.,  1.],        [0.,  2.,  0.],        [3.,  0.,  0.]])</td></tr></tbody></table><pre class=" language-python"><code class="language-python">A <span class="token operator">=</span> np<span class="token punctuation">.</span>diag<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Aarray<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>fliplr<span class="token punctuation">(</span>A<span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="关于Torch-Tensor和Numpy-array之间的相互转换"><a href="#关于Torch-Tensor和Numpy-array之间的相互转换" class="headerlink" title="关于Torch Tensor和Numpy array之间的相互转换"></a>关于Torch Tensor和Numpy array之间的相互转换</h3><p>Torch Tensor和Numpy array共享底层的内存空间, 因此改变其中一个的值, 另一个也会随之被改变.</p><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>torch.from_numpy(变量1)</td><td>将Numpy array转换为Torch Tensor</td></tr><tr><td>b = 变量1.numpy()</td><td>将Torch Tensor转换为Numpy array</td></tr><tr><td>【：，取数值竖列的数字】</td><td></td></tr><tr><td>tensor.detach().numpy()</td><td>require_grad=True不能够直接转换，调用，tensor.detach().numpy()才能进行转换</td></tr><tr><td></td><td></td></tr><tr><td>torch.<strong>FloatTensor</strong>()</td><td>类型转换, 将list ,numpy转化为tensor。 以list -&gt; tensor为例</td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="autograd-package-自动求导工具包"><a href="#autograd-package-自动求导工具包" class="headerlink" title="autograd package(自动求导工具包)"></a>autograd package(自动求导工具包)</h4><ul><li>autograd package提供了一个对Tensors上所有的操作进行自动微分的功能.</li><li></li></ul><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td><strong>torch.Tensor</strong></td><td><strong>package中的核心类</strong></td></tr><tr><td>.grad</td><td>在这个Tensor上的所有梯度将被累加</td></tr><tr><td>.detach()</td><td>可以将该Tensor从计算图中撤下, 在未来的回溯计算中也不会再计算该Tensor.</td></tr><tr><td>with torch.no_grad()</td><td>非常适用于对模型进行预测的时候, 因为预测阶段不再需要对梯度进行计算.</td></tr><tr><td><strong>torch.Function</strong></td><td><strong>和Tensor类同等重要的一个核心类</strong></td></tr><tr><td></td><td></td></tr><tr><td>torch.rand([行，列])</td><td>定义数据</td></tr><tr><td>tensor.detach().numpy()</td><td>require_grad=True不能够直接转换，调用，tensor.detach().numpy()</td></tr><tr><td>(变量,requires_grad=True)</td><td>加上这个来运算，后面对它进行加减乘除的时候都可以来执行自动求导，与后面的都进行了关联，将追踪在这个类上定义的所有操作</td></tr><tr><td>.requires_grad_()</td><td>该方法可以原地改变Tensor的属性.requires_grad的值. 如果没有主动设定默认为False.</td></tr><tr><td>变量1.requires_grad_=True</td><td>这个也可以进行就地加上，等于True的时候加入了求导的运行</td></tr><tr><td>.grad_fn</td><td>每次计算都会修改其grad_fn属性，用来记录做过的操作</td></tr><tr><td>optim.<strong>SGD</strong>(model.parameters(), lr=1e-3)</td><td>实现：SGD优化参数</td></tr><tr><td>nn.<strong>MSELoss</strong>()</td><td>损失函数</td></tr><tr><td><strong>net.zero_grad()</strong></td><td>Pytorch中执行梯度清零的代码</td></tr><tr><td><strong>loss</strong>.<strong>backward</strong>()：反向传播的时候要先进行上一步的操作梯度清零：net.zero_grad()，所有的属性都为：requires_grad=True时就参与梯度求导的运算</td><td>计算图可以通过链式法则求导。计算梯度，反向传播实现的.</td></tr><tr><td>optimizer.step（）</td><td>进行单次优化 (参数更新).</td></tr><tr><td>with torch.no_gard():</td><td>不进行求导的这个是在测试集上进行</td></tr><tr><td>.data()：直接提取里面的数字（浅拷贝）</td><td>在tensor的require_grad=False，tensor.data和tensor等价 require_grad=True时，tensor.data仅仅是获取tensor中的数据</td></tr><tr><td>net.<strong>parameters</strong>()</td><td>模型中所有的可训练参数, 可以通过net.parameters()来获得.equire_grad=True时的参数取出</td></tr><tr><td><strong>在训练和评估模型中进行写入</strong></td><td></td></tr><tr><td>model.train()：在训练模块中进行写入</td><td>启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True</td></tr><tr><td>model.eval()：在评估（或测试）模块进行写入</td><td>不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False，在评估测试中进行使用</td></tr><tr><td>x.data.numpy()：</td><td>直接进行提取数值在进行转换成np类型，可以进行绘图</td></tr><tr><td>.datach</td><td>直接提取里面的数字（深拷贝）</td></tr><tr><td><strong>创建模型</strong></td><td></td></tr><tr><td>import torch.nn as nn</td><td>创建的神经网络的包</td></tr><tr><td>import torch.nn.<strong>functional</strong> as F</td><td>创建</td></tr><tr><td>nn.Conv2d（）</td><td>创建一个卷积</td></tr><tr><td>F.max_pool2d(F.relu(self.conv2(x)), 2)</td><td>创建最大池化</td></tr><tr><td>nn.Linear（<code>in_features</code>，<code>out_features</code>）</td><td><strong>可以进行定义对象，在括号内进行输入变量： **  <strong>in_features</strong>：指的是输入的二维张量的大小，即</strong>输入的**<code>[batch_size, size]</code>中的size  <strong>out_features</strong>:指的是输出的二维张量的大小，即<strong>输出的</strong>二维张量的形状为<code>[batch_size，output_size]</code>，当然，它也代表了<strong>该全连接层的神经元个数</strong>。</td></tr><tr><td><strong>nn.Sequential</strong>（nn.Linear,(),nn.Linear()）</td><td>构建模型可以创建多个全连接层可以自动完成forward函数来创建</td></tr><tr><td></td><td></td></tr><tr><td><strong>创建优化器</strong></td><td><strong>torch.optim</strong>函数下的：</td></tr><tr><td>torch.optim.SGD(参数，学习率)</td><td>以SGD优化器执行</td></tr><tr><td>torch.optim.Adam(参数，学习率)</td><td>以Adam优化器来执行</td></tr><tr><td></td><td></td></tr><tr><td>nn.<strong>functional</strong>.avg_pool1d</td><td>在由多个输入平面组成的输入信号上应用一维平均池。</td></tr><tr><td>`torch.nn.functional.<strong>avg_pool1d</strong>（<em>输入</em>，<em>kernel_size</em>，<em>stride = None</em>，<em>padding = 0</em>，<em>ceil_mode = False</em>，<em>count_include_pad = True</em> ）</td><td><strong>输入</strong>–输入张量的形状（\ text {minibatch}，\ text {in \ _channels}，iW）（小批量，in_channels，<em>我**W</em> ） <strong>kernel_size</strong> –窗口的大小。可以是单个数字或元组（kW，） <strong>步幅</strong>–窗口的步幅。可以是单个数字或元组 （sW，）。默认：<code>kernel_size</code> <strong>padding</strong> –输入两侧的隐式零填充。可以是一个数字或一个元组（padW，）。默认值：0 <strong>ceil_mode</strong> –为True时，将使用ceil而不是floor来计算输出形状。默认：<code>False</code> <strong>count_include_pad</strong> –为True时，将在平均计算中包括零填充。默认：<code>True</code></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h4><table><thead><tr><th>函数</th><th>函数性质</th><th>函数解释</th></tr></thead><tbody><tr><td><code>state_dict</code>（ ）</td><td>将缩放器的状态返回为  <a href="https://docs.python.org/3/library/stdtypes.html#dict"><code>dict</code></a>。  它包含五个条目：                                             <code>&quot;scale&quot;</code> -包含当前比例的Python浮点数                                    <code>&quot;growth_factor&quot;</code> -包含当前增长因子的Python浮点数                               <code>&quot;backoff_factor&quot;</code> -包含当前退避因子的Python浮点数                                                                          <code>&quot;growth_interval&quot;</code> -包含当前增长间隔的Python int                                                                <code>&quot;_growth_tracker&quot;</code> -包含最近连续未跳过步骤数的Python int。                                                                                                                            如果未启用此实例，则返回一个空字典。</td><td>如果您希望在特定迭代后检查定标器的状态，  <a href="https://pytorch.org/docs/stable/amp.html?highlight=state_dict#torch.cuda.amp.GradScaler.state_dict"><code>state_dict()</code></a><br/>应该在之后  <a href="https://pytorch.org/docs/stable/amp.html?highlight=state_dict#torch.cuda.amp.GradScaler.update"><code>update()</code></a>.</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 首先设定模型的保存路径</span>PATH <span class="token operator">=</span> <span class="token string">'./cifar_net.pth'</span><span class="token comment" spellcheck="true"># 保存模型的状态字典</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>模型名字<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span></code></pre><h4 id="优化器的创建"><a href="#优化器的创建" class="headerlink" title="优化器的创建"></a>优化器的创建</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 首先导入优化器的包, optim中包含若干常用的优化算法, 比如SGD, Adam等</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token comment" spellcheck="true"># 通过optim创建优化器对象</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将优化器执行梯度清零的操作</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>output <span class="token operator">=</span> net<span class="token punctuation">(</span>input<span class="token punctuation">)</span>loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 对损失值执行反向传播的操作</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 参数的更新通过一行标准代码来执行</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h4 id="损失函数系列"><a href="#损失函数系列" class="headerlink" title="损失函数系列"></a>损失函数系列</h4><table><thead><tr><th>函数</th><th>函数性质</th></tr></thead><tbody><tr><td>zero_grad()</td><td>把梯度置零，也就是把loss关于weight的导数变成0，就是每一批次进行清零操作</td></tr><tr><td>nn.MSELoss</td><td>均方误差：常用于回归问题</td></tr><tr><td>nn.CrassEntropyLoss(，预测值，真实值)</td><td>交叉熵损失：分类问题，</td></tr><tr><td></td><td></td></tr><tr><td>class torch.nn.Parameter()</td><td>模型中所有的可训练参数,</td></tr><tr><td>view(1, -1)</td><td>view()函数作用是将一个多行的Tensor,拼接成一行。返回一个有相同数据但大小不同的tensor。返回的tensor必须有与原tensor相同的数据和相同数目的元素，但可以有不同的大小。一个tensor必须是连续的<code>contiguous()</code>才能被查看。</td></tr></tbody></table><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<span class="token comment" spellcheck="true"># 1. 定义数据</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> x<span class="token operator">*</span><span class="token number">3</span> <span class="token operator">+</span> <span class="token number">0.8</span><span class="token comment" spellcheck="true">#2 .定义模型</span><span class="token keyword">class</span> <span class="token class-name">Lr</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Lr<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span>）<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    out <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">return</span> out                            <span class="token comment" spellcheck="true"># 2. 实例化模型，损失loss，和优化器，</span>model <span class="token operator">=</span> Lr<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>里面的参数都进行固定不动<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#3. 训练模型</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">30000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    out <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#3.1 获取预测值</span>    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y<span class="token punctuation">,</span>out<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#3.2 计算损失</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#3.3 梯度归零</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#3.4 计算梯度</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 3.5 更新梯度</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">20</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch[&amp;#123;&amp;#125;/&amp;#123;&amp;#125;], loss: &amp;#123;:.6f&amp;#125;'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span><span class="token number">30000</span><span class="token punctuation">,</span>loss<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#4. 模型评估</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#设置模型为评估模式，即预测模式，将model.parameters()里面的参数都进行固定不动，预测的时候固定不变，才能预测，就把requires_grad=False了</span>predict <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#把x进去得到我们的预测</span>predict <span class="token operator">=</span> predict<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#真实的值拿出来</span>plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>c<span class="token operator">=</span><span class="token string">"r"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>predict<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;pytorch架构基础&quot;&gt;&lt;a href=&quot;#pytorch架构基础&quot; class=&quot;headerlink&quot; title=&quot;pytorch架构基础&quot;&gt;&lt;/a&gt;pytorch架构基础&lt;/h2&gt;&lt;h4 id=&quot;创建固定，&quot;&gt;&lt;a href=&quot;#创建固定，&quot; class</summary>
      
    
    
    
    <category term="pytorch" scheme="https://xiaoyvlongoing.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="https://xiaoyvlongoing.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>机器学习，神经网络 -- 上</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA--%E4%B8%8A/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA--%E4%B8%8A/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-06-24T17:45:35.512Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h2><p>——-先观察后照做<br> ——-机器学习算法都试图以某种它可以的方式来模拟两个数据之间存在的模式，从而做到用一个数据集来预测另一个数据集<br> ——-机器学习的大部分工作都是训练某种监督分类器</p><ul><li>机器学习：<br> 机器学习是计算机科学的的一个分支，机器学习观察某个任务中存在的模式，并试图以某种直接或者间接的方式模仿它。</li></ul><h2 id="机器学习理论基础"><a href="#机器学习理论基础" class="headerlink" title="机器学习理论基础"></a>机器学习理论基础</h2><p><strong>机器学习工作流程的总结</strong></p><ul><li><p>获取数据集：<br> 要有多少的样本每个样本的特征，根据特征再进行归类成相应的目标值</p><pre><code> 1：数据要有代表性，否者会过拟合，（数据集尽量差异大，提取会更好）。 2：对数据的分类问题，数据偏斜不能过严重，不同类别的数据数量不要有数量级的差距。 3：对数据集的量级做一个评估，多少个样本，多少个特征，可以估算出内存的消耗，     太大放不下就要考虑改进算法和做一些降维的技巧，如果数据集太大，就要考虑做分布式的处理方式。</code></pre></li><li><p>数据集的基本处理：</p><pre><code> 1：数据的清洗很关键的一步，能够使算法效果和性能显著的提高：     如归一化，离散化，因子化，缺失值的处理，去除共线性等，数据的挖掘过程就花在他们上面。 2：数据的增强操作：翻转。</code></pre></li><li><p>特征工程的提取：</p><pre><code> 1：良好的数据能过提取良好的特征才能发挥作用。 2：筛选出显著的特征，放弃非显著的特征，非常简单的算法也能有不错的结果。 3：需要运用有效性的分析相关的技术，相关的系数，卡方检验，平均互信息，条件熵，后验概率，逻辑回归权重等方法。</code></pre></li><li><p>机器学习（构建模型训练）：<br> 训练的数据（70%-80%）测试数据集（20%-30%）</p><pre><code> 1：怎样才能把这些算法的（超参数）使得变得更加优良，理解越深才能更有效的提出优化的方案。</code></pre></li><li><p>模型的评估：</p><pre><code> 没有达到要求，从新上面的步骤 达到一定的要求，上线服务</code></pre></li></ul><h2 id="机器学习的算法分类"><a href="#机器学习的算法分类" class="headerlink" title="机器学习的算法分类"></a>机器学习的算法分类</h2><p><strong>两种机器学习的主要类型：</strong><br> 数据集的组成不同，算法就有所不同</p><pre><code>    监督和无监督主要关注被学习的模式的类型    无监督学习和监督学习都是将一个数据集转化为另一个数据集</code></pre><ul><li><p>监督学习：你知道的—–》监督学习—-》你想知道的</p></li><li><p>监督机器学习：创建能接收可观察，可记录的，输入数据的方法，他能将转换为所需要再进行逻辑分析的有价值的输出数据。</p><pre><code> 他可以直接模仿两个数据集之间的模式，尝试获取一个输入数据集并将其转换为     对应的输出数据集：如使用像素来检测猫存在与否</code></pre></li><li><p>监督学习对数据集进行转换</p></li><li><p>监督学习是一种将一个数据集转换为另一个数据集的方法</p><pre><code> 1：输入数据是由输入特征和目标值所组成的：      （一）：函数的输出可以是一个连续的值（称为回归）      （二）：或是输出是个有限的离散值（称为分类）</code></pre></li><li><p>无监督学习：一系列的数据点—–》无监督学习—–》一系列的数据聚类标签。</p></li><li><p>将数据集进行聚类就是一种无监督学习： 聚类算法：将一系列数据点转换为对应的一系列类别的标签。</p></li><li><p>无监督学习对数据集进行分组</p><pre><code> 1：输入数据是由输入特征值组成，没有目标值：     （一）：输入数据没有被标记，也没有确定的结果，数据类别未知     （二）：需要根据样本间的相似性对样本进行类别的分类</code></pre></li><li><p>半监督学习</p><pre><code> 1：训练集同时含有标记的数据和未标记的数据样本</code></pre></li><li><p>强化学习：</p><pre><code> 1：实质是make decisions 问题，即自动进行决策，并且可以做练习决策   （一）：五个元素：代理品，行动，奖励，环境，观察情况   （二）：强化学习目标是获取最多的累计奖励</code></pre><h2 id="四种不同类型的算法：监督或者无监督，参数或者无参数"><a href="#四种不同类型的算法：监督或者无监督，参数或者无参数" class="headerlink" title="四种不同类型的算法：监督或者无监督，参数或者无参数"></a>四种不同类型的算法：监督或者无监督，参数或者无参数</h2></li><li><p>监督和无监督的特性是：主要关注被学习的模式的类型</p></li><li><p>参数和非参数类型：主要关注存储学习参数的方式</p><h1 id="参数学习和非参数学习"><a href="#参数学习和非参数学习" class="headerlink" title="参数学习和非参数学习"></a>参数学习和非参数学习</h1></li><li><p>参数学习和非参数学习：试错学习和计数和概率<br> 参数是一种通用的术语，所指的仅是用于识别模式的-组数字–并不影响限制我们我们第一这些数字的正常使用，<br> 技术的参数权重是参数</p><pre><code> 主要关注存储学习参数的方式，更进一步说是关注学习的方法 参数模型的特征是具有固定的数量的参数，而非参数的参数数量是无限的（由数据决定）     1：参数模型倾向于试错法     2：非参数模型倾向于计数法</code></pre><h2 id="监督学习和强化学习的对比"><a href="#监督学习和强化学习的对比" class="headerlink" title="监督学习和强化学习的对比"></a>监督学习和强化学习的对比</h2><pre><code> 反馈映射:          监督学习：输出的是之间的关系，可以告诉算法什么样的输入对应着什么样的输出。          强化学习：输出的是给机器的反馈 reward function，即用来判断这个行为是好是坏。 反馈时间          监督学习：做了比较坏的选择会立刻反馈给算法。          强化学习：结果反馈有延时，有时候可能需要走了很多步以后才知道以前的某一步的选择是好还是坏。 输入特征          监督学习：输入是独立同分布的。          强化学习：面对的输入总是在变化，每当算法做出一个行为，它影响下一次决策的输入，一直在变化的  拓展概念：什么是独立同分布：</code></pre></li></ul><h2 id="监督参数学习：使用旋钮进行试错学习"><a href="#监督参数学习：使用旋钮进行试错学习" class="headerlink" title="监督参数学习：使用旋钮进行试错学习"></a>监督参数学习：使用旋钮进行试错学习</h2><ul><li>机器根据旋钮的角度对输入数据进行处理，并转换为预测结果</li></ul><p>预测—与真值进行比较—-学习模式<br> 通过调节参数来达到最好的预判结果</p><h2 id="无监督参数学习"><a href="#无监督参数学习" class="headerlink" title="无监督参数学习"></a>无监督参数学习</h2><p>无监督参数学习就是就是使用旋钮对数据经行分组，每个类别都设置了几个旋钮，每个旋钮都反映了输入数据到那个特等类别的相似度</p><h2 id="非参数学习：基于计数的方法"><a href="#非参数学习：基于计数的方法" class="headerlink" title="非参数学习：基于计数的方法"></a>非参数学习：基于计数的方法</h2><ul><li>参数算法有固定数量的参数，而非参数算法会根据数据集调整参数个数</li></ul><h2 id="神经网络：它将输入乘以权重，将输入“缩放”一定比例"><a href="#神经网络：它将输入乘以权重，将输入“缩放”一定比例" class="headerlink" title="神经网络：它将输入乘以权重，将输入“缩放”一定比例"></a>神经网络：它将输入乘以权重，将输入“缩放”一定比例</h2><p><strong>数据——-机器——预测</strong><br> <strong>预测——比较——学习—&gt;模式</strong><br> 神经网络做了：</p><pre><code> 将输入乘于权重 将输入缩放一定比例</code></pre><p><strong>神经网络的交互：</strong></p><pre><code>  接受输入变量，作为信息来源，   拥有权重变量，以此作为知识，  然后融入信息和知识，输出预测结果</code></pre><p><strong>理解神经网络权重：</strong></p><pre><code>是将它作为网络的输入和预测之间敏感度的度量，权重非常的高，即使最小的输入也会对预测结果产生非常大的影响，权重非常小时再大的输入也只能对预测产生很小的扰动</code></pre><p><strong>多种输入</strong></p><pre><code>将三个输入或更多输入乘于三项权重，并使他们求和得到加权后的权和</code></pre><p><strong>加权和（点积）</strong></p><pre><code>将每个输入乘于其各自的权重，然后对所有的局部预测结果进行求和则成为输入的加权和，简称点积</code></pre><p><strong>点积</strong></p><pre><code> 点积让你了解两个向量之间的相似性， 最大的加权和：出现在完全相同的向量之间 向量之间没有重合的权重，点积为0 重合的权重值为负数，这个会抵消掉他们之间的正相似性</code></pre><p><strong>点积的属性</strong></p><pre><code>点积的属性类比作逻辑上的AND（与）操作，向量中没有相同的，最终得分为0负权重值往往意味着逻辑上的NOT（非）运算符，因为任何正权值与负权值配对都会导致得分下降，两个为负的就会得正值</code></pre><p>OR（或）如果任何一行的权重不为0，分数就会受到影响</p><p><strong>简单的神经网络</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np存储的维度元素数组weights <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#数值为权重</span><span class="token comment" spellcheck="true">#array:为NP中的存储元组的维度</span><span class="token keyword">def</span> <span class="token function">neural_network</span><span class="token punctuation">(</span>input<span class="token punctuation">,</span>weights<span class="token punctuation">)</span>    pred <span class="token operator">=</span> input<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>weights<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#dot:为Numpy中的乘</span>    returu predtoes <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8.5</span><span class="token punctuation">,</span><span class="token number">9.5</span><span class="token punctuation">,</span><span class="token number">9.9</span><span class="token punctuation">,</span><span class="token number">9.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#数值为输入信息数据</span>wlrec <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.65</span><span class="token punctuation">,</span><span class="token number">0.8</span><span class="token punctuation">,</span><span class="token number">0.8</span><span class="token punctuation">,</span><span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">)</span>nfans <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.2</span><span class="token punctuation">,</span><span class="token number">1.3</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>input <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>toes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>wlerc<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>nfans<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>pred <span class="token operator">=</span> neural_network<span class="token punctuation">(</span>input<span class="token punctuation">,</span>weights<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>pred<span class="token punctuation">)</span></code></pre><h2 id="误差"><a href="#误差" class="headerlink" title="误差"></a>误差</h2><p>预测出来的结果与实际中的结果存在着多少的误差</p><ul><li>均方误差：是评估神经网络准确性的众多方法之一</li><li>比较是在逻辑上的输出是“冷/热”这一类信号</li><li>提升权重：up_prediction = input*(weight+step_amount)#step_amount=0.001(增加权重值)</li></ul><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><ul><li><strong>学习</strong>：就是告诉权重应该如何改变以及降低误差，深度学习最流行的方法为：梯度下降，在预测步骤结束时：学习这一步会为每项权重计算一个数字，这个数字告诉我们，减少误差，权重应该向那个方向变化，然后，我们根据这个数字对权重做出相应的调节，直到达到目的。</li><li><strong>学习</strong>：学习就是调整权重，将误差减小到0</li></ul><h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><p>这一步会让你知道自己的模型错了多少</p><ul><li>比较：你是的神经网络是否做好的预测</li></ul><h2 id="测量误差"><a href="#测量误差" class="headerlink" title="测量误差"></a>测量误差</h2><ul><li>训练神经网络的目的是做出正确的预测</li><li>纯误差：（（权重<em>输入数据）-  真实数据） -均方误差：（（权重</em>输入数据）-  真实数据）**2 将误差进行平方，进负数的误差进行正值，和将更大的误差更大，更小的误差更小（小于1的数更小，大于1的数更大）</li><li>冷热学习：就是通过扰动权重从新确定向那个方向调整可以使得误差的降低幅度最大，基于此将权重值向那个方向移动<br> 权重实际进行调节取决于那个方向所得到的误差更小</li><li>执行缩放，负值翻转和停止调节对纯误差进行修正以及更新权重（（权重*输入）- 真实数据）× 输入数据</li><li>缩放，停止，负值反转：三个属性作用进纯误差转换为我们需要的权重调节的绝对幅度</li></ul><pre class=" language-python"><code class="language-python">weight <span class="token operator">=</span> <span class="token number">0.5</span><span class="token comment" spellcheck="true">#权重</span>input <span class="token operator">=</span> <span class="token number">0.5</span><span class="token comment" spellcheck="true">#数据</span>goal_prediction <span class="token operator">=</span> <span class="token number">0.8</span><span class="token comment" spellcheck="true">#真实的值（goal_pred:真值）</span>step_amount <span class="token operator">=</span> <span class="token number">0.001</span><span class="token comment" spellcheck="true">#权重增加的值</span><span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1101</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#循环迭代预测</span>predidction <span class="token operator">=</span> input <span class="token operator">*</span> weight <span class="token comment" spellcheck="true">#预测出来的值</span>weight_delta<span class="token punctuation">(</span>误差增量就是你的导数<span class="token punctuation">)</span>error <span class="token operator">=</span> <span class="token punctuation">(</span>predidction <span class="token operator">-</span> goal_prediction<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token comment" spellcheck="true">#误差值进行平方</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Error:"</span><span class="token operator">+</span> str<span class="token punctuation">(</span>error<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"predidction:"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>predidction<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#</span>up_prediction <span class="token operator">=</span> input <span class="token operator">*</span> <span class="token punctuation">(</span>weight <span class="token operator">+</span> srep_amount<span class="token punctuation">)</span>up_error <span class="token operator">=</span> goal_prediction <span class="token operator">-</span> up_prediction<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>down_predicton <span class="token operator">=</span> input <span class="token operator">*</span> <span class="token punctuation">(</span>weight <span class="token operator">-</span> dtep_amount<span class="token punctuation">)</span>down_error <span class="token operator">=</span><span class="token punctuation">(</span>goal_prediction <span class="token operator">-</span> down_prediction<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token keyword">if</span> <span class="token punctuation">(</span>down_error <span class="token operator">&lt;</span> up_error<span class="token punctuation">)</span><span class="token punctuation">:</span>    weight <span class="token operator">=</span> weight <span class="token operator">-</span> step_amount<span class="token keyword">if</span> <span class="token punctuation">(</span>down_error <span class="token operator">></span> up_error<span class="token punctuation">)</span><span class="token punctuation">:</span>    weight <span class="token operator">=</span> weight <span class="token operator">+</span> step_amount</code></pre><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降:"></a>梯度下降:</h2><h3 id="更新出来的新权重的值-weight-：（（（权重-×-输入）-真实数据）×-输入）×-训练之前确定的权重增量的值（alpha）"><a href="#更新出来的新权重的值-weight-：（（（权重-×-输入）-真实数据）×-输入）×-训练之前确定的权重增量的值（alpha）" class="headerlink" title="**更新出来的新权重的值(weight)**：（（（权重 × 输入）- 真实数据）× 输入）× 训练之前确定的权重增量的值（alpha）"></a>**更新出来的新权重的值(weight)**：（（（权重 × 输入）- 真实数据）× 输入）× 训练之前确定的权重增量的值（alpha）</h3><p><strong>alpha</strong>：调节控制网络学习的速度。<br> 测量误差，找出权重调节的方向和幅度，对方向和幅度的计算，对权重进行调整以减少错误<br> direction_and_amount = (pred - goal_pred) * input<br> #纯误差，<br> #与输入相乘，用于执行缩放，负值反转和停止调节，对纯误差进行修正以更新权重</p><ul><li><strong>预测的数据（pred）</strong>：权重*输入</li><li><strong>输出节点（delta）</strong>：（权重*输入）- 真实数据（delta）<br> ：当前节点的变化以便完美第预测结果</li><li><strong>误差的增量（weight_delta</strong>）：（（权重*输入）- 真实数据）× 输入数据：将权重的输出节点增加（delta）乘以权重的输入，进行权重缩放操作<br> ：基于导数对权重移动的方向和数量的估计</li><li>原始权重 - 权重的增量</li><li>**更新出来的新权重的值(weight)*<em>：（（（权重</em>输入）- 真实数据）× 输入）× 训练之前确定的权重增量的值（alpha）<br> <strong>纯误差</strong>：预测值和真实值的差值，表示当前的错误的原始方向和幅度，如果是一个正值，那么预测就太高了，反之亦然，如果很大的数字那么就错了很多<br> <strong>停止调节</strong>：例子CD碟片插入立体播放机，把音量调到最大，但是CD播放器是关着的，就是输入为0时，模型就不会进行学习，所有的权重值都会产生相同的误差，对其进行调节没有什么区别，预测的一直为0，<br> <strong>负值翻转</strong>：当输入为负值时纯误差也为负值，再进与输入相乘，权重向着正值方向前进<br> <strong>缩放</strong>：是对纯误差进行第三项修正，是输入引起，如果输入很大，则权重更新也会变得更大，就相当于副作用，进程可能失去控制，<br> <strong>权重修改</strong>：权重的修改，能够使预测的值之间的误差逐渐接近于0，达到预测值和真实值无限接近</li></ul><pre class=" language-python"><code class="language-python">weight<span class="token punctuation">,</span>goal_pred<span class="token punctuation">,</span>input <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.8</span><span class="token punctuation">,</span><span class="token number">1.1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#权重， 真实值，  输入</span><span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"----\nWeight:"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>weight<span class="token punctuation">)</span><span class="token punctuation">)</span>    pred <span class="token operator">=</span> input<span class="token operator">*</span>weight  <span class="token comment" spellcheck="true">#输入乘权重预测值</span>    error <span class="token operator">=</span> <span class="token punctuation">(</span>pred <span class="token operator">-</span> goal_pred<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token comment" spellcheck="true">#预测值-真实值的平方：均方误差</span>    delta <span class="token operator">=</span> pred <span class="token operator">-</span> goal_pred<span class="token comment" spellcheck="true">#纯误差</span>    weight_delta <span class="token operator">=</span> delta <span class="token operator">*</span> input<span class="token comment" spellcheck="true">#反转预测后的值数</span>    weight <span class="token operator">=</span> weight <span class="token operator">-</span> weight_delta<span class="token comment" spellcheck="true">#节点delta值，实际误差与反转后判定的进行相互补</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Error:"</span><span class="token operator">+</span>str<span class="token punctuation">(</span>error<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">"prediction:"</span><span class="token operator">+</span>str<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"delta:"</span><span class="token operator">+</span>str<span class="token punctuation">(</span>delta<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">"weight delta:"</span><span class="token operator">+</span>str<span class="token punctuation">(</span>weight_delta<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="误差减小"><a href="#误差减小" class="headerlink" title="误差减小"></a>误差减小</h2><p>为了使误差减小，我们尽力去寻求权重应该变化的方向和幅度，导数给出了函数中任意两个变量之间的关系，我们可以用导数来确定权重和误差之间的关系，然后把权重的值向导数的反方向移动，就能得到使误差更小的权重值。</p><h2 id="学习就是减少误差"><a href="#学习就是减少误差" class="headerlink" title="学习就是减少误差"></a>学习就是减少误差</h2><ol><li>可以修改权重减少误差：就是找到正确的方向和幅度来改变权重，使误差减小</li><li>原始权重 - 权重的增量</li><li>用斜率来帮助神经网络减少的误差：不管你在碗型曲线的那个位置，斜率都指向碗的底部（此时误差最小）</li></ol><h2 id="学习就是调整权重将误差减小到0"><a href="#学习就是调整权重将误差减小到0" class="headerlink" title="学习就是调整权重将误差减小到0"></a>学习就是调整权重将误差减小到0</h2><ol><li>改变权重值误差的变化就是变化量</li><li>给定一个函数，如果你改变其中一个变量，则它的导数代表了另一个变量发生变化的方向和幅度</li><li>导数是直线或者曲线上的一点斜率</li><li>画出U型曲线：曲线的斜率是正值，这个点就在右边，曲线的斜率为负值</li><li>斜率的符号表示方向，斜率的坡度表示量你可以使这两个方法来帮助我们找到目标权重</li><li>导数代表着，某个函数中两个变量之间的关系</li><li>误差的增量就是你的导数（weight_delta*<em>）：（（权重</em>输入）- 真实数据）× 输入数据：将权重的输出节点增加（delta）乘以权重的输入，进行权重缩放操作，</li><li>画出来的虚色直线是斜率，也是导数，当改变权重时误差会跟着变化多少，如果方向是朝下的，也就说斜率就是负的</li><li>直线或曲线的斜率总是与error最低点的方向相反，所以斜率是负的就增加weight寻求最小的error</li></ol><h2 id="深度学习的目标"><a href="#深度学习的目标" class="headerlink" title="深度学习的目标"></a>深度学习的目标</h2><ol><li>就是误差的减小，尽力寻求权重应该变化的方向和幅度，可以用导数来确定权重和误差之间的关系，然后再把权重的值再向导数的反方向移动，就能得到使误差更小的权重值</li></ol><h2 id="引用α"><a href="#引用α" class="headerlink" title="引用α"></a>引用α</h2><ol><li>α又名alpha：能够减少权重的增量，取值范围0到1之间<br> 把 weight_delta 变量名替换成derivative<br> 2.*更新出来的新权重的值： weight  = weight - (alpha * derivative)</li></ol><p><strong>引用α变量</strong></p><pre class=" language-bash"><code class="language-bash">weight <span class="token operator">=</span> 0.5<span class="token comment" spellcheck="true">#权重</span>goal_pred <span class="token operator">=</span> 0.8<span class="token comment" spellcheck="true">#真实值</span>input <span class="token operator">=</span> 2.0<span class="token comment" spellcheck="true">#输入值</span>alpha <span class="token operator">=</span> 0.1<span class="token comment" spellcheck="true">#α值</span><span class="token keyword">for</span> iteration <span class="token keyword">in</span> range<span class="token punctuation">(</span>20<span class="token punctuation">)</span>:    pred <span class="token operator">=</span> input * weight<span class="token comment" spellcheck="true">#预测值</span>    error <span class="token operator">=</span> <span class="token punctuation">(</span>pred - goal_pred<span class="token punctuation">)</span>**2<span class="token comment" spellcheck="true">#误差值（均方误差）</span>    derivative <span class="token operator">=</span> input * <span class="token punctuation">(</span>pred - goal_pred<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#误差增量:输入数据乘（预测值减去真实值）</span>    weight <span class="token operator">=</span> weight - <span class="token punctuation">(</span>alpha * derivative <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#更新后的权重：权重值减去（误差增量乘a值）</span>    print<span class="token punctuation">(</span><span class="token string">"Error:"</span> +str<span class="token punctuation">(</span>error<span class="token punctuation">)</span> + <span class="token string">"Predicton:"</span> + str<span class="token punctuation">(</span>pred<span class="token punctuation">))</span></code></pre><h2 id="一直更新的权重值"><a href="#一直更新的权重值" class="headerlink" title="一直更新的权重值"></a>一直更新的权重值</h2><p>weight（一直更新权重值） =初始权重值   – [ {（【<strong>预测值：</strong>（输入值×权重值）】减去–【真实值】）×【输入值】} × 【α】]</p><ol><li>预测值 = 输入值×权重值</li><li>误差增量 = （预测值 – 真实值）× 输入值</li><li>更新权重值 = 权重值 - （α × 误差增量）</li></ol><pre class=" language-bash"><code class="language-bash">def w_sum<span class="token punctuation">(</span>a,b<span class="token punctuation">)</span>:    assert <span class="token punctuation">(</span>len<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>b<span class="token punctuation">))</span><span class="token comment" spellcheck="true">#断言:assert:如不相等则抛出异常</span>    output <span class="token operator">=</span> 0<span class="token comment" spellcheck="true">#建立一个空变量</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>a<span class="token punctuation">))</span>:        output +<span class="token operator">=</span> <span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span> * b <span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#权重与输入的数据</span>        <span class="token keyword">return</span> outputweights <span class="token operator">=</span> <span class="token punctuation">[</span>0.1,0.2,-1<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#权重a</span>def neural_network<span class="token punctuation">(</span>input,weights<span class="token punctuation">)</span>:    pred <span class="token operator">=</span> w_sum<span class="token punctuation">(</span>input,weights<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#调用了上例的方法</span>    <span class="token keyword">return</span> pred<span class="token comment" spellcheck="true">#返回了预测的值</span><span class="token comment" spellcheck="true">#预测和比较：做出预测，计算误差和增量</span>toes <span class="token operator">=</span> <span class="token punctuation">[</span>8.5,9.5,9.9,9.0<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#踢进去的球</span>wlrec <span class="token operator">=</span> <span class="token punctuation">[</span>0.65,0.8,0.8,0.9<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#胜率的百分比</span>nfans <span class="token operator">=</span> <span class="token punctuation">[</span>1.2,1.3,0.5,1.0<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#粉丝的数量</span>win_or_lose_binary <span class="token operator">=</span> <span class="token punctuation">[</span>1,1,0,1<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#四队的真实值</span><span class="token boolean">true</span> <span class="token operator">=</span> win_or_lose_binary<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#遍历调用列表值</span>input <span class="token operator">=</span> <span class="token punctuation">[</span>toes<span class="token punctuation">[</span>0<span class="token punctuation">]</span>,wlrec<span class="token punctuation">[</span>0<span class="token punctuation">]</span>,nfans<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#输入遍历的值</span>pred <span class="token operator">=</span>neural_network<span class="token punctuation">(</span>input,weights<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#调用方法</span>error <span class="token operator">=</span> <span class="token punctuation">(</span>pred - true<span class="token punctuation">)</span> ** 2<span class="token comment" spellcheck="true">#误差值</span>delta <span class="token operator">=</span> pred - <span class="token boolean">true</span><span class="token comment" spellcheck="true">#纯误差，预测值-真实值</span><span class="token comment" spellcheck="true">#学习计算权重增量，将它应用对应的权重上</span>def ele_mul<span class="token punctuation">(</span>number,vector<span class="token punctuation">)</span>:<span class="token comment" spellcheck="true">#误差的增量，权重误差乘于输入值</span>    output <span class="token operator">=</span> <span class="token punctuation">[</span>0,0,0<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#值的增量</span>    assert <span class="token punctuation">(</span>len<span class="token punctuation">(</span>output<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>vector<span class="token punctuation">))</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>output<span class="token punctuation">))</span>:        output<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> number * vector<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#更新权重的误差值，纯误差乘于输入值</span>    <span class="token keyword">return</span> outputinput <span class="token operator">=</span> <span class="token punctuation">[</span>toes<span class="token punctuation">[</span>0<span class="token punctuation">]</span>,wlrec<span class="token punctuation">[</span>0<span class="token punctuation">]</span>,nfans<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#输入值</span>pred <span class="token operator">=</span><span class="token punctuation">(</span>pred - true<span class="token punctuation">)</span> ** 2<span class="token comment" spellcheck="true">#均方误差</span>delta <span class="token operator">=</span> pred - <span class="token boolean">true</span><span class="token comment" spellcheck="true">#纯误差</span>weight_deltas <span class="token operator">=</span> ele_mul<span class="token punctuation">(</span>delta,input<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#调用方法</span><span class="token comment" spellcheck="true">#学习:更新权重</span>input <span class="token operator">=</span> <span class="token punctuation">[</span>toes<span class="token punctuation">[</span>0<span class="token punctuation">]</span>,wlrec<span class="token punctuation">[</span>0<span class="token punctuation">]</span>,nfans<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token punctuation">]</span>pred <span class="token operator">=</span> neural_network<span class="token variable"><span class="token punctuation">((</span>input<span class="token punctuation">,</span>weights<span class="token punctuation">))</span></span>error <span class="token operator">=</span> <span class="token punctuation">(</span>pred - true<span class="token punctuation">)</span> ** 2delta <span class="token operator">=</span> pred - <span class="token boolean">true</span>weight_deltas <span class="token operator">=</span> ele_mul<span class="token punctuation">(</span>delta,input<span class="token punctuation">)</span>alpha <span class="token operator">=</span> 0.1<span class="token comment" spellcheck="true">#alpha防止权重过度修正</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>weights<span class="token punctuation">))</span>:    weights<span class="token punctuation">[</span>i<span class="token punctuation">]</span> -<span class="token operator">=</span> alpha*weight_deltas<span class="token punctuation">[</span>i<span class="token punctuation">]</span>    print<span class="token punctuation">(</span><span class="token string">"weights:"</span>+str<span class="token punctuation">(</span>weights<span class="token punctuation">))</span>    print<span class="token punctuation">(</span><span class="token string">"weights Deltas:"</span>+str<span class="token punctuation">(</span>weight_deltas<span class="token punctuation">))</span><span class="token comment" spellcheck="true">#先用方法构成预测数值</span>def neursl_network<span class="token punctuation">(</span>input,weights<span class="token punctuation">)</span>:    out <span class="token operator">=</span> 0    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>input<span class="token punctuation">))</span>:        out +<span class="token operator">=</span> <span class="token punctuation">(</span>input<span class="token punctuation">[</span>i<span class="token punctuation">]</span>*weights<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#预测的值</span>    <span class="token keyword">return</span> out<span class="token comment" spellcheck="true">#预测值乘于输入值，</span>def ele_mul<span class="token punctuation">(</span>scalar,vector<span class="token punctuation">)</span>:    out <span class="token operator">=</span> <span class="token punctuation">[</span>0,0,0<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#有4队但是只有测出来的三个类型</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>out<span class="token punctuation">))</span>:        out<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> vector<span class="token punctuation">[</span>i<span class="token punctuation">]</span> * scalar    <span class="token keyword">return</span> out<span class="token comment" spellcheck="true"># 输入数据</span>toes <span class="token operator">=</span> <span class="token punctuation">[</span>8.5,9.5,9.9,9.0<span class="token punctuation">]</span>wlrec <span class="token operator">=</span> <span class="token punctuation">[</span>0.65,0.8,0.8,0.9<span class="token punctuation">]</span>nfans <span class="token operator">=</span> <span class="token punctuation">[</span>1.2,1.3,0.5,1.0<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 真实数据</span>win_or_lose_binary <span class="token operator">=</span> <span class="token punctuation">[</span>1,1,0,1<span class="token punctuation">]</span><span class="token boolean">true</span> <span class="token operator">=</span> win_or_lose_binary <span class="token punctuation">[</span>0<span class="token punctuation">]</span>alpha <span class="token operator">=</span> 0.01weights <span class="token operator">=</span> <span class="token punctuation">[</span>0.1,0.2,-0.1<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#权重值</span>input <span class="token operator">=</span> <span class="token punctuation">[</span>toes<span class="token punctuation">[</span>0<span class="token punctuation">]</span>,wlrec<span class="token punctuation">[</span>0<span class="token punctuation">]</span>,nfans<span class="token punctuation">[</span>0<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token keyword">for</span> iter <span class="token keyword">in</span> range<span class="token punctuation">(</span>3<span class="token punctuation">)</span>:    pred <span class="token operator">=</span> neursl_network<span class="token punctuation">(</span>input,weights<span class="token punctuation">)</span>    error <span class="token operator">=</span> <span class="token punctuation">(</span>pred - true<span class="token punctuation">)</span>**2<span class="token comment" spellcheck="true">#均方误差值</span>    delta <span class="token operator">=</span> pred - <span class="token boolean">true</span><span class="token comment" spellcheck="true">#纯误差值</span>    weight_deltas <span class="token operator">=</span> ele_mul<span class="token punctuation">(</span>delta,input<span class="token punctuation">)</span>    print<span class="token punctuation">(</span><span class="token string">"Iteration:"</span> + str<span class="token punctuation">(</span>iter+1<span class="token punctuation">))</span>    print<span class="token punctuation">(</span><span class="token string">"pred:"</span>+ str<span class="token punctuation">(</span>pred<span class="token punctuation">))</span>    print<span class="token punctuation">(</span><span class="token string">"eeore"</span>+ str<span class="token punctuation">(</span>error<span class="token punctuation">))</span>    print<span class="token punctuation">(</span><span class="token string">"delta:"</span>+ str<span class="token punctuation">(</span>delta<span class="token punctuation">))</span>    print<span class="token punctuation">(</span><span class="token string">"weights:"</span>,str<span class="token punctuation">(</span>weights<span class="token punctuation">))</span>    print<span class="token punctuation">(</span><span class="token string">"weight_deltas"</span>,weight_deltas<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#更新权重值</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>weights<span class="token punctuation">))</span>:        weights<span class="token punctuation">[</span>i<span class="token punctuation">]</span> -<span class="token operator">=</span> alpha*weight_deltas<span class="token punctuation">[</span>i<span class="token punctuation">]</span></code></pre><h2 id="具有多个输入和输出的梯度下降"><a href="#具有多个输入和输出的梯度下降" class="headerlink" title="具有多个输入和输出的梯度下降"></a>具有多个输入和输出的梯度下降</h2><h3 id="一次学习多个权重值"><a href="#一次学习多个权重值" class="headerlink" title="一次学习多个权重值"></a>一次学习多个权重值</h3><p>每个单独权重对于全局误差所产生的影响的度量，因此，因为误差是共享的，当一项权重达到U型曲线的底部时，所有的权重都会达到U型的底部</p><p>权重A可能对应着重要的数据，对预测有着举足轻重的影响，但如果网络在训练中意外的找到一种不需要它也可能准确推断预测的情况，那么权重A则不将对预测结果产生任何影响。</p><p>误差由训练数据决定，任何网络的权重都可以任意取值，但给定任意特定权重的设置后，则误差值百分百由数据决定。</p><h2 id="具有多输入的梯度下降学习"><a href="#具有多输入的梯度下降学习" class="headerlink" title="具有多输入的梯度下降学习"></a>具有多输入的梯度下降学习</h2><p><strong>神经网络也可以用一个输入做多个预测</strong></p><p>一张图片（28×28）的像素矩阵输入到一维的神经网络中—-》把图片的压成一个1×784的向量，抽出第一行的像素值，再首尾相连，得到一个一维的像素列表（784像素长）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是机器学习&quot;&gt;&lt;a href=&quot;#什么是机器学习&quot; class=&quot;headerlink&quot; title=&quot;什么是机器学习&quot;&gt;&lt;/a&gt;什么是机器学习&lt;/h2&gt;&lt;p&gt;——-先观察后照做&lt;br&gt; ——-机器学习算法都试图以某种它可以的方式来模拟两个数据之间存在的模式，</summary>
      
    
    
    
    <category term="机器学习，神经网络" scheme="https://xiaoyvlongoing.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="机器学习，神经网络" scheme="https://xiaoyvlongoing.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>网络编程</title>
    <link href="https://xiaoyvlongoing.github.io/2020/02/10/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BTC/"/>
    <id>https://xiaoyvlongoing.github.io/2020/02/10/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BTC/</id>
    <published>2020-02-10T06:47:40.000Z</published>
    <updated>2021-06-10T00:50:38.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络编程TC"><a href="#网络编程TC" class="headerlink" title="网络编程TC"></a>网络编程TC</h2><p>[TOC]</p><h2 id="P客户端"><a href="#P客户端" class="headerlink" title="P客户端"></a>P客户端</h2><table><thead><tr><th align="center">变量名称</th><th>变量使用方法</th></tr></thead><tbody><tr><td align="center">ifconfig</td><td>查看网络的ip</td></tr><tr><td align="center">ping</td><td>查看网络是否连通</td></tr><tr><td align="center">socket</td><td>网络编程包</td></tr><tr><td align="center">socket.AF_INET</td><td>ip地址</td></tr><tr><td align="center">socket.SOCK_STREAM</td><td>端口</td></tr><tr><td align="center">tcp_clien_socket.connect</td><td>客户端：传入ip和端口，用于建立与指定指定socket的连接</td></tr><tr><td align="center">tcp_clien_socket.send（字符串.encode(encoding=”utf-8”)）</td><td>send（）：把数据组转换成二进制，encode：写入编辑的字符串和要转成数据组的函数：encode（encoding = “”）</td></tr><tr><td align="center">字符串.encode(encoding=”utf-8”)）</td><td>encode：把字符串转换成数据组</td></tr><tr><td align="center">tcp_clien_socket.recv(数值)</td><td>套接字最大传入数据数值，接收来自socket缓存区对字节数据，当缓存区没有数值时会一直进行阻塞</td></tr><tr><td align="center">.decode()</td><td>对服务端发来的数据进行解码</td></tr><tr><td align="center">conn_socket.close()</td><td>关闭套接字</td></tr></tbody></table><h2 id="网络编程TCP服务端"><a href="#网络编程TCP服务端" class="headerlink" title="网络编程TCP服务端"></a>网络编程TCP服务端</h2><table><thead><tr><th>变量名称</th><th>变量使用方法</th></tr></thead><tbody><tr><td>socket</td><td>网络编程包</td></tr><tr><td>bind</td><td>服务端：创建ip和端口</td></tr><tr><td>tcp_evens_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)</td><td>端口的复用</td></tr><tr><td>tcp_sever_socket.listen（数值）</td><td>设置端口监听功能监听多少个数值上限</td></tr><tr><td>tcp_server_socket.accept()</td><td><code>accept()</code>接受一个客户端的连接请求，接收的数据用于recv，并返回一个新的套接字，用于了send分开数据和ip不同于以上<code>socket()</code>返回的用于监听和接受客户端的连接请求的套接字；，</td></tr></tbody></table><h2 id="扩展函数："><a href="#扩展函数：" class="headerlink" title="扩展函数："></a>扩展函数：</h2><table><thead><tr><th>函数</th><th>函数表达意思</th></tr></thead><tbody><tr><td>client_socket.recv(1024).<strong>decode()</strong></td><td>decode()：将查询的值翻译成其他的值，以其他形式表现出来</td></tr><tr><td><strong>os.listdir</strong></td><td>方法用于返回指定的文件夹包含的文件或文件夹的名字的列表。</td></tr><tr><td><strong>os.path.getsize()</strong></td><td><strong>获得文件的大小（字节）</strong></td></tr><tr><td>f.read（）</td><td>从文件指针所在的位置开始读</td></tr><tr><td>isdigit</td><td></td></tr></tbody></table><pre><code></code></pre><h2 id="静态服务器："><a href="#静态服务器：" class="headerlink" title="静态服务器："></a>静态服务器：</h2><table><thead><tr><th>函数</th><th>函数定义</th></tr></thead><tbody><tr><td>.open（“地址”，“要读或写”）</td><td>用于打开一个文件，创建一个 file 对象，相关的方法才可以调用它进行读写。</td></tr><tr><td>.read()</td><td>读取文件信息内容</td></tr><tr><td>英文：response</td><td>翻译：相应</td></tr><tr><td>英文：Server</td><td>翻译：服务器</td></tr><tr><td>localhost:8080</td><td>浏览器查看</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="使用多线程多客户端连接服务器，多发送信息"><a href="#使用多线程多客户端连接服务器，多发送信息" class="headerlink" title="使用多线程多客户端连接服务器，多发送信息"></a>使用多线程多客户端连接服务器，多发送信息</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> socket<span class="token keyword">import</span> threading<span class="token keyword">def</span> <span class="token function">duoe_sock</span><span class="token punctuation">(</span>code_sock<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        recv_data <span class="token operator">=</span> code_sock<span class="token punctuation">.</span>recv<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span>        data <span class="token operator">=</span> recv_data<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>        code_sock<span class="token punctuation">.</span>send<span class="token punctuation">(</span><span class="token string">"niaho1"</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> data <span class="token operator">==</span> <span class="token string">"quit"</span> <span class="token operator">or</span> <span class="token operator">not</span> data<span class="token punctuation">:</span>            <span class="token keyword">break</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"连接关闭"</span><span class="token punctuation">)</span>    code_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    tcp_evelt_socket <span class="token operator">=</span> socket<span class="token punctuation">.</span>socket<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>AF_INET<span class="token punctuation">,</span>socket<span class="token punctuation">.</span>SOCK_STREAM<span class="token punctuation">)</span>    tcp_evelt_socket<span class="token punctuation">.</span>setsockopt<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>SOL_SOCKET<span class="token punctuation">,</span>socket<span class="token punctuation">.</span>SO_REUSEADDR<span class="token punctuation">,</span><span class="token boolean">True</span><span class="token punctuation">)</span>    tcp_evelt_socket<span class="token punctuation">.</span>bind<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">,</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    tcp_evelt_socket<span class="token punctuation">.</span>listen<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        code_sock<span class="token punctuation">,</span>ip_lies <span class="token operator">=</span> tcp_evelt_socket<span class="token punctuation">.</span>accept<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"ip:"</span><span class="token punctuation">,</span>ip_lies<span class="token punctuation">)</span>        smeus_pskd <span class="token operator">=</span> threading<span class="token punctuation">.</span>Thread<span class="token punctuation">(</span>target<span class="token operator">=</span>duoe_sock<span class="token punctuation">,</span>args<span class="token operator">=</span><span class="token punctuation">(</span>code_sock<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        smeus_pskd<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>    code_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="静态服务器返回固态页面："><a href="#静态服务器返回固态页面：" class="headerlink" title="静态服务器返回固态页面："></a>静态服务器返回固态页面：</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> socket<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    tcp_serer_sock <span class="token operator">=</span> socket<span class="token punctuation">.</span>socket<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>AF_INET<span class="token punctuation">,</span>socket<span class="token punctuation">.</span>SOCK_STREAM<span class="token punctuation">)</span>    tcp_serer_sock<span class="token punctuation">.</span>setsockopt<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>SOL_SOCKET<span class="token punctuation">,</span> socket<span class="token punctuation">.</span>SO_REUSEADDR<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>    tcp_serer_sock<span class="token punctuation">.</span>bind<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">,</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    tcp_serer_sock<span class="token punctuation">.</span>listen<span class="token punctuation">(</span><span class="token number">800</span><span class="token punctuation">)</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        clien_sock<span class="token punctuation">,</span>client_abbr <span class="token operator">=</span> tcp_serer_sock<span class="token punctuation">.</span>accept<span class="token punctuation">(</span><span class="token punctuation">)</span>        clien_request_data <span class="token operator">=</span> clien_sock<span class="token punctuation">.</span>recv<span class="token punctuation">(</span><span class="token number">10230</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>clien_request_data<span class="token punctuation">)</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"./nksld.html"</span><span class="token punctuation">,</span><span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span><span class="token comment" spellcheck="true">#打开文件夹有会报错，异常处理</span>            <span class="token comment" spellcheck="true"># f = open("当前文件夹","rb")</span>            file_data <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 应答行</span>        response_line <span class="token operator">=</span> <span class="token string">"HTTP/1.1 200 OK\r\n"</span>        <span class="token comment" spellcheck="true"># 头</span>        response_header <span class="token operator">=</span> <span class="token string">"Server:pwd\r\n"</span>        <span class="token comment" spellcheck="true"># 体</span>        response_body <span class="token operator">=</span> file_data<span class="token comment" spellcheck="true">#界面的数据</span>        <span class="token comment" spellcheck="true">#应答数据</span>        response_data<span class="token operator">=</span> <span class="token punctuation">(</span>response_line <span class="token operator">+</span> response_header <span class="token operator">+</span> <span class="token string">"\r\n"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> response_body        clien_sock<span class="token punctuation">.</span>send<span class="token punctuation">(</span>response_data<span class="token punctuation">)</span>        clien_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="静态服务器返回指定页面"><a href="#静态服务器返回指定页面" class="headerlink" title="静态服务器返回指定页面"></a>静态服务器返回指定页面</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> socket<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    tcp_server_sock <span class="token operator">=</span> socket<span class="token punctuation">.</span>socket<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>AF_INET<span class="token punctuation">,</span>socket<span class="token punctuation">.</span>SOCK_STREAM<span class="token punctuation">)</span>    tcp_server_sock<span class="token punctuation">.</span>setsockopt<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>SOL_SOCKET<span class="token punctuation">,</span>socket<span class="token punctuation">.</span>SO_REUSEADDR<span class="token punctuation">,</span><span class="token boolean">True</span><span class="token punctuation">)</span>    tcp_server_sock<span class="token punctuation">.</span>bind<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">,</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    tcp_server_sock<span class="token punctuation">.</span>listen<span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        tcp_sock<span class="token punctuation">,</span>ip_sock <span class="token operator">=</span> tcp_server_sock<span class="token punctuation">.</span>accept<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"ip"</span><span class="token punctuation">,</span>ip_sock<span class="token punctuation">)</span>        recv_data <span class="token operator">=</span> tcp_sock<span class="token punctuation">.</span>recv<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">)</span>        spli_dat <span class="token operator">=</span> recv_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>spli_dat<span class="token punctuation">)</span> <span class="token operator">&lt;=</span><span class="token number">1</span><span class="token punctuation">:</span>            tcp_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"浏览器关闭"</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            data <span class="token operator">=</span> spli_dat<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>            <span class="token keyword">if</span> data <span class="token operator">==</span> <span class="token string">"/"</span><span class="token punctuation">:</span>                data <span class="token operator">=</span> <span class="token string">"/001.jpg"</span>            <span class="token keyword">try</span><span class="token punctuation">:</span>                <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"."</span><span class="token operator">+</span> data<span class="token punctuation">,</span><span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>                    f_data <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>                tecponse_hine <span class="token operator">=</span> <span class="token string">"HTTP/1.1 404 NG\r\n"</span>                tecponse_toue <span class="token operator">=</span> <span class="token string">"Server : long\r\n"</span>                tecponse_toes <span class="token operator">=</span> <span class="token string">"404 NG\r\n"</span>                tecponse_zhen <span class="token operator">=</span> <span class="token punctuation">(</span>tecponse_hine<span class="token operator">+</span>tecponse_toue<span class="token operator">+</span><span class="token string">"\r\n"</span><span class="token operator">+</span>tecponse_toes<span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">)</span>                tcp_sock<span class="token punctuation">.</span>send<span class="token punctuation">(</span>tecponse_zhen<span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                tecponse_hine <span class="token operator">=</span> <span class="token string">"HTTP/1.1 200 OK\r\n"</span>                tecponse_toue <span class="token operator">=</span> <span class="token string">"Server : long\r\n"</span>                tecponse_toes <span class="token operator">=</span> f_data                tecponse_zhen <span class="token operator">=</span> <span class="token punctuation">(</span>tecponse_hine  <span class="token operator">+</span> tecponse_toue<span class="token operator">+</span> <span class="token string">"\r\n"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span> tecponse_toes                tcp_sock<span class="token punctuation">.</span>send<span class="token punctuation">(</span>tecponse_zhen<span class="token punctuation">)</span>            <span class="token keyword">finally</span><span class="token punctuation">:</span>                tcp_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="静态服务器多线程任务"><a href="#静态服务器多线程任务" class="headerlink" title="静态服务器多线程任务"></a>静态服务器多线程任务</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> threading<span class="token keyword">import</span> socket<span class="token keyword">def</span> <span class="token function">nuws_sock</span><span class="token punctuation">(</span>tcp_sock<span class="token punctuation">)</span><span class="token punctuation">:</span>    recv_data <span class="token operator">=</span> tcp_sock<span class="token punctuation">.</span>recv<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">)</span>    spli_dat <span class="token operator">=</span> recv_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> len<span class="token punctuation">(</span>spli_dat<span class="token punctuation">)</span> <span class="token operator">&lt;=</span><span class="token number">1</span><span class="token punctuation">:</span>        tcp_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"浏览器关闭"</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        data <span class="token operator">=</span> spli_dat<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> data <span class="token operator">==</span> <span class="token string">"/"</span><span class="token punctuation">:</span>            data <span class="token operator">=</span> <span class="token string">"/001.jpg"</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"."</span><span class="token operator">+</span> data<span class="token punctuation">,</span><span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>                f_data <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>            tecponse_hine <span class="token operator">=</span> <span class="token string">"HTTP/1.1 404 NG\r\n"</span>            tecponse_toue <span class="token operator">=</span> <span class="token string">"Server : long\r\n"</span>            tecponse_toes <span class="token operator">=</span> <span class="token string">"404 NG\r\n"</span>            tecponse_zhen <span class="token operator">=</span> <span class="token punctuation">(</span>tecponse_hine<span class="token operator">+</span>tecponse_toue<span class="token operator">+</span><span class="token string">"\r\n"</span><span class="token operator">+</span>tecponse_toes<span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">)</span>            tcp_sock<span class="token punctuation">.</span>send<span class="token punctuation">(</span>tecponse_zhen<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            tecponse_hine <span class="token operator">=</span> <span class="token string">"HTTP/1.1 200 OK\r\n"</span>            tecponse_toue <span class="token operator">=</span> <span class="token string">"Server : long\r\n"</span>            tecponse_toes <span class="token operator">=</span> f_data            tecponse_zhen <span class="token operator">=</span> <span class="token punctuation">(</span>tecponse_hine  <span class="token operator">+</span> tecponse_toue<span class="token operator">+</span> <span class="token string">"\r\n"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span> tecponse_toes            tcp_sock<span class="token punctuation">.</span>send<span class="token punctuation">(</span>tecponse_zhen<span class="token punctuation">)</span>        <span class="token keyword">finally</span><span class="token punctuation">:</span>            tcp_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    tcp_server_sock <span class="token operator">=</span> socket<span class="token punctuation">.</span>socket<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>AF_INET<span class="token punctuation">,</span>socket<span class="token punctuation">.</span>SOCK_STREAM<span class="token punctuation">)</span>    tcp_server_sock<span class="token punctuation">.</span>setsockopt<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>SOL_SOCKET<span class="token punctuation">,</span>socket<span class="token punctuation">.</span>SO_REUSEADDR<span class="token punctuation">,</span><span class="token boolean">True</span><span class="token punctuation">)</span>    tcp_server_sock<span class="token punctuation">.</span>bind<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">,</span><span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    tcp_server_sock<span class="token punctuation">.</span>listen<span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        tcp_sock<span class="token punctuation">,</span>ip_sock <span class="token operator">=</span> tcp_server_sock<span class="token punctuation">.</span>accept<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"ip"</span><span class="token punctuation">,</span>ip_sock<span class="token punctuation">)</span>        nues_sock <span class="token operator">=</span>  threading<span class="token punctuation">.</span>Thread<span class="token punctuation">(</span>target<span class="token operator">=</span>nuws_sock<span class="token punctuation">,</span>args<span class="token operator">=</span><span class="token punctuation">(</span>tcp_sock<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        nues_sock<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="静态服务器对象调用"><a href="#静态服务器对象调用" class="headerlink" title="静态服务器对象调用"></a>静态服务器对象调用</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> threading<span class="token keyword">import</span> socket<span class="token keyword">class</span> <span class="token class-name">Pues_sock</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>tcp_server_sock <span class="token operator">=</span> socket<span class="token punctuation">.</span>socket<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>AF_INET<span class="token punctuation">,</span> socket<span class="token punctuation">.</span>SOCK_STREAM<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tcp_server_sock<span class="token punctuation">.</span>setsockopt<span class="token punctuation">(</span>socket<span class="token punctuation">.</span>SOL_SOCKET<span class="token punctuation">,</span> socket<span class="token punctuation">.</span>SO_REUSEADDR<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tcp_server_sock<span class="token punctuation">.</span>bind<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">,</span> <span class="token number">8080</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tcp_server_sock<span class="token punctuation">.</span>listen<span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">nuws_sock</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>tcp_sock<span class="token punctuation">)</span><span class="token punctuation">:</span>        recv_data <span class="token operator">=</span> tcp_sock<span class="token punctuation">.</span>recv<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">)</span>        spli_dat <span class="token operator">=</span> recv_data<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>spli_dat<span class="token punctuation">)</span> <span class="token operator">&lt;=</span><span class="token number">1</span><span class="token punctuation">:</span>            tcp_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"浏览器关闭"</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            data <span class="token operator">=</span> spli_dat<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>            <span class="token keyword">if</span> data <span class="token operator">==</span> <span class="token string">"/"</span><span class="token punctuation">:</span>                data <span class="token operator">=</span> <span class="token string">"/001.jpg"</span>            <span class="token keyword">try</span><span class="token punctuation">:</span>                <span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"."</span><span class="token operator">+</span> data<span class="token punctuation">,</span><span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>                    f_data <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>                tecponse_hine <span class="token operator">=</span> <span class="token string">"HTTP/1.1 404 NG\r\n"</span>                tecponse_toue <span class="token operator">=</span> <span class="token string">"Server : long\r\n"</span>                tecponse_toes <span class="token operator">=</span> <span class="token string">"404 NG\r\n"</span>                tecponse_zhen <span class="token operator">=</span> <span class="token punctuation">(</span>tecponse_hine<span class="token operator">+</span>tecponse_toue<span class="token operator">+</span><span class="token string">"\r\n"</span><span class="token operator">+</span>tecponse_toes<span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">)</span>                tcp_sock<span class="token punctuation">.</span>send<span class="token punctuation">(</span>tecponse_zhen<span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                tecponse_hine <span class="token operator">=</span> <span class="token string">"HTTP/1.1 200 OK\r\n"</span>                tecponse_toue <span class="token operator">=</span> <span class="token string">"Server : long\r\n"</span>                tecponse_toes <span class="token operator">=</span> f_data                tecponse_zhen <span class="token operator">=</span> <span class="token punctuation">(</span>tecponse_hine  <span class="token operator">+</span> tecponse_toue<span class="token operator">+</span> <span class="token string">"\r\n"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span> tecponse_toes                tcp_sock<span class="token punctuation">.</span>send<span class="token punctuation">(</span>tecponse_zhen<span class="token punctuation">)</span>            <span class="token keyword">finally</span><span class="token punctuation">:</span>                tcp_sock<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">nuse_sock</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>            tcp_sock<span class="token punctuation">,</span> ip_sock <span class="token operator">=</span> self<span class="token punctuation">.</span>tcp_server_sock<span class="token punctuation">.</span>accept<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"ip"</span><span class="token punctuation">,</span> ip_sock<span class="token punctuation">)</span>            nues_sock <span class="token operator">=</span> threading<span class="token punctuation">.</span>Thread<span class="token punctuation">(</span>target<span class="token operator">=</span>self<span class="token punctuation">.</span>nuws_sock<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span>tcp_sock<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            nues_sock<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    nues <span class="token operator">=</span> Pues_sock<span class="token punctuation">(</span><span class="token punctuation">)</span>    nues<span class="token punctuation">.</span>nuse_sock<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="静态服务器变换端口号"><a href="#静态服务器变换端口号" class="headerlink" title="静态服务器变换端口号"></a>静态服务器变换端口号</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> socket<span class="token keyword">import</span> threading<span class="token keyword">import</span> sys<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 获取执行python程序的终端命令行参数</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">)</span>    <span class="token keyword">if</span> len<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"格式错误 python3 xxx.py 9090"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span>    <span class="token comment" spellcheck="true"># 判断参数的类型，设置端口号必须是整型</span>    <span class="token keyword">if</span> <span class="token operator">not</span> sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isdigit<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"格式错误 python3 xxx.py 9090"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span>    port <span class="token operator">=</span> int<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 创建服务器对象</span>    <span class="token comment" spellcheck="true"># 给Web服务器类的初始化方法添加一个端口号参数，用于绑定端口号</span>    my_web_server <span class="token operator">=</span> HttpWebServer<span class="token punctuation">(</span>port<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 启动服务器</span>    my_web_server<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="TCP协议是什么？提示：TCP定义及特点"><a href="#TCP协议是什么？提示：TCP定义及特点" class="headerlink" title="TCP协议是什么？提示：TCP定义及特点"></a>TCP协议是什么？提示：TCP定义及特点</h2><h4 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h4><p>传输控制协议，是一种面向连接，可靠的，基于字节流的传输层通讯协议</p><h5 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h5><p>面向连接：通信双方必须先建立好连接才能进行数据的传输，数据传输完成后，双方必须断开连接，以释放系统资源</p><p>可靠的通信方式</p><p>基于字节流</p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="2-TCP特点是什么？"><a href="#2-TCP特点是什么？" class="headerlink" title="2. TCP特点是什么？"></a>2. TCP特点是什么？</h2><p>提示：TCP特点及解释说明</p><p>基于字节流的</p><p>面向连接</p><p>可靠通信</p><p>在网络状况不佳的时候尽量降低系统由于重传带来的宽开销</p><p>通信连接维护是面向通信的两个端点，而不考虑中间网段和节点</p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><h2 id="3-在TCP客户端使用connect的作用是什么？"><a href="#3-在TCP客户端使用connect的作用是什么？" class="headerlink" title="3. 在TCP客户端使用connect的作用是什么？"></a>3. 在TCP客户端使用connect的作用是什么？</h2><p>提示：connect的作用及connect()函数的参数形式</p><p>connenct：用于建立与指定指定socket的连接，对于流类套接口（SOCK—STREAMl类型），利用名字来与一个远程主机建立连接，一但套用接口调用成功，他就能收发数据，对于数据报类套接口（SOCK—STREAMl类型），则设置成一个缺省的目的并用它来进行后序的send与recv调用</p><h3 id="4-在TCP服务器中listen的作用是什么？"><a href="#4-在TCP服务器中listen的作用是什么？" class="headerlink" title="4. 在TCP服务器中listen的作用是什么？"></a>4. 在TCP服务器中listen的作用是什么？</h3><p>提示：listen()函数的作用及参数含义、</p><p>在网络中服务端是被动的，客户是主动的，被动要用listen来监听，listen（）作用是将socket（）得到一个sockfd被动监听的套接字，来通知内核来完成连接</p><h2 id="-2"><a href="#-2" class="headerlink" title=""></a></h2><h2 id="5-简述tcp客户端创建的流程。"><a href="#5-简述tcp客户端创建的流程。" class="headerlink" title="5. 简述tcp客户端创建的流程。"></a>5. 简述tcp客户端创建的流程。</h2><p>提示：参考课件代码说出客户端创建步骤</p><p>创建一个套接字协议用：connenct赖建立连接</p><p>导入IP地址可端口</p><p>创建发送的数据用send（数据encode），先把字符串转换成数据组，在把数据组装换成二进制</p><p>设置最大字节数据：recv（）</p><p>接收客户端发来的数据用decode来解码</p><p>关闭套接字用close</p><h2 id="-3"><a href="#-3" class="headerlink" title=""></a></h2><h2 id="6-recv函数的参数表示什么含义？"><a href="#6-recv函数的参数表示什么含义？" class="headerlink" title="6. recv函数的参数表示什么含义？"></a>6. recv函数的参数表示什么含义？</h2><p>提示：recv()函数的作用及参数含义</p><p>接收来自socket缓存区对字节数据，当缓存区没有数值时会一直进行阻塞，直到有数据时进行读取，远程关闭并读取所有数据之后会返回空字符串</p><h2 id="7-简述TCP服务器创建的流程、"><a href="#7-简述TCP服务器创建的流程、" class="headerlink" title="7. 简述TCP服务器创建的流程、"></a>7. 简述TCP服务器创建的流程、</h2><p>提示：参考课件代码，说出服务器创建流程</p><p>1：创建套接字，进行网络协议</p><p>2：进行端口复用</p><p>3：进行bind进行套接字协议</p><p>4：进行与服务端监听连接的最大数值，listen</p><p>5：accept（）接收数据再返回一个新的socket字，接收的数据用于recv，返回客户端的数据用于了send分开数据和ip，</p><p>6：进行设置recv进行数据的阻塞，</p><p>7：给客户端发送一个数据用send（encode）来转成二进制</p><p>8：接收客户端发来的数据，用decode来进行解码</p><h2 id="8-accept的返回值分别代表的什么？"><a href="#8-accept的返回值分别代表的什么？" class="headerlink" title="8. accept的返回值分别代表的什么？"></a>8. accept的返回值分别代表的什么？</h2><p>提示：accept()函数的作用，及返回值的形式和返回值的含义</p><p>accept（）接收数据再返回一个新的socket字，接收的数据用于recv，返回客户端的数据用于了send分开数据和ip，</p><h2 id="9-完成TCP客户端的编写。"><a href="#9-完成TCP客户端的编写。" class="headerlink" title="9. 完成TCP客户端的编写。"></a>9. 完成TCP客户端的编写。</h2><p>服务器的ip和端口号需手动输入</p><p>要发送的信息需要手动输入</p><p>接收服务器返回的数据，并打印。</p><p>请在下方的注释下补充代码：</p><p>​       # 导入socket模块</p><pre><code>import socketif __name__ == &#39;__main__&#39;:    tcp_codne_socket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)    tcp_codne_socket.connect((&quot;&quot;,8080))    tcp_codne_socket.send(&quot;&quot;.encode())    cone_recv = tcp_codne_socket.recv(1024)    print(cone_recv.decode())    tcp_codne_socket.close()</code></pre><h2 id="-4"><a href="#-4" class="headerlink" title=""></a></h2><h2 id="10-完成TCP服务的的编写"><a href="#10-完成TCP服务的的编写" class="headerlink" title="10. 完成TCP服务的的编写"></a>10. 完成TCP服务的的编写</h2><p>请在下方注释下补充代码：</p><p>  # 导入socket模块</p><pre><code>import socketif __name__ == &#39;__main__&#39;:    tcp_evens_socket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)    tcp_evens_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)    tcp_evens_socket.bind((&quot;&quot;,8080))    tcp_evens_socket.listen(138)    coet_socket,ip_mes = tcp_evens_socket.accept()    recv_data = coet_socket.recv(1025)    print(recv_data.decode())    coet_socket.send(&quot;&quot;.encode())    coet_socket.close()</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;网络编程TC&quot;&gt;&lt;a href=&quot;#网络编程TC&quot; class=&quot;headerlink&quot; title=&quot;网络编程TC&quot;&gt;&lt;/a&gt;网络编程TC&lt;/h2&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;P客户端&quot;&gt;&lt;a href=&quot;#P客户端&quot; class=&quot;headerl</summary>
      
    
    
    
    <category term="python高级" scheme="https://xiaoyvlongoing.github.io/categories/python%E9%AB%98%E7%BA%A7/"/>
    
    
    <category term="python高级" scheme="https://xiaoyvlongoing.github.io/tags/python%E9%AB%98%E7%BA%A7/"/>
    
  </entry>
  
</feed>
