<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2021%2F06%2F10%2F%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[1：文本摘要项目理论基本概念文本摘要思想：就是一个长文本把重要的摘要出来 1：NLP角度来看文本摘要任务，主流的概括两大方法： 抽取式摘要：Extraction-based（从文本中抽取）：直接从原文中选择若干条重要的句子, 并对它们进行排序和重组, 以形成摘要的方法. 无监督抽取：不需要平行语料, 节省了人工标记的成本. 大体上有如下几种: Lead Centroid ClusterCMRW TextRank：最经典 有监督抽取：将文本摘要抽象成二分类问题, 通过神经网络来学习句子及其标签之间的对应关系. 需要平行语料, 需要人工标记的成本. 常见方法有如下几种 R2N2 NeuralSum SummaRuNNer BertSum：原本用来计算网页之间的关联性，后来被应用在句子上，句子之间反复传播，选出最关键的语句进行排序。因为文章总有中心句，而textrank就试图找出这个中心句。 取式缺点：主要考虑单词词频，没有过多的语义信息，所以无法建立段落中的完整语义信息 生成式摘要: Abstraction-based：需要通过转述、同义替换、句子缩写等生成。 生成式神经网络模型的基本结构主要由编码器（encoder）和解码器（decoder）组成，编码和解码都由神经网络实现。 编码器负责将输入的原文本编码成一个向量C（context）， 而解码器负责从这个向量C提取重要信息、加工剪辑，生成文本摘要。 Sequence-to-Sequence（以下简称Seq2Seq），被广泛应用于存在输入序列和输出序列的场景，比如机器翻译（一种语言序列到另一种语言序列）、image captioning（图片像素序列到语言序列）、对话机器人（如问题到回答）等 2：数据的处理：一般来说, 在任何项目中, 面对原始数据都要进行接下来的几点工作: 删除空值. 删除”脏”数据. 在这个数据集中的脏数据为： 有很多的技师说和，车主说 文本中有很多的图片字样和语音字样 有很多进口，车型，还有一些符号 删除特定字符的集合. 分词. 完成字符到id的映射. 完成padding, cutting的工作. import pandas as pd train_path = 'train.csv' test_path = 'test.csv' df = pd.read_csv(train_path, encoding='utf-8') df.info() print('**********************') df = pd.read_csv(test_path, encoding='utf-8') df.info() 打印：&lt;class 'pandas.core.frame.DataFrame'> RangeIndex: 82943 entries, 0 to 82942 Data columns (total 6 columns): # Column Non-Null Count Dtype 0 QID 82943 non-null object 1 Brand 81642 non-null object 2 Model 81642 non-null object 3 Question 82943 non-null object 4 Dialogue 82941 non-null object 5 Report 82873 non-null object dtypes: object(6) memory usage: 3.8+ MB &lt;class 'pandas.core.frame.DataFrame'> RangeIndex: 20000 entries, 0 to 19999 Data columns (total 5 columns): # Column Non-Null Count Dtype 0 QID 20000 non-null object 1 Brand 19987 non-null object 2 Model 19987 non-null object 3 Question 20000 non-null object 4 Dialogue 20000 non-null object dtypes: object(5) memory usage: 781.4+ KB 3：TextRank：算法理论基础对比于衡量网页重要性的PageRank算法, TextRank算法用于衡量哪些单词是关键词, 类比之下的算法思想也就很好理解了: 如果一个单词出现在很多单词的后面, 就是它和很多单词有关联, 那么说明这个单词比较重要. 如果一个TextRank值很高的单词后面跟着另一个单词, 那么后面这个单词的TextRank值也会相应的被提高. 3.1：TextRank算法代码实践 在本小节中, 我们仅以示例代码跑通几段小程序, 让同学们掌握如何具体在代码层面用TextRank. 关键词抽取(keyword extraction)：是指从文本中确定一些能够描述文档含义的关键术语的过程 对关键词抽取而言, 用于构建顶点集的文本单元可以使句子中的一个或多个字. 根据这些字之间的关系构建边. 根据任务的需要, 可以使用语法过滤器(syntactic filters)对顶点集进行优化. 语法过滤器的主要作用是将某一类或者某几类词性的字过滤出来作为顶点集. 关键短语抽取(keyphrase extraction) 关键句抽取(sentence extraction) # coding=utf-8 # 导入textrank4zh的相关工具包 from textrank4zh import TextRank4Keyword, TextRank4Sentence import os import sys # 导入常用工具包 import pandas as pd import numpy as np #关键词抽取 def keywords_extraction(text): # allow_speech_tags : 词性列表, 用于过滤某些词性的词 tr4w = TextRank4Keyword(allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz']) # text: 文本内容, 字符串 # window: 窗口大小, int, 用来构造单词之间的边, 默认值为2 # lower: 是否将英文文本转换为小写, 默认值为False # vertex_source: 选择使用words_no_filter, words_no_stop_words, words_all_filters中的>哪一个来构造pagerank对应的图中的节点 #默认值为'all_filters', 可选值为'no_filter', 'no_stop_words', 'all_filters' # edge_source: 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪>一个来构造pagerank对应的图中的节点之间的边 #默认值为'no_stop_words', 可选值为'no_filter', 'no_stop_words', 'all_filters', 边的构造要结合window参数 # pagerank_config: pagerank算法参数配置, 阻尼系数为0.85 tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words', pagerank_config={'alpha': 0.85, }) # num: 返回关键词数量 # word_min_len: 词的最小长度, 默认值为1 keywords = tr4w.get_keywords(num=6, word_min_len=2) #提取重要关键短句 # keywords_num: 抽取的关键词数量 # min_occur_num: 关键短语在文中的最少出现次数 #keyphrases = tr4w.get_keyphrases(keywords_num=6, min_occur_num=1) #提取关键句 #tr4s = TextRank4Sentence() # text: 文本内容, 字符串 # lower: 是否将英文文本转换为小写, 默认值为False # source: 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来 生成句子之间的相似度 #默认值为'all_filters', 可选值为'no_filter', 'no_stop_words', 'all_filters' #tr4s.analyze(text, lower=True, source='all_filters') # 获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要 #keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6) # 返回关键词 return keywords if __name__ == "__main__": text = "来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，" \ "我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、" \ "副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”" \ "据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，" \ "获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，" \ "国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，" \ "正式将该小行星命名为“周又元星”。" #关键词抽取 keywords=keywords_extraction(text) print(keywords) 打印：#关键词抽取 [{'word': '小行星', 'weight': 0.05808441467341854}, {'word': '天文台', 'weight': 0.05721653775742513}, {'word': '命名', 'weight': 0.0485177005159723}, {'word': '中国', 'weight': 0.045716478124251815}, {'word': '中国科学院', 'weight': 0.037818937836996636}, {'word': '国家', 'weight': 0.03438059254484016}] #提取重要关键短句 ['小行星命名'] #提取关键句 [{'index': 4, 'sentence': '2018年9月25日，经国家天文台申报，国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，正式将该小行星命名为“周又元星”', 'weight': 0.2281040325096452}, {'index': 3, 'sentence': '”据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，获得国际永久编号第120730号', 'weight': 0.2106246105971721}, {'index': 1, 'sentence': '4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂', 'weight': 0.2020923401661083}] 3.2：基于jieba的TextRank算法 jieba工具不仅仅可以用来分词, 进行词性分析. 也可以用来完成TextRank. import jieba.analyse def jieba_keywords_textrank(text): keywords = jieba.analyse.textrank(text, topK=6) return keywords if __name__ == "__main__": text = "来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，" \ "我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、" \ "副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”" \ "据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，" \ "获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，" \ "国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，" \ "正式将该小行星命名为“周又元星”。" # 基于jieba的textrank算法实现 keywords = jieba_keywords_textrank(text) print(keywords) 输出结果: ['小行星', '命名', '国际', '中国', '国家', '天文学家'] 2： TextRank实现baseline-0模型数据预处理： 我们在第一章的1.2小节曾经讨论过原始数据存在的各种问题, 这些问题都需要在数据预处理的这个环节一一解决. 接下来按照如下步骤进行处理: 第一步: 提取特定的文本. 第二步: 删除”脏”数据. 第三步: 删除特定的字符集合. 第四步: 删除特殊位置的特定字符. 第一步: 提取特定的文本.面对原始语料, 并不是说我们必须要全部纳入模型中, 可以根据业务需求, 或者程序员的项目经验, 或许出于尝试的态度, 只选取一部分出来作为我们后续模型的输入数据. def clean_sentence(sentence): # 1. 将sentence按照'|'分句，并只提取技师的话 sub_jishi = [] # 按照'|'字符将车主和用户的对话分离 sub = sentence.split('|') # 遍历每个子句 for i in range(len(sub)): # 如果不是以句号结尾, 增加一个句号 if not sub[i].endswith('。'): sub[i] += '。' # 只使用技师说的句子 if sub[i].startswith('技师'): sub_jishi.append(sub[i]) # 拼接成字符串并返回 sentence = ''.join(sub_jishi) return sentence if __name__ == '__main__': # 读取数据, 并指定编码格式为'utf-8' df = pd.read_csv('train.csv', engine='python', encoding='utf-8') texts = df['Dialogue'].tolist() print('预处理前的第一条句子：', texts[0]) print('********************************') # 数据预处理 res = clean_sentence(texts[0]) print('预处理后的第一条句子: ', res) 输出结果: 预处理前的第一条句子： 技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了方向机带的有|车主说：[图片]|技师说：[语音]|车主说：有助力就是重，这车要匹配吧|技师说：不需要|技师说：你这是更换的部件有问题|车主说：跑快了还好点，就倒车重的很。|技师说：是非常重吗|车主说：是的，累人|技师说：[语音]|车主说：我觉得也是，可是车主是以前没这么重，选吧助理泵换了不行，又把放向机换了，现在还这样就不知道咋和车主解释。|技师说：[语音]|技师说：[语音] 预处理后的第一条句子: 技师说：[语音]。技师说：[语音]。技师说：[语音]。技师说：不需要。技师说：你这是更换的部件有问题。技师说：是非常重吗。技师说：[语音]。技师说：[语音]。技师说：[语音]。 数据进行分割形成小型数据 nf = df[:5000] nf.to_csv("dev.csv",mode='w') dfa = pd.read_csv("dev.csv",encoding='utf-8') dfa.head() 打印结果 第二步: 删除”脏”数据. 关于什么是”脏”数据是个千人千面的问题, 我们在第一章中也讨论过. 这一步也仅仅处理一个baseline的级别. # 导入正则表达式工具包, 用来删除特定模式的数据 import re def clean_sentence(sentence): # 1. 将sentence按照'|'分句，并只提取技师的话 sub_jishi = [] # 按照'|'字符将车主和用户的对话分离 sub = sentence.split('|') # 遍历每个子句 for i in range(len(sub)): # 如果不是以句号结尾, 增加一个句号 if not sub[i].endswith('。'): sub[i] += '。' # 只使用技师说的句子 if sub[i].startswith('技师'): sub_jishi.append(sub[i]) # 拼接成字符串并返回 sentence = ''.join(sub_jishi) # 第二步中添加的两个处理, 利用正则表达式re工具 # 2. 删除1. 2. 3. 这些标题 r = re.compile("\D(\d\.)\D") sentence = r.sub("", sentence) # 3. 删除一些无关紧要的词以及语气助词 r = re.compile(r"车主说|技师说|语音|图片|呢|吧|哈|啊|啦") sentence = r.sub("", sentence) return sentence if __name__ == '__main__': # 读取数据, 并指定编码格式为'utf-8' df = pd.read_csv('dev.csv', engine='python', encoding='utf-8') texts = df['Dialogue'].tolist() print('预处理前的第一条句子：', texts[0]) print('********************************') # 数据预处理 res = clean_sentence(texts[0]) print('预处理后的第一条句子: ', res) 处理后的数据 预处理前的第一条句子： 技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了方向机带的有|车主说：[图片]|技师说：[语音]|车主说：有助力就是重，这车要匹配吧|技师说：不需要|技师说：你这是更换的部件有问题|车主说：跑快了还好点，就倒车重的很。|技师说：是非常重吗|车主说：是的，累人|技师说：[语音]|车主说：我觉得也是，可是车主是以前没这么重，选吧助理泵换了不行，又把放向机换了，现在还这样就不知道咋和车主解释。|技师说：[语音]|技师说：[语音] 预处理后的第一条句子: ：[]。：[]。：[]。：不需要。：你这是更换的部件有问题。：是非常重吗。：[]。：[]。：[] 第三步: 删除特定的字符集合. 1: 我们发现原始数据文件中有若干的”进口”, “海外”字样, 可认为是需要删除的特定字符. 2: 为了后续处理文本容易, 除了汉字还有数字, 英文字母, 特定的几个标点符号, 其他都删除. 3: 将标点符号的半角格式, 转变成全角格式. 4: 将问号, 感叹号, 转变成句号. # 导入正则表达式工具包, 用来删除特定模式的数据 import re def clean_sentence(sentence): # 第一步要处理的代码 # 1. 将sentence按照'|'分句，并只提取技师的话 sub_jishi = [] # 按照'|'字符将车主和用户的对话分离 sub = sentence.split('|') # 遍历每个子句 for i in range(len(sub)): # 如果不是以句号结尾, 增加一个句号 if not sub[i].endswith('。'): sub[i] += '。' # 只使用技师说的句子 if sub[i].startswith('技师'): sub_jishi.append(sub[i]) # 拼接成字符串并返回 sentence = ''.join(sub_jishi) # 第二步中添加的两个处理, 利用正则表达式re工具 # 2. 删除1. 2. 3. 这些标题 r = re.compile("\D(\d\.)\D") sentence = r.sub("", sentence) # 3. 删除一些无关紧要的词以及语气助词 r = re.compile(r"车主说|技师说|语音|图片|呢|吧|哈|啊|啦") sentence = r.sub("", sentence) # 第三步中添加的4个处理 # 4. 删除带括号的 进口 海外 r = re.compile(r"[(（]进口[)）]|\(海外\)") sentence = r.sub("", sentence) # 5. 删除除了汉字数字字母和，！？。.- 以外的字符 r = re.compile("[^，！？。\.\-\u4e00-\u9fa5_a-zA-Z0-9]") # 6. 半角变为全角 sentence = sentence.replace(",", "，") sentence = sentence.replace("!", "！") sentence = sentence.replace("?", "？") # 7. 问号叹号变为句号 sentence = sentence.replace("？", "。") sentence = sentence.replace("！", "。") sentence = r.sub("", sentence) # 第四步添加的删除特定位置的特定字符 # 8. 删除句子开头的逗号 if sentence.startswith('，'): sentence = sentence[1:] return sentence if __name__ == '__main__': # 读取数据, 并指定编码格式为'utf-8' df = pd.read_csv('dev.csv', engine='python', encoding='utf-8') texts = df['Dialogue'].tolist() print('预处理前的第一条句子：', texts[0]) print('********************************') # 数据预处理 res = clean_sentence(texts[0]) print('预处理后的第一条句子: ', res) 打印操作预处理前的第一条句子： 技师说：[语音]|车主说：新的都换了|车主说：助力泵，方向机|技师说：[语音]|车主说：换了方向机带的有|车主说：[图片]|技师说：[语音]|车主说：有助力就是重，这车要匹配吧|技师说：不需要|技师说：你这是更换的部件有问题|车主说：跑快了还好点，就倒车重的很。|技师说：是非常重吗|车主说：是的，累人|技师说：[语音]|车主说：我觉得也是，可是车主是以前没这么重，选吧助理泵换了不行，又把放向机换了，现在还这样就不知道咋和车主解释。|技师说：[语音]|技师说：[语音] 预处理后的第一条句子: 。。。不需要。你这是更换的部件有问题。是非常重吗。。。。 2.1：TextRank模型代码实现：进行数数的机器学习模型# 导入正则表达式工具包, 用来删除特定模式的数据 import re def clean_sentence(sentence): # 第一步要处理的代码 # 1. 将sentence按照'|'分句，并只提取技师的话 sub_jishi = [] # 按照'|'字符将车主和用户的对话分离 sub = sentence.split('|') # 遍历每个子句 for i in range(len(sub)): # 如果不是以句号结尾, 增加一个句号 if not sub[i].endswith('。'): sub[i] += '。' # 只使用技师说的句子 if sub[i].startswith('技师'): sub_jishi.append(sub[i]) # 拼接成字符串并返回 sentence = ''.join(sub_jishi) # 第二步中添加的两个处理, 利用正则表达式re工具 # 2. 删除1. 2. 3. 这些标题 r = re.compile("\D(\d\.)\D") sentence = r.sub("", sentence) # 3. 删除一些无关紧要的词以及语气助词 r = re.compile(r"车主说|技师说|语音|图片|呢|吧|哈|啊|啦") sentence = r.sub("", sentence) # 第三步中添加的4个处理 # 4. 删除带括号的 进口 海外 r = re.compile(r"[(（]进口[)）]|\(海外\)") sentence = r.sub("", sentence) # 5. 删除除了汉字数字字母和，！？。.- 以外的字符 r = re.compile("[^，！？。\.\-\u4e00-\u9fa5_a-zA-Z0-9]") # 6. 半角变为全角 sentence = sentence.replace(",", "，") sentence = sentence.replace("!", "！") sentence = sentence.replace("?", "？") # 7. 问号叹号变为句号 sentence = sentence.replace("？", "。") sentence = sentence.replace("！", "。") sentence = r.sub("", sentence) # 第四步添加的删除特定位置的特定字符 # 8. 删除句子开头的逗号 if sentence.startswith('，'): sentence = sentence[1:] return sentence if __name__ == '__main__': # 读取数据, 并指定编码格式为'utf-8' df = pd.read_csv('dev.csv', engine='python', encoding='utf-8') texts = df['Dialogue'].tolist() for i in range(len(texts)): texts[i] = clean_sentence(texts[i]) if i % 500 == 0: print("i=",i) # 初始化结果存放的列表 results = [] # 初始化textrank4zh类对象 tr4s = TextRank4Sentence() for i in range(len(texts)): text = texts[i] tr4s.analyze(text = text,lower = True,source = "all_filters") result = "" # 直接调用函数获取关键语句 # num=3: 获取重要性最高的3个句子. # sentence_min_len=2: 句子的长度最小等于2. for item in tr4s.get_key_sentences(num=3, sentence_min_len=2): result += item.sentence result += '。' results.append(result) # 间隔100次打印结果 if (i + 1) % 100 == 0: print(i + 1, result) print('result length: ', len(results)) # 保存结果 df['Prediction'] = results # 提取ID, Report, 和预测结果这3列 df = df[['QID', 'Report', 'Prediction']] # 保存结果，这里自动生成一个结果名 df.to_csv('textrank_result_.csv', index=None, sep=',') # 将空行置换为随时联系, 文件保存格式指定为utf-8 df = pd.read_csv('textrank_result_.csv', engine='python', encoding='utf-8') df = df.fillna('随时联系。') # 将处理后的文件保存起来 df.to_csv('textrank_result_final_.csv', index=None, sep=',') 打印结果 i= 0 i= 500 i= 1000 i= 1500 i= 2000 i= 2500 i= 3000 i= 3500 i= 4000 i= 4500 100 这个故障一般是转向系统的故障，需要重点检查一下车辆的转向灯泡，是否有不亮的。换了就可以解决问题。你好，车辆故障信息代码标识。 200 主要是找到发动机和变速器连接部位的支架以及底盘上的各个胶套，是否存在松旷，造成硬性的接触共振。你好，这种情况主要考虑检查底盘是否存在共振区域。这个需要全面检查，如果之前没有这种问题，可以试驾其他同款车辆，确认是否都有这种问题。 300 您好出厂的时候变速箱油是正常的，在刻度线以内，流出半升之后还是在刻度线以内，所以不用加，亲。加不进去了。顶进去的。 400 那就要去检查一下皮带轮了。如果高于1200还有，就要检查一下皮带轮。这款发动机在900-1100转时确实有一点嗡嗡声。 500 这种单边积水的情况基本上不是泡水的原因，而是有地方密封不严，下雨漏水进去，这个一般是门边密封条老化，或者是防风玻璃漏水，需要关上车门，在车外冲水，在车内慢慢找漏水的地方。海绵积水可以用洗车的泡沫海绵放在上面挤压，能把大部的水吸出来，然后大太阳的时候打开车门得晒一个星期左右，差不多就会干了，主要还是得找出进水的原因，不然弄干水了，下次又进水了，时间长了，海绵会发霉发臭的。我知道是地板上积水，空调管堵塞也是会造成这种情况的，也是需要检查的。 600 您好，需要更换加油管和碳罐，加油时气体排不出去导致跳枪。估计外面找不到改款配件。加油管是改款双管的，原车是单管的。 700 找领导处理，更换新件并赔偿。送保养次数。投诉厂家，欺骗消费者。 800 用T10170和百分表，转曲轴看百分表指针，指针转到顺时针的最顶端是一缸上止点。把曲轴转到一缸上止点，然后挂链条就可以了。直接量一缸的上止点不就可以了，不用管其他缸。 3： seq2seq实现baseline-1模型：文本摘要seq2seq实现文本摘要的架构 首选回顾一下在英译法任务中的经典seq2seq架构图 编码器端负责将输入数据进行编码, 得到中间语义张量. 解码器端负责一次次的循环解析中间语义张量, 得到最终的结果语句. 一般来说, 我们将注意力机制添加在解码器端. 对比于英译法任务, 我们再来看文本摘要任务下的seq2seq架构图: 编码器端负责进行原始文本的编码. 注意力层结合编码张量和解码器端的当前输入, 得到总体上的内容张量. 最后在注意力机制的指导下, 解码器端得到完整的单词分布, 解码出当前时间步的单词. seq2seq实现文本摘要的架构代码实践若干工具函数的实现 在这一部分中我们要实现如下几个工具函数: 第一步: 实现配置函数config.py 第二步: 实现多核并行处理的函数multi_proc_utils.py 第三步: 实现参数配置函数params_utils.py 第四步: 实现保存字典的函数word2vec_utils.py 第五步: 实现数据加载的函数data_loader.py 第一步: 实现配置函数config.py 代码文件路径: /home/ec2-user/text_summary/seq2seq/utils/config.py # 导入os工具包 import os import sys # 设置项目代码库的root路径, 为后续所有的包导入提供便利 root_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) sys.path.append(root_path) print(root_path) # 设置原始数据文件的路径, 通过以项目root路径为基础, 逐级添加到文件路径 train_raw_data_path = os.path.join(root_path, 'data', 'train.csv') test_raw_data_path = os.path.join(root_path, 'data', 'test.csv') # 停用词路径和jieba分词用户自定义字典路径 stop_words_path = os.path.join(root_path, 'data', 'stopwords.txt') user_dict_path = os.path.join(root_path, 'data', 'user_dict.txt') # 预处理+切分后的训练测试数据路径 train_seg_path = os.path.join(root_path, 'data', 'train_seg_data.csv') test_seg_path = os.path.join(root_path, 'data', 'test_seg_data.csv') # 将训练集和测试机数据混合后的文件路径 merged_seg_path = os.path.join(root_path, 'data', 'merged_seg_data.csv') # 样本与标签分离，并经过pad处理后的数据路径 train_x_pad_path = os.path.join(root_path, 'data', 'train_X_pad_data.csv') train_y_pad_path = os.path.join(root_path, 'data', 'train_Y_pad_data.csv') test_x_pad_path = os.path.join(root_path, 'data', 'test_X_pad_data.csv') # numpy转换为数字后最终使用的的数据路径 train_x_path = os.path.join(root_path, 'data', 'train_X.npy') train_y_path = os.path.join(root_path, 'data', 'train_Y.npy') test_x_path = os.path.join(root_path, 'data', 'test_X.npy') # 正向词典和反向词典路径 vocab_path = os.path.join(root_path, 'data', 'wv', 'vocab.txt') reverse_vocab_path = os.path.join(root_path, 'data', 'wv', 'reverse_vocab.txt') # 测试集结果保存路径 result_save_path = os.path.join(root_path, 'data', 'result') 输出结果: /home/ec2-user/text_summary/seq2seq 第二步: 实现多核并行处理的函数multi_proc_utils.py 代码文件路径: /home/ec2-user/text_summary/seq2seq/utils/multi_proc_utils.py import pandas as pd import numpy as np from multiprocessing import cpu_count, Pool # 计算当前服务器CPU的数量 cores = cpu_count() # 将分块个数设置为CPU的数量 partitions = cores print(cores) def parallelize(df, func): # 数据切分 data_split = np.array_split(df, partitions) # 初始化线程池 pool = Pool(cores) # 数据分发, 处理, 再合并 data = pd.concat(pool.map(func, data_split)) # 关闭线程池 pool.close() # 执行完close后不会有新的进程加入到pool, join函数等待所有子进程结束 pool.join() # 返回处理后的数据 return data 输出结果: # 当前服务器是一个8核CPU, 32GB内存的机器 8 第三步: 实现参数配置函数params_utils.py 代码文件路径: /home/ec2-user/text_summary/seq2seq/utils/params_utils.py import argparse def get_params(): parser = argparse.ArgumentParser() # 编码器和解码器的最大序列长度 parser.add_argument("--max_enc_len", default=300, help="Encoder input max sequence length", type=int) parser.add_argument("--max_dec_len", default=50, help="Decoder input max sequence length", type=int) # 一个训练批次的大小 parser.add_argument("--batch_size", default=64, help="Batch size", type=int) # seq2seq训练轮数 parser.add_argument("--seq2seq_train_epochs", default=20, help="Seq2seq model training epochs", type=int) # 词嵌入大小 parser.add_argument("--embed_size", default=500, help="Words embeddings dimension", type=int) # 编码器、解码器以及attention的隐含层单元数 parser.add_argument("--enc_units", default=512, help="Encoder GRU cell units number", type=int) parser.add_argument("--dec_units", default=512, help="Decoder GRU cell units number", type=int) parser.add_argument("--attn_units", default=20, help="Used to compute the attention weights", type=int) # 学习率 parser.add_argument("--learning_rate", default=0.001, help="Learning rate", type=float) args = parser.parse_args() # param是一个字典类型的变量，键为参数名，值为参数值 params = vars(args) return params if __name__ == '__main__': res = get_params() print(res) 输出结果: {'max_enc_len': 300, 'max_dec_len': 50, 'batch_size': 64, 'seq2seq_train_epochs': 第四步: 实现保存字典的函数word2vec_utils.py 代码文件路径: /home/ec2-user/text_summary/seq2seq/utils/word2vec_utils.py from gensim.models.word2vec import Word2Vec def load_embedding_matrix_from_model(wv_model_path): # 从word2vec模型中获取词向量矩阵 # wv_model_path: word2vec模型的路径 wv_model = Word2Vec.load(wv_model_path) # wv_model.wv.vectors包含词向量矩阵 embedding_matrix = wv_model.wv.vectors return embedding_matrix def get_vocab_from_model(wv_model_path): # 从word2vec模型中获取正向和反向词典 # wv_model_path: word2vec模型的路径 wv_model = Word2Vec.load(wv_model_path) id_to_word = {index: word for index, word in enumerate(wv_model.wv.index2word)} word_to_id = {word: index for index, word in enumerate(wv_model.wv.index2word)} return word_to_id, id_to_word def save_vocab_as_txt(filename, word_to_id): # 保存字典 # filename: 目标txt文件路径 # word_to_id: 要保存的字典 with open(filename, 'w', encoding='utf-8') as f: for k, v in word_to_id.items(): f.write("{}\t{}\n".format(k, v))]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F06%2F10%2F1%EF%BC%9A%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81%E9%A1%B9%E7%9B%AE%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[1：文本摘要项目理论基本概念文本摘要思想：就是一个长文本把重要的摘要出来 1：NLP角度来看文本摘要任务，主流的概括两大方法： 抽取式摘要：Extraction-based（从文本中抽取）：直接从原文中选择若干条重要的句子, 并对它们进行排序和重组, 以形成摘要的方法. 无监督抽取：不需要平行语料, 节省了人工标记的成本. 大体上有如下几种: Lead Centroid ClusterCMRW TextRank：最经典 有监督抽取：将文本摘要抽象成二分类问题, 通过神经网络来学习句子及其标签之间的对应关系. 需要平行语料, 需要人工标记的成本. 常见方法有如下几种 R2N2 NeuralSum SummaRuNNer BertSum：原本用来计算网页之间的关联性，后来被应用在句子上，句子之间反复传播，选出最关键的语句进行排序。因为文章总有中心句，而textrank就试图找出这个中心句。 取式缺点：主要考虑单词词频，没有过多的语义信息，所以无法建立段落中的完整语义信息 生成式摘要: Abstraction-based：需要通过转述、同义替换、句子缩写等生成。 生成式神经网络模型的基本结构主要由编码器（encoder）和解码器（decoder）组成，编码和解码都由神经网络实现。 编码器负责将输入的原文本编码成一个向量C（context）， 而解码器负责从这个向量C提取重要信息、加工剪辑，生成文本摘要。 Sequence-to-Sequence（以下简称Seq2Seq），被广泛应用于存在输入序列和输出序列的场景，比如机器翻译（一种语言序列到另一种语言序列）、image captioning（图片像素序列到语言序列）、对话机器人（如问题到回答）等 2：数据的处理：一般来说, 在任何项目中, 面对原始数据都要进行接下来的几点工作: 删除空值. 删除”脏”数据. 在这个数据集中的脏数据为： 有很多的技师说和，车主说 文本中有很多的图片字样和语音字样 有很多进口，车型，还有一些符号 删除特定字符的集合. 分词. 完成字符到id的映射. 完成padding, cutting的工作. import pandas as pd train_path = 'train.csv' test_path = 'test.csv' df = pd.read_csv(train_path, encoding='utf-8') df.info() print('**********************') df = pd.read_csv(test_path, encoding='utf-8') df.info() 打印：&lt;class 'pandas.core.frame.DataFrame'> RangeIndex: 82943 entries, 0 to 82942 Data columns (total 6 columns): # Column Non-Null Count Dtype 0 QID 82943 non-null object 1 Brand 81642 non-null object 2 Model 81642 non-null object 3 Question 82943 non-null object 4 Dialogue 82941 non-null object 5 Report 82873 non-null object dtypes: object(6) memory usage: 3.8+ MB &lt;class 'pandas.core.frame.DataFrame'> RangeIndex: 20000 entries, 0 to 19999 Data columns (total 5 columns): # Column Non-Null Count Dtype 0 QID 20000 non-null object 1 Brand 19987 non-null object 2 Model 19987 non-null object 3 Question 20000 non-null object 4 Dialogue 20000 non-null object dtypes: object(5) memory usage: 781.4+ KB 3：TextRank：算法理论基础对比于衡量网页重要性的PageRank算法, TextRank算法用于衡量哪些单词是关键词, 类比之下的算法思想也就很好理解了: 如果一个单词出现在很多单词的后面, 就是它和很多单词有关联, 那么说明这个单词比较重要. 如果一个TextRank值很高的单词后面跟着另一个单词, 那么后面这个单词的TextRank值也会相应的被提高. 3.1：TextRank算法代码实践 在本小节中, 我们仅以示例代码跑通几段小程序, 让同学们掌握如何具体在代码层面用TextRank. 关键词抽取(keyword extraction)：是指从文本中确定一些能够描述文档含义的关键术语的过程 对关键词抽取而言, 用于构建顶点集的文本单元可以使句子中的一个或多个字. 根据这些字之间的关系构建边. 根据任务的需要, 可以使用语法过滤器(syntactic filters)对顶点集进行优化. 语法过滤器的主要作用是将某一类或者某几类词性的字过滤出来作为顶点集. 关键短语抽取(keyphrase extraction)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F05%2F23%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%9A%E6%97%A0%E5%BA%8F%E6%89%A7%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[函数名 使用的方法 导入的函数包import multiprocessing 创建的进程导入的包名 multiprocessing.Process([group [, target [, name [, args：传入指定形参 [, kwargs：以字典的类型进行传参]]]]]) 导入进程函数说明 target=方法名 执行的目标任务名（函数名，线程名） name=进程的名字 进程名字 对象名=multiprocessing.Process(target=方法名) 启动进程的方法 子进程对象.start() 对象.start（）：启动子进程实例（创建子进程） os.getpid print(os.getpid)查看子进程编号 os.getppid print(os.getppid)查看父进程编号 multiprocessing.current_process().name进程名字 查看当前进程编号 子进程对象.join() 加入jojn后主线程会等待子线程执行后再结束主线程，让线程可以形成串行，形成有顺序的，线性和进程的也可进行串行，结束前一个，进行下一个执行 子进程对象.terminate() 不管任务是否完成，立即终止子进程 子进程对象.daemom = True 不管任务是否完成，立即终止子进程 进程在ubuntu中 ps -aux|grep 要查找的线程的名字 过滤需要的进程来查看ID os.kill(os.getpid(),9) 进程的id 强制杀死进程 多线程：无序执行 函数名 使用方法 import threading 创建线程的包 方法名= threading.Thread(target=方法名) 启动进程的方法 对象名 = threading.Thread(target=方法名,daemon= True) 主线程结束，会让子线程结束(方法1） 对象名 .setDaemon(True)：在启动线程之前 主线程结束，会让子线程结束(方法2） current = threading.current_thread().name 获取线程的名称 多线程不能强制杀死，要把线程进行循环停止加上一个 变量 = lock = threading.Lock() 创建互斥锁 lock.acquire() 创建锁 lock.release 解锁 [TOC] 1：进程是操作系统资源分配的最小单位 2：进程之间执行也是无序的，它是由操作系统调度决定的，操作系统调度哪个进程，哪个进程就先执行，没有调度的进程不能执行 3：进程不共享全局变量 二：线程和进程有什么区别： 1：进程是操作系统资源分配的最小单位，线程是cpu调度的最小单位 2：线程依附于进程，没有进程就没有线程，一个进程默认创造一个主线程，一个进程可以创造多个线程 3：进程不共享全局变量，而同一个进程的线程可以共享进程的全局变量 4：进程开销比较大，可以利用多核（并行），线程开销比较小不能利用多核（并发） 三：什么是主进程： 主进程是：程序执行的入口，一个进程里默认带一个线程，主进程可以创建多个子线程， 四：写出创造进程的步骤： 1：导入进程包： ​ import multiprocessing 2：创建进程函数（方法） ​ def num（）： ​ pass 3：创建进程，调用进程函数（方法） ​ 变量 = multiprocessing.Process(target = num) 4:启动进程 变量.start（） 五：怎么获得进程的pid 1：improre os 2：os.getppid : print(os.getppid)查看父进程id编号 3：os.getpid ： print（os.getpid）查看当前进程id编号 4：multiprocessing.current_process().name进程（线程）名字 ：查看当前进程（线程）的id编号 六：怎么给进程函数传递参数： 1：通过元组进程传递参数 multiprocessing.Process(target = coding , args = （参数，）) 传入的是一个元组需要加上，传入的参数要保持一致 2：通过字典进程传参： multiprocessing.Process(target = coding , kwargs = {传递的进程变量名：传入的参数}) 因为用的是字典的要与进程参数对住 七：在主进程中，怎么堵塞等待子进程完成后，才能继续运行1：把join加入到子进程中，子进程变量.join，形成串联运行完子进程在运行主线线程 八：Process创建的实例对象的常用的方法有什么？ 1：start（）：启动子进程实例（创建子进程） 2：join（）：等待子进程执行结束，形成串联 3：terminate（）：不管任务是否完成，立即终止子进程 八《一》： Process常用的属性： 1：name：当前进程的别名，默认为Process - n ，n为从1开始递增的整数 2：pid：当前进程的id 九：编写一个程序，实现创建一个子进程，子进程每一秒打印一次：这是子进程，循环的第n次，打印 的次数是由主进程传递给子进程。 1：#利用args和kwargs传参 import multiprocessing import time def ness(num): for i in range(num): time.sleep(1) print("这个是一个线程") if __name__ == '__main__': # mues = multiprocessing.Process(target= ness,args=(6,)) mues = multiprocessing.Process(target=ness, kwargs={"num":6}) mues.start() 2：#利用类方法实现线程 import multiprocessing import time class nees(): def __init__(self,name): super(nees, self).__init__() self.name = name def num(self): for i in range(self.name): time.sleep(1) print("这个是一个子进程") if __name__ == '__main__': num_process = multiprocessing.Process(target=nees) A=nees(3) A.num() ：线程资源竞争问题： 1：100万次加法，线程1和线程2谁先执行完 2：线程1计算的结果一定比线程2小吗？ 3：某个线程计算的结果有没有可能小于100万？为什么： ​ 1：不一定谁先执行完，因为线程是由cpu进行调度的，没有执行顺序那个先执行完都是不一定的 ​ 2：不一定，因为没办法知道那个线程能先执行完，所有没有办法知道执行的时间，没有办法知道执行的大小 ​ 3：有的，因为多线程共享全局变量，再赋予全局变量的时候，线程一和线程二计算的先后没有办法确定下来，当线程一计算的多时，线程二计算的少时，线程一又把线程二中的值共享过来这时就不安之前的顺序的，会小于100万]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F05%2F23%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BTC%2F</url>
    <content type="text"><![CDATA[网络编程TC网络编程TC[TOC] P客户端 变量名称 变量使用方法 ifconfig 查看网络的ip ping 查看网络是否连通 socket 网络编程包 socket.AF_INET ip地址 socket.SOCK_STREAM 端口 tcp_clien_socket.connect 客户端：传入ip和端口，用于建立与指定指定socket的连接 tcp_clien_socket.send（字符串.encode(encoding=”utf-8”)） send（）：把数据组转换成二进制，encode：写入编辑的字符串和要转成数据组的函数：encode（encoding = “”） 字符串.encode(encoding=”utf-8”)） encode：把字符串转换成数据组 tcp_clien_socket.recv(数值) 套接字最大传入数据数值，接收来自socket缓存区对字节数据，当缓存区没有数值时会一直进行阻塞 .decode() 对服务端发来的数据进行解码 conn_socket.close() 关闭套接字 网络编程TCP服务端 变量名称 变量使用方法 socket 网络编程包 bind 服务端：创建ip和端口 tcp_evens_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) 端口的复用 tcp_sever_socket.listen（数值） 设置端口监听功能监听多少个数值上限 tcp_server_socket.accept() accept()接受一个客户端的连接请求，接收的数据用于recv，并返回一个新的套接字，用于了send分开数据和ip不同于以上socket()返回的用于监听和接受客户端的连接请求的套接字；， 扩展函数： 函数 函数表达意思 client_socket.recv(1024).decode() decode()：将查询的值翻译成其他的值，以其他形式表现出来 os.listdir 方法用于返回指定的文件夹包含的文件或文件夹的名字的列表。 os.path.getsize() 获得文件的大小（字节） f.read（） 从文件指针所在的位置开始读 isdigit 静态服务器： 函数 函数定义 .open（“地址”，“要读或写”） 用于打开一个文件，创建一个 file 对象，相关的方法才可以调用它进行读写。 .read() 读取文件信息内容 英文：response 翻译：相应 英文：Server 翻译：服务器 localhost:8080 浏览器查看 使用多线程多客户端连接服务器，多发送信息import socket import threading def duoe_sock(code_sock): while True: recv_data = code_sock.recv(1024) data = recv_data.decode() print(data) code_sock.send("niaho1".encode()) if data == "quit" or not data: break print("连接关闭") code_sock.close() if __name__ == '__main__': tcp_evelt_socket = socket.socket(socket.AF_INET,socket.SOCK_STREAM) tcp_evelt_socket.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,True) tcp_evelt_socket.bind(("",8080)) tcp_evelt_socket.listen(128) while True: code_sock,ip_lies = tcp_evelt_socket.accept() print("ip:",ip_lies) smeus_pskd = threading.Thread(target=duoe_sock,args=(code_sock,)) smeus_pskd.start() code_sock.close() 静态服务器返回固态页面：import socket if __name__ == '__main__': tcp_serer_sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM) tcp_serer_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) tcp_serer_sock.bind(("",8080)) tcp_serer_sock.listen(800) while True: clien_sock,client_abbr = tcp_serer_sock.accept() clien_request_data = clien_sock.recv(10230).decode() print(clien_request_data) with open("./nksld.html","rb") as f:#打开文件夹有会报错，异常处理 # f = open("当前文件夹","rb") file_data = f.read() # 应答行 response_line = "HTTP/1.1 200 OK\r\n" # 头 response_header = "Server:pwd\r\n" # 体 response_body = file_data#界面的数据 #应答数据 response_data= (response_line + response_header + "\r\n").encode() + response_body clien_sock.send(response_data) clien_sock.close() 静态服务器返回指定页面import socket if __name__ == '__main__': tcp_server_sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM) tcp_server_sock.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,True) tcp_server_sock.bind(("",8080)) tcp_server_sock.listen(123) while True: tcp_sock,ip_sock = tcp_server_sock.accept() print("ip",ip_sock) recv_data = tcp_sock.recv(1024).decode() spli_dat = recv_data.split() if len(spli_dat) &lt;=1: tcp_sock.close() print("浏览器关闭") else: data = spli_dat[1] if data == "/": data = "/001.jpg" try: with open("."+ data,"rb") as f: f_data = f.read() except Exception as e: tecponse_hine = "HTTP/1.1 404 NG\r\n" tecponse_toue = "Server : long\r\n" tecponse_toes = "404 NG\r\n" tecponse_zhen = (tecponse_hine+tecponse_toue+"\r\n"+tecponse_toes).encode() tcp_sock.send(tecponse_zhen) else: tecponse_hine = "HTTP/1.1 200 OK\r\n" tecponse_toue = "Server : long\r\n" tecponse_toes = f_data tecponse_zhen = (tecponse_hine + tecponse_toue+ "\r\n").encode()+ tecponse_toes tcp_sock.send(tecponse_zhen) finally: tcp_sock.close() 静态服务器多线程任务import threading import socket def nuws_sock(tcp_sock): recv_data = tcp_sock.recv(1024).decode() spli_dat = recv_data.split() if len(spli_dat) &lt;=1: tcp_sock.close() print("浏览器关闭") else: data = spli_dat[1] if data == "/": data = "/001.jpg" try: with open("."+ data,"rb") as f: f_data = f.read() except Exception as e: tecponse_hine = "HTTP/1.1 404 NG\r\n" tecponse_toue = "Server : long\r\n" tecponse_toes = "404 NG\r\n" tecponse_zhen = (tecponse_hine+tecponse_toue+"\r\n"+tecponse_toes).encode() tcp_sock.send(tecponse_zhen) else: tecponse_hine = "HTTP/1.1 200 OK\r\n" tecponse_toue = "Server : long\r\n" tecponse_toes = f_data tecponse_zhen = (tecponse_hine + tecponse_toue+ "\r\n").encode()+ tecponse_toes tcp_sock.send(tecponse_zhen) finally: tcp_sock.close() if __name__ == '__main__': tcp_server_sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM) tcp_server_sock.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,True) tcp_server_sock.bind(("",8080)) tcp_server_sock.listen(123) while True: tcp_sock,ip_sock = tcp_server_sock.accept() print("ip",ip_sock) nues_sock = threading.Thread(target=nuws_sock,args=(tcp_sock,)) nues_sock.start() 静态服务器对象调用import threading import socket class Pues_sock(): def __init__(self): self.tcp_server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.tcp_server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) self.tcp_server_sock.bind(("", 8080)) self.tcp_server_sock.listen(123) def nuws_sock(self,tcp_sock): recv_data = tcp_sock.recv(1024).decode() spli_dat = recv_data.split() if len(spli_dat) &lt;=1: tcp_sock.close() print("浏览器关闭") else: data = spli_dat[1] if data == "/": data = "/001.jpg" try: with open("."+ data,"rb") as f: f_data = f.read() except Exception as e: tecponse_hine = "HTTP/1.1 404 NG\r\n" tecponse_toue = "Server : long\r\n" tecponse_toes = "404 NG\r\n" tecponse_zhen = (tecponse_hine+tecponse_toue+"\r\n"+tecponse_toes).encode() tcp_sock.send(tecponse_zhen) else: tecponse_hine = "HTTP/1.1 200 OK\r\n" tecponse_toue = "Server : long\r\n" tecponse_toes = f_data tecponse_zhen = (tecponse_hine + tecponse_toue+ "\r\n").encode()+ tecponse_toes tcp_sock.send(tecponse_zhen) finally: tcp_sock.close() def nuse_sock(self): while True: tcp_sock, ip_sock = self.tcp_server_sock.accept() print("ip", ip_sock) nues_sock = threading.Thread(target=self.nuws_sock, args=(tcp_sock,)) nues_sock.start() if __name__ == '__main__': nues = Pues_sock() nues.nuse_sock() 静态服务器变换端口号import socket import threading import sys def main(): # 获取执行python程序的终端命令行参数 print(sys.argv) if len(sys.argv) != 2: print("格式错误 python3 xxx.py 9090") return # 判断参数的类型，设置端口号必须是整型 if not sys.argv[1].isdigit(): print("格式错误 python3 xxx.py 9090") return port = int(sys.argv[1]) # 创建服务器对象 # 给Web服务器类的初始化方法添加一个端口号参数，用于绑定端口号 my_web_server = HttpWebServer(port) # 启动服务器 my_web_server.start() if __name__ == '__main__': main() TCP协议是什么？提示：TCP定义及特点定义：传输控制协议，是一种面向连接，可靠的，基于字节流的传输层通讯协议 特点：面向连接：通信双方必须先建立好连接才能进行数据的传输，数据传输完成后，双方必须断开连接，以释放系统资源 可靠的通信方式 基于字节流 2. TCP特点是什么？提示：TCP特点及解释说明 基于字节流的 面向连接 可靠通信 在网络状况不佳的时候尽量降低系统由于重传带来的宽开销 通信连接维护是面向通信的两个端点，而不考虑中间网段和节点 3. 在TCP客户端使用connect的作用是什么？提示：connect的作用及connect()函数的参数形式 connenct：用于建立与指定指定socket的连接，对于流类套接口（SOCK—STREAMl类型），利用名字来与一个远程主机建立连接，一但套用接口调用成功，他就能收发数据，对于数据报类套接口（SOCK—STREAMl类型），则设置成一个缺省的目的并用它来进行后序的send与recv调用 4. 在TCP服务器中listen的作用是什么？提示：listen()函数的作用及参数含义、 在网络中服务端是被动的，客户是主动的，被动要用listen来监听，listen（）作用是将socket（）得到一个sockfd被动监听的套接字，来通知内核来完成连接 5. 简述tcp客户端创建的流程。提示：参考课件代码说出客户端创建步骤 创建一个套接字协议用：connenct赖建立连接 导入IP地址可端口 创建发送的数据用send（数据encode），先把字符串转换成数据组，在把数据组装换成二进制 设置最大字节数据：recv（） 接收客户端发来的数据用decode来解码 关闭套接字用close 6. recv函数的参数表示什么含义？提示：recv()函数的作用及参数含义 接收来自socket缓存区对字节数据，当缓存区没有数值时会一直进行阻塞，直到有数据时进行读取，远程关闭并读取所有数据之后会返回空字符串 7. 简述TCP服务器创建的流程、提示：参考课件代码，说出服务器创建流程 1：创建套接字，进行网络协议 2：进行端口复用 3：进行bind进行套接字协议 4：进行与服务端监听连接的最大数值，listen 5：accept（）接收数据再返回一个新的socket字，接收的数据用于recv，返回客户端的数据用于了send分开数据和ip， 6：进行设置recv进行数据的阻塞， 7：给客户端发送一个数据用send（encode）来转成二进制 8：接收客户端发来的数据，用decode来进行解码 8. accept的返回值分别代表的什么？提示：accept()函数的作用，及返回值的形式和返回值的含义 accept（）接收数据再返回一个新的socket字，接收的数据用于recv，返回客户端的数据用于了send分开数据和ip， 9. 完成TCP客户端的编写。服务器的ip和端口号需手动输入 要发送的信息需要手动输入 接收服务器返回的数据，并打印。 请在下方的注释下补充代码： ​ # 导入socket模块 import socket if __name__ == '__main__': tcp_codne_socket = socket.socket(socket.AF_INET,socket.SOCK_STREAM) tcp_codne_socket.connect(("",8080)) tcp_codne_socket.send("".encode()) cone_recv = tcp_codne_socket.recv(1024) print(cone_recv.decode()) tcp_codne_socket.close() 10. 完成TCP服务的的编写请在下方注释下补充代码： # 导入socket模块 import socket if __name__ == '__main__': tcp_evens_socket = socket.socket(socket.AF_INET,socket.SOCK_STREAM) tcp_evens_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) tcp_evens_socket.bind(("",8080)) tcp_evens_socket.listen(138) coet_socket,ip_mes = tcp_evens_socket.accept() recv_data = coet_socket.recv(1025) print(recv_data.decode()) coet_socket.send("".encode()) coet_socket.close()]]></content>
  </entry>
</search>
